{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Database connection setup\n",
    "db_config = {\n",
    "    'host': 'localhost',\n",
    "    'database': 'postgres',\n",
    "    'user': 'postgres',\n",
    "    'password': 'kaviyam123',\n",
    "    'port': '5432'\n",
    "}\n",
    "connection_string = f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['database']}\"\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Step 1: Load data from PostgreSQL\n",
    "query = 'SELECT * FROM public.pre_table;'\n",
    "df = pd.read_sql(query, con=engine)\n",
    "\n",
    "# Step 4: Identify anomalies\n",
    "group_cols = ['policy no']\n",
    "columns_to_check = ['cleaned insured name', 'Cleaned Branch Name 2']\n",
    "\n",
    "# Add a column to store non-unique column names\n",
    "df['Non_unique_columns'] = ''\n",
    "\n",
    "# Create a boolean mask for anomalies\n",
    "mask = pd.Series(False, index=df.index)\n",
    "grouped = df.groupby(group_cols)\n",
    "\n",
    "for col in columns_to_check:\n",
    "    # Find groups with more than one unique value\n",
    "    unique_within_group = grouped[col].transform('nunique')\n",
    "    col_mask = unique_within_group > 1\n",
    "    mask |= col_mask\n",
    "    df.loc[col_mask, 'Non_unique_columns'] += col + ', '\n",
    "\n",
    "# Remove trailing commas from 'Non_unique_columns'\n",
    "df['Non_unique_columns'] = df['Non_unique_columns'].str.rstrip(', ')\n",
    "\n",
    "# Step 5: Separate anomalous and correct data\n",
    "anomalous_data = df[mask]\n",
    "correct_data = df[~mask]\n",
    "\n",
    "# Step 6: Save the results\n",
    "# Save anomalous data to CSV\n",
    "anomalous_data.to_csv('anomalous_data.csv', index=False)\n",
    "\n",
    "# Save correct data to the database or a CSV\n",
    "correct_data.to_sql('corrected_pre_table', engine, if_exists='replace', index=False)\n",
    "\n",
    "# Display summary\n",
    "print(f\"Anomalous data saved to 'anomalous_data.csv' with {len(anomalous_data)} rows.\")\n",
    "print(f\"Correct data saved to 'corrected_merged_data' table in the database with {len(correct_data)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_7396\\1334600520.py:21: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  claim_data = pd.read_csv('unique_rows(claim).csv')\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_7396\\1334600520.py:26: UserWarning: Parsing dates in %d-%m-%Y %H:%M format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  claim_data['Policy Start Date_claim'] = pd.to_datetime(claim_data['Policy Start Date_claim'], errors='coerce')\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_7396\\1334600520.py:27: UserWarning: Parsing dates in %d-%m-%Y %H:%M format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  claim_data['Policy End Date_claim'] = pd.to_datetime(claim_data['Policy End Date_claim'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data has been saved to the 'overallcorrected_base_pr_claim' table in the database.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.types import Text, Integer, Float, DateTime\n",
    "\n",
    "# Database connection setup\n",
    "db_config = {\n",
    "    'host': 'localhost',\n",
    "    'database': 'postgres',\n",
    "    'user': 'postgres',\n",
    "    'password': 'kaviyam123',\n",
    "    'port': '5432'\n",
    "}\n",
    "connection_string = f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['database']}\"\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Step 1: Load policy data from PostgreSQL\n",
    "policy_query = 'SELECT * FROM mapoldpolicy_handled_bookedcase_base_pr;'\n",
    "policy_data = pd.read_sql(policy_query, con=engine)\n",
    "\n",
    "# Step 2: Load claim data from Excel\n",
    "claim_data = pd.read_csv('unique_rows(claim).csv')\n",
    "\n",
    "# Step 3: Convert date columns to datetime format\n",
    "policy_data['policy start date'] = pd.to_datetime(policy_data['policy start date'], errors='coerce')\n",
    "policy_data['policy end date'] = pd.to_datetime(policy_data['policy end date'], errors='coerce')\n",
    "claim_data['Policy Start Date_claim'] = pd.to_datetime(claim_data['Policy Start Date_claim'], errors='coerce')\n",
    "claim_data['Policy End Date_claim'] = pd.to_datetime(claim_data['Policy End Date_claim'], errors='coerce')\n",
    "\n",
    "# Step 4: Merge the datasets on the specified columns\n",
    "merged_data = pd.merge(\n",
    "    policy_data,\n",
    "    claim_data,\n",
    "    how='left',  # Use 'left' to keep all rows in the policy data\n",
    "    left_on=['policy no', 'policy start date', 'policy end date'],\n",
    "    right_on=['Policy No (Str)', 'Policy Start Date_claim', 'Policy End Date_claim'],\n",
    "    suffixes=('_policy', '_claim')  # Avoid column conflicts\n",
    ")\n",
    "\n",
    "# Step 5: Map pandas dtypes to SQLAlchemy types dynamically\n",
    "def map_column_types(df):\n",
    "    dtype_mapping = {}\n",
    "    for col in df.columns:\n",
    "        if pd.api.types.is_integer_dtype(df[col]):\n",
    "            dtype_mapping[col] = Integer\n",
    "        elif pd.api.types.is_float_dtype(df[col]):\n",
    "            dtype_mapping[col] = Float\n",
    "        elif pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "            dtype_mapping[col] = DateTime\n",
    "        elif pd.api.types.is_object_dtype(df[col]) or pd.api.types.is_string_dtype(df[col]):\n",
    "            dtype_mapping[col] = Text\n",
    "        else:\n",
    "            dtype_mapping[col] = Text  # Default to Text for unsupported types\n",
    "    return dtype_mapping\n",
    "\n",
    "dtype_mapping = map_column_types(merged_data)\n",
    "\n",
    "# Step 6: Save the merged data to PostgreSQL\n",
    "merged_table_name = 'overallcorrected_base_pr_claim'\n",
    "merged_data.to_sql(\n",
    "    merged_table_name,\n",
    "    con=engine,\n",
    "    if_exists='replace',\n",
    "    index=False,\n",
    "    dtype=dtype_mapping\n",
    ")\n",
    "\n",
    "# Step 7: Save the merged data to a CSV file (optional)\n",
    "# merged_data.to_csv('merged_policy_claim_data(Liberty).csv', index=False)\n",
    "\n",
    "# Display success message\n",
    "print(f\"Merged data has been saved to the '{merged_table_name}' table in the database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Database connection setup\n",
    "db_config = {\n",
    "    'host': 'localhost',\n",
    "    'database': 'postgres',\n",
    "    'user': 'postgres',\n",
    "    'password': 'kaviyam123',\n",
    "    'port': '5432'\n",
    "}\n",
    "connection_string = f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['database']}\"\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Step 1: Load data from PostgreSQL\n",
    "query = 'SELECT * FROM public.corrected_pre_table_claim;'\n",
    "df = pd.read_sql(query, con=engine)\n",
    "\n",
    "\n",
    "df['CustomerID_Base'] = (df['cleaned insured name'].astype(str) + '_' +\n",
    "                         df['Cleaned Branch Name 2'].astype(str))\n",
    "df['CustomerID'] = (df.groupby('CustomerID_Base').ngroup() + 1000001).astype(str)\n",
    "\n",
    "# Convert dates to datetime\n",
    "df['policy start date'] = pd.to_datetime(df['policy start date'], errors='coerce')\n",
    "df['policy end date'] = pd.to_datetime(df['policy end date'], errors='coerce')\n",
    "\n",
    "\n",
    "# policy_status_map = {'0.0': 'Not Renewed', '1.0': 'Renewed', '2.0' : 'Open'}\n",
    "# df['Policy Status'] = df['upd_booked'].map(policy_status_map)\n",
    "\n",
    "# Step 2: Calculate Policy Tenure (Months) for each individual policy\n",
    "df['Policy Tenure Month'] = ((df['policy end date'].dt.year - df['policy start date'].dt.year) * 12 +\n",
    "                             (df['policy end date'].dt.month - df['policy start date'].dt.month))\n",
    "\n",
    "# Calculate policy tenure in years (rounded)\n",
    "df['Policy Tenure'] = (df['Policy Tenure Month'] / 12).round(0)\n",
    "\n",
    "# Step 2: Extract Start Year for grouping\n",
    "df['Start Year'] = df['policy start date'].dt.year\n",
    "\n",
    "# Extract the year from start and end dates for other calculations\n",
    "df['End Year'] = df['policy end date'].dt.year\n",
    "\n",
    "# Step 3: Group by CustomerID and Start Year to calculate min start date, max end date, and yearly tenure\n",
    "yearly_tenure = (\n",
    "    df.groupby(['CustomerID', 'Start Year'])\n",
    "    .agg({'policy start date': 'min', 'policy end date': 'max'})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Calculate Yearly Tenure (Months)\n",
    "yearly_tenure['Yearly Tenure (Months)'] = (\n",
    "    (yearly_tenure['policy end date'].dt.year - yearly_tenure['policy start date'].dt.year) * 12 +\n",
    "    (yearly_tenure['policy end date'].dt.month - yearly_tenure['policy start date'].dt.month)\n",
    ")\n",
    "\n",
    "# Step 4: Calculate Cumulative Tenure\n",
    "yearly_tenure['Cumulative Tenure (Months)'] = (\n",
    "    yearly_tenure.groupby('CustomerID')['Yearly Tenure (Months)']\n",
    "    .cumsum()\n",
    ")\n",
    "\n",
    "# Convert Cumulative Tenure to years and calculate Customer Tenure\n",
    "yearly_tenure['Tenure Decimal'] = yearly_tenure['Cumulative Tenure (Months)'] / 12\n",
    "yearly_tenure['Customer Tenure'] = yearly_tenure['Tenure Decimal'].round(0)\n",
    "\n",
    "# Select relevant columns for mapping back to original data\n",
    "tenure_mapping = yearly_tenure[['CustomerID', 'Start Year', 'Cumulative Tenure (Months)', 'Tenure Decimal', 'Customer Tenure']]\n",
    "\n",
    "# Step 5: Map back to the original data\n",
    "df = df.merge(tenure_mapping, on=['CustomerID', 'Start Year'], how='left')\n",
    "\n",
    "# Step 8: Add New Customers column\n",
    "df['FirstPolicyYear'] = df.groupby('CustomerID')['Start Year'].transform('min')\n",
    "df['New_Customer_ID'] = df.apply(\n",
    "    lambda row: f\"{row['FirstPolicyYear']}_{row['CustomerID']}\" if row['Start Year'] == row['FirstPolicyYear'] else '',\n",
    "    axis=1\n",
    ")\n",
    "df['New Customers'] = df['New_Customer_ID'].apply(lambda x: 'Yes' if x else 'No')\n",
    "\n",
    "# Step 10: Calculate year-wise churn status\n",
    "def calculate_churn_status(group):\n",
    "    unique_statuses = group.unique()\n",
    "    if len(unique_statuses) == 1 and unique_statuses[0] == 'Not Renewed':\n",
    "        return 'Yes'\n",
    "    else:\n",
    "        return 'No'\n",
    "\n",
    "df['Churn Label'] = df.groupby(['CustomerID', 'End Year'])['Policy Status'].transform(lambda x: calculate_churn_status(x))\n",
    "\n",
    "from sqlalchemy import text\n",
    "\n",
    "# Save the processed data into PostgreSQL\n",
    "processed_table_name = 'overall_cleaned_base_and_pr_ef'  # Target table name\n",
    "\n",
    "# Create a connection to the database\n",
    "with engine.connect() as connection:\n",
    "    # Drop the table if it exists\n",
    "    drop_query = f\"DROP TABLE IF EXISTS {processed_table_name};\"\n",
    "    connection.execute(text(drop_query))  # Execute the drop statement\n",
    "    print(f\"Table {processed_table_name} dropped successfully.\")\n",
    "\n",
    "    # Load the new data into the table\n",
    "    df.to_sql(processed_table_name, con=engine, if_exists='replace', index=False)\n",
    "    print(f\"Data loaded into {processed_table_name} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Database connection setup\n",
    "db_config = {\n",
    "    'host': 'localhost',\n",
    "    'database': 'postgres',\n",
    "    'user': 'postgres',\n",
    "    'password': 'kaviyam123',\n",
    "    'port': '5432'\n",
    "}\n",
    "connection_string = f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['database']}\"\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Step 1: Load data from PostgreSQL\n",
    "query = 'SELECT * FROM public.overallcorrected_base_pr_claim;'\n",
    "df = pd.read_sql(query, con=engine)\n",
    "\n",
    "\n",
    "df['CustomerID_Base'] = (df['corrected_name'].astype(str) + '_' +\n",
    "                         df['Cleaned Branch Name 2'].astype(str))\n",
    "df['CustomerID'] = (df.groupby('CustomerID_Base').ngroup() + 1000001).astype(str)\n",
    "\n",
    "# Convert dates to datetime\n",
    "df['policy start date'] = pd.to_datetime(df['policy start date'], errors='coerce')\n",
    "df['policy end date'] = pd.to_datetime(df['policy end date'], errors='coerce')\n",
    "\n",
    "\n",
    "# policy_status_map = {'0.0': 'Not Renewed', '1.0': 'Renewed', '2.0' : 'Open'}\n",
    "# df['Policy Status'] = df['upd_booked'].map(policy_status_map)\n",
    "\n",
    "# Step 2: Calculate Policy Tenure (Months) for each individual policy\n",
    "df['Policy Tenure Month'] = ((df['policy end date'].dt.year - df['policy start date'].dt.year) * 12 +\n",
    "                             (df['policy end date'].dt.month - df['policy start date'].dt.month))\n",
    "\n",
    "# Calculate policy tenure in years (rounded)\n",
    "df['Policy Tenure'] = (df['Policy Tenure Month'] / 12).round(0)\n",
    "\n",
    "# Step 2: Extract Start Year for grouping\n",
    "df['Start Year'] = df['policy start date'].dt.year\n",
    "\n",
    "# Extract the year from start and end dates for other calculations\n",
    "df['End Year'] = df['policy end date'].dt.year\n",
    "\n",
    "# Step 3: Group by CustomerID and Start Year to calculate min start date, max end date, and yearly tenure\n",
    "yearly_tenure = (\n",
    "    df.groupby(['CustomerID', 'Start Year'])\n",
    "    .agg({'policy start date': 'min', 'policy end date': 'max'})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Calculate Yearly Tenure (Months)\n",
    "yearly_tenure['Yearly Tenure (Months)'] = (\n",
    "    (yearly_tenure['policy end date'].dt.year - yearly_tenure['policy start date'].dt.year) * 12 +\n",
    "    (yearly_tenure['policy end date'].dt.month - yearly_tenure['policy start date'].dt.month)\n",
    ")\n",
    "\n",
    "# Step 4: Calculate Cumulative Tenure\n",
    "yearly_tenure['Cumulative Tenure (Months)'] = (\n",
    "    yearly_tenure.groupby('CustomerID')['Yearly Tenure (Months)']\n",
    "    .cumsum()\n",
    ")\n",
    "\n",
    "# Convert Cumulative Tenure to years and calculate Customer Tenure\n",
    "yearly_tenure['Tenure Decimal'] = yearly_tenure['Cumulative Tenure (Months)'] / 12\n",
    "yearly_tenure['Customer Tenure'] = yearly_tenure['Tenure Decimal'].round(0)\n",
    "\n",
    "# Select relevant columns for mapping back to original data\n",
    "tenure_mapping = yearly_tenure[['CustomerID', 'Start Year', 'Cumulative Tenure (Months)', 'Tenure Decimal', 'Customer Tenure']]\n",
    "\n",
    "# Step 5: Map back to the original data\n",
    "df = df.merge(tenure_mapping, on=['CustomerID', 'Start Year'], how='left')\n",
    "\n",
    "# Step 8: Add New Customers column\n",
    "df['FirstPolicyYear'] = df.groupby('CustomerID')['Start Year'].transform('min')\n",
    "df['New_Customer_ID'] = df.apply(\n",
    "    lambda row: f\"{row['FirstPolicyYear']}_{row['CustomerID']}\" if row['Start Year'] == row['FirstPolicyYear'] else '',\n",
    "    axis=1\n",
    ")\n",
    "df['New Customers'] = df['New_Customer_ID'].apply(lambda x: 'Yes' if x else 'No')\n",
    "\n",
    "# Step 10: Calculate year-wise churn status\n",
    "def calculate_churn_status(group):\n",
    "    unique_statuses = group.unique()\n",
    "    if len(unique_statuses) == 1 and unique_statuses[0] == 'Not Renewed':\n",
    "        return 'Yes'\n",
    "    else:\n",
    "        return 'No'\n",
    "\n",
    "df['Churn Label'] = df.groupby(['CustomerID', 'End Year'])['Policy Status'].transform(lambda x: calculate_churn_status(x))\n",
    "\n",
    "from sqlalchemy import text\n",
    "\n",
    "# # Save the processed data into PostgreSQL\n",
    "# processed_table_name = 'overall_cleaned_base_and_pr_ef'  # Target table name\n",
    "\n",
    "# # Create a connection to the database\n",
    "# with engine.connect() as connection:\n",
    "#     # Drop the table if it exists\n",
    "#     drop_query = f\"DROP TABLE IF EXISTS {processed_table_name};\"\n",
    "#     connection.execute(text(drop_query))  # Execute the drop statement\n",
    "#     print(f\"Table {processed_table_name} dropped successfully.\")\n",
    "\n",
    "#     # Load the new data into the table\n",
    "#     df.to_sql(processed_table_name, con=engine, if_exists='replace', index=False)\n",
    "#     print(f\"Data loaded into {processed_table_name} successfully.\")\n",
    "\n",
    "# Save correct data to the database or a CSV\n",
    "df.to_sql('overall_cleaned_base_and_pr_ef', engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

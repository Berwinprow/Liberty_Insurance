{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_curve, roc_auc_score, classification_report, log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "# Database connection setup\n",
    "db_config = {\n",
    "    'host': 'localhost',\n",
    "    'database': 'postgres',\n",
    "    'user': 'postgres',\n",
    "    'password': 'kaviyam123',\n",
    "    'port': '5432'\n",
    "}\n",
    "connection_string = f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['database']}\"\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Load data from PostgreSQL\n",
    "query = 'SELECT * FROM public.overall_cleaned_base_and_pr_ef_policyef;'\n",
    "df = pd.read_sql(query, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_13860\\3725740938.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['Status_Binary'] = df_filtered['Policy Status'].apply(lambda x: 1 if x == 'Not Renewed' else 0)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_13860\\3725740938.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected[numerical_columns] = df_selected[numerical_columns].apply(pd.to_numeric, errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (1503014, 47)\n",
      "Shape of y: (1503014,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "df_filtered = df[df['Policy Status'].isin(['Renewed', 'Not Renewed'])]\n",
    "\n",
    "df_filtered['Status_Binary'] = df_filtered['Policy Status'].apply(lambda x: 1 if x == 'Not Renewed' else 0)\n",
    "\n",
    "# Select relevant columns\n",
    "selected_columns = ['renewal type', 'product name', 'product name 2',  'biztype', 'age', 'vehicle idv', 'before gst add-on gwp', 'total od premium', 'total tp premium', 'gst', \n",
    " 'total premium payable', 'ncb % previous year', 'applicable discount with ncb', 'tie up',\n",
    " 'Number of claims', 'approved', 'denied', 'Policy Tenure', 'Customer Tenure', 'New Customers', 'Claim Happaned/Not', \n",
    " 'Renewal Rate Status', 'withdrawn', 'policy_wise_purchase', 'Status_Binary']\n",
    "\n",
    "df_selected = df_filtered[selected_columns]\n",
    "\n",
    "# Convert numerical columns to float\n",
    "numerical_columns = ['age', 'vehicle idv', 'before gst add-on gwp', 'total od premium', 'total tp premium', 'gst', \n",
    " 'total premium payable', 'ncb % previous year', 'applicable discount with ncb',\n",
    " 'Number of claims', 'approved', 'denied', 'Policy Tenure', 'Customer Tenure', 'withdrawn', 'policy_wise_purchase', 'Status_Binary']\n",
    "\n",
    "\n",
    "df_selected[numerical_columns] = df_selected[numerical_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# One-hot encode categorical columns\n",
    "categorical_columns = ['renewal type', 'product name', 'product name 2',  'biztype', 'tie up', 'New Customers', 'Claim Happaned/Not', \n",
    " 'Renewal Rate Status']\n",
    "\n",
    "df_selected = pd.get_dummies(df_selected, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "df_selected = pd.DataFrame(imputer.fit_transform(df_selected), columns=df_selected.columns)\n",
    "\n",
    "# Convert to sparse matrix\n",
    "X = csr_matrix(df_selected.drop(columns=['Status_Binary']).values)\n",
    "y = df_selected['Status_Binary']\n",
    "\n",
    "# Debugging dimensions\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training XGBoost Model - Single Run\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CatBoostClassifier' object has no attribute 'get_booster'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m cat_model\u001b[38;5;241m.\u001b[39mfit(X, y)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Extract tree dump (all trees)\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m booster \u001b[38;5;241m=\u001b[39m \u001b[43mcat_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_booster\u001b[49m()\n\u001b[0;32m     25\u001b[0m tree_dumps \u001b[38;5;241m=\u001b[39m booster\u001b[38;5;241m.\u001b[39mget_dump()\n\u001b[0;32m     27\u001b[0m formatted_trees \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CatBoostClassifier' object has no attribute 'get_booster'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Feature dependencies\n",
    "all_feature_dependencies = defaultdict(list)\n",
    "\n",
    "# Prepare feature names\n",
    "feature_names = df_selected.drop(columns=['Status_Binary']).columns.tolist()\n",
    "\n",
    "# Train CatBoost model\n",
    "print(\"\\nTraining CatBoost Model - Single Run\")\n",
    "\n",
    "cat_model = CatBoostClassifier(\n",
    "    depth=10, learning_rate=0.1, iterations=500,\n",
    "    random_seed=42, verbose=0\n",
    ")\n",
    "\n",
    "cat_model.fit(X, y)\n",
    "\n",
    "# **ðŸ”¹ Extract tree dump (all trees)**\n",
    "model_file = \"catboost_model.json\"\n",
    "cat_model.save_model(model_file, format=\"json\")\n",
    "\n",
    "# Load JSON model\n",
    "with open(model_file, \"r\") as file:\n",
    "    model_data = json.load(file)\n",
    "\n",
    "formatted_trees = []\n",
    "\n",
    "# **ðŸ”¹ Process each tree separately**\n",
    "for tree_index, tree in enumerate(model_data[\"oblivious_trees\"]):\n",
    "    formatted_tree = []\n",
    "    indent = \"    \"  # Indentation for formatting\n",
    "\n",
    "    print(f\"Processing tree {tree_index + 1}/{len(model_data['oblivious_trees'])}...\")\n",
    "\n",
    "    for split_index, split in enumerate(tree[\"splits\"]):\n",
    "        feature_index = split[\"float_feature_index\"]  # Feature index used in split\n",
    "        threshold = split[\"border\"]  # Threshold value for splitting\n",
    "        feature_name = feature_names[feature_index]  # Map index to actual feature name\n",
    "\n",
    "        # Format tree structure like XGBoost output\n",
    "        condition = f\"f{feature_index}<{threshold}\"\n",
    "        formatted_condition = f\"{feature_name} < {threshold}\"\n",
    "        formatted_line = f\"|--- Split {split_index}: {formatted_condition}\"\n",
    "        \n",
    "        formatted_tree.append(indent + formatted_line)\n",
    "\n",
    "    # **ðŸ”¹ Convert leaf values into class labels**\n",
    "    leaf_values = tree[\"leaf_values\"]\n",
    "    for leaf_index, value in enumerate(leaf_values):\n",
    "        logit = float(value)\n",
    "        probability = 1 / (1 + np.exp(-logit))  # Sigmoid function\n",
    "        predicted_class = 1 if probability > 0.5 else 0\n",
    "        formatted_tree.append(f\"{indent}|--- Leaf {leaf_index}: class {predicted_class}\")\n",
    "\n",
    "    formatted_trees.append(\"\\n\".join(formatted_tree))\n",
    "\n",
    "# **ðŸ”¹ Save formatted trees**\n",
    "output_file = \"catboost_tree_run.txt\"\n",
    "with open(output_file, \"w\") as file:\n",
    "    file.write(\"\\n\\n\".join(formatted_trees))\n",
    "\n",
    "print(f\"Decision trees saved as '{output_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training CatBoost Model - Single Run\n",
      "Processing tree 1/500...\n",
      "Processing tree 2/500...\n",
      "Processing tree 3/500...\n",
      "Processing tree 4/500...\n",
      "Processing tree 5/500...\n",
      "Processing tree 6/500...\n",
      "Processing tree 7/500...\n",
      "Processing tree 8/500...\n",
      "Processing tree 9/500...\n",
      "Processing tree 10/500...\n",
      "Processing tree 11/500...\n",
      "Processing tree 12/500...\n",
      "Processing tree 13/500...\n",
      "Processing tree 14/500...\n",
      "Processing tree 15/500...\n",
      "Processing tree 16/500...\n",
      "Processing tree 17/500...\n",
      "Processing tree 18/500...\n",
      "Processing tree 19/500...\n",
      "Processing tree 20/500...\n",
      "Processing tree 21/500...\n",
      "Processing tree 22/500...\n",
      "Processing tree 23/500...\n",
      "Processing tree 24/500...\n",
      "Processing tree 25/500...\n",
      "Processing tree 26/500...\n",
      "Processing tree 27/500...\n",
      "Processing tree 28/500...\n",
      "Processing tree 29/500...\n",
      "Processing tree 30/500...\n",
      "Processing tree 31/500...\n",
      "Processing tree 32/500...\n",
      "Processing tree 33/500...\n",
      "Processing tree 34/500...\n",
      "Processing tree 35/500...\n",
      "Processing tree 36/500...\n",
      "Processing tree 37/500...\n",
      "Processing tree 38/500...\n",
      "Processing tree 39/500...\n",
      "Processing tree 40/500...\n",
      "Processing tree 41/500...\n",
      "Processing tree 42/500...\n",
      "Processing tree 43/500...\n",
      "Processing tree 44/500...\n",
      "Processing tree 45/500...\n",
      "Processing tree 46/500...\n",
      "Processing tree 47/500...\n",
      "Processing tree 48/500...\n",
      "Processing tree 49/500...\n",
      "Processing tree 50/500...\n",
      "Processing tree 51/500...\n",
      "Processing tree 52/500...\n",
      "Processing tree 53/500...\n",
      "Processing tree 54/500...\n",
      "Processing tree 55/500...\n",
      "Processing tree 56/500...\n",
      "Processing tree 57/500...\n",
      "Processing tree 58/500...\n",
      "Processing tree 59/500...\n",
      "Processing tree 60/500...\n",
      "Processing tree 61/500...\n",
      "Processing tree 62/500...\n",
      "Processing tree 63/500...\n",
      "Processing tree 64/500...\n",
      "Processing tree 65/500...\n",
      "Processing tree 66/500...\n",
      "Processing tree 67/500...\n",
      "Processing tree 68/500...\n",
      "Processing tree 69/500...\n",
      "Processing tree 70/500...\n",
      "Processing tree 71/500...\n",
      "Processing tree 72/500...\n",
      "Processing tree 73/500...\n",
      "Processing tree 74/500...\n",
      "Processing tree 75/500...\n",
      "Processing tree 76/500...\n",
      "Processing tree 77/500...\n",
      "Processing tree 78/500...\n",
      "Processing tree 79/500...\n",
      "Processing tree 80/500...\n",
      "Processing tree 81/500...\n",
      "Processing tree 82/500...\n",
      "Processing tree 83/500...\n",
      "Processing tree 84/500...\n",
      "Processing tree 85/500...\n",
      "Processing tree 86/500...\n",
      "Processing tree 87/500...\n",
      "Processing tree 88/500...\n",
      "Processing tree 89/500...\n",
      "Processing tree 90/500...\n",
      "Processing tree 91/500...\n",
      "Processing tree 92/500...\n",
      "Processing tree 93/500...\n",
      "Processing tree 94/500...\n",
      "Processing tree 95/500...\n",
      "Processing tree 96/500...\n",
      "Processing tree 97/500...\n",
      "Processing tree 98/500...\n",
      "Processing tree 99/500...\n",
      "Processing tree 100/500...\n",
      "Processing tree 101/500...\n",
      "Processing tree 102/500...\n",
      "Processing tree 103/500...\n",
      "Processing tree 104/500...\n",
      "Processing tree 105/500...\n",
      "Processing tree 106/500...\n",
      "Processing tree 107/500...\n",
      "Processing tree 108/500...\n",
      "Processing tree 109/500...\n",
      "Processing tree 110/500...\n",
      "Processing tree 111/500...\n",
      "Processing tree 112/500...\n",
      "Processing tree 113/500...\n",
      "Processing tree 114/500...\n",
      "Processing tree 115/500...\n",
      "Processing tree 116/500...\n",
      "Processing tree 117/500...\n",
      "Processing tree 118/500...\n",
      "Processing tree 119/500...\n",
      "Processing tree 120/500...\n",
      "Processing tree 121/500...\n",
      "Processing tree 122/500...\n",
      "Processing tree 123/500...\n",
      "Processing tree 124/500...\n",
      "Processing tree 125/500...\n",
      "Processing tree 126/500...\n",
      "Processing tree 127/500...\n",
      "Processing tree 128/500...\n",
      "Processing tree 129/500...\n",
      "Processing tree 130/500...\n",
      "Processing tree 131/500...\n",
      "Processing tree 132/500...\n",
      "Processing tree 133/500...\n",
      "Processing tree 134/500...\n",
      "Processing tree 135/500...\n",
      "Processing tree 136/500...\n",
      "Processing tree 137/500...\n",
      "Processing tree 138/500...\n",
      "Processing tree 139/500...\n",
      "Processing tree 140/500...\n",
      "Processing tree 141/500...\n",
      "Processing tree 142/500...\n",
      "Processing tree 143/500...\n",
      "Processing tree 144/500...\n",
      "Processing tree 145/500...\n",
      "Processing tree 146/500...\n",
      "Processing tree 147/500...\n",
      "Processing tree 148/500...\n",
      "Processing tree 149/500...\n",
      "Processing tree 150/500...\n",
      "Processing tree 151/500...\n",
      "Processing tree 152/500...\n",
      "Processing tree 153/500...\n",
      "Processing tree 154/500...\n",
      "Processing tree 155/500...\n",
      "Processing tree 156/500...\n",
      "Processing tree 157/500...\n",
      "Processing tree 158/500...\n",
      "Processing tree 159/500...\n",
      "Processing tree 160/500...\n",
      "Processing tree 161/500...\n",
      "Processing tree 162/500...\n",
      "Processing tree 163/500...\n",
      "Processing tree 164/500...\n",
      "Processing tree 165/500...\n",
      "Processing tree 166/500...\n",
      "Processing tree 167/500...\n",
      "Processing tree 168/500...\n",
      "Processing tree 169/500...\n",
      "Processing tree 170/500...\n",
      "Processing tree 171/500...\n",
      "Processing tree 172/500...\n",
      "Processing tree 173/500...\n",
      "Processing tree 174/500...\n",
      "Processing tree 175/500...\n",
      "Processing tree 176/500...\n",
      "Processing tree 177/500...\n",
      "Processing tree 178/500...\n",
      "Processing tree 179/500...\n",
      "Processing tree 180/500...\n",
      "Processing tree 181/500...\n",
      "Processing tree 182/500...\n",
      "Processing tree 183/500...\n",
      "Processing tree 184/500...\n",
      "Processing tree 185/500...\n",
      "Processing tree 186/500...\n",
      "Processing tree 187/500...\n",
      "Processing tree 188/500...\n",
      "Processing tree 189/500...\n",
      "Processing tree 190/500...\n",
      "Processing tree 191/500...\n",
      "Processing tree 192/500...\n",
      "Processing tree 193/500...\n",
      "Processing tree 194/500...\n",
      "Processing tree 195/500...\n",
      "Processing tree 196/500...\n",
      "Processing tree 197/500...\n",
      "Processing tree 198/500...\n",
      "Processing tree 199/500...\n",
      "Processing tree 200/500...\n",
      "Processing tree 201/500...\n",
      "Processing tree 202/500...\n",
      "Processing tree 203/500...\n",
      "Processing tree 204/500...\n",
      "Processing tree 205/500...\n",
      "Processing tree 206/500...\n",
      "Processing tree 207/500...\n",
      "Processing tree 208/500...\n",
      "Processing tree 209/500...\n",
      "Processing tree 210/500...\n",
      "Processing tree 211/500...\n",
      "Processing tree 212/500...\n",
      "Processing tree 213/500...\n",
      "Processing tree 214/500...\n",
      "Processing tree 215/500...\n",
      "Processing tree 216/500...\n",
      "Processing tree 217/500...\n",
      "Processing tree 218/500...\n",
      "Processing tree 219/500...\n",
      "Processing tree 220/500...\n",
      "Processing tree 221/500...\n",
      "Processing tree 222/500...\n",
      "Processing tree 223/500...\n",
      "Processing tree 224/500...\n",
      "Processing tree 225/500...\n",
      "Processing tree 226/500...\n",
      "Processing tree 227/500...\n",
      "Processing tree 228/500...\n",
      "Processing tree 229/500...\n",
      "Processing tree 230/500...\n",
      "Processing tree 231/500...\n",
      "Processing tree 232/500...\n",
      "Processing tree 233/500...\n",
      "Processing tree 234/500...\n",
      "Processing tree 235/500...\n",
      "Processing tree 236/500...\n",
      "Processing tree 237/500...\n",
      "Processing tree 238/500...\n",
      "Processing tree 239/500...\n",
      "Processing tree 240/500...\n",
      "Processing tree 241/500...\n",
      "Processing tree 242/500...\n",
      "Processing tree 243/500...\n",
      "Processing tree 244/500...\n",
      "Processing tree 245/500...\n",
      "Processing tree 246/500...\n",
      "Processing tree 247/500...\n",
      "Processing tree 248/500...\n",
      "Processing tree 249/500...\n",
      "Processing tree 250/500...\n",
      "Processing tree 251/500...\n",
      "Processing tree 252/500...\n",
      "Processing tree 253/500...\n",
      "Processing tree 254/500...\n",
      "Processing tree 255/500...\n",
      "Processing tree 256/500...\n",
      "Processing tree 257/500...\n",
      "Processing tree 258/500...\n",
      "Processing tree 259/500...\n",
      "Processing tree 260/500...\n",
      "Processing tree 261/500...\n",
      "Processing tree 262/500...\n",
      "Processing tree 263/500...\n",
      "Processing tree 264/500...\n",
      "Processing tree 265/500...\n",
      "Processing tree 266/500...\n",
      "Processing tree 267/500...\n",
      "Processing tree 268/500...\n",
      "Processing tree 269/500...\n",
      "Processing tree 270/500...\n",
      "Processing tree 271/500...\n",
      "Processing tree 272/500...\n",
      "Processing tree 273/500...\n",
      "Processing tree 274/500...\n",
      "Processing tree 275/500...\n",
      "Processing tree 276/500...\n",
      "Processing tree 277/500...\n",
      "Processing tree 278/500...\n",
      "Processing tree 279/500...\n",
      "Processing tree 280/500...\n",
      "Processing tree 281/500...\n",
      "Processing tree 282/500...\n",
      "Processing tree 283/500...\n",
      "Processing tree 284/500...\n",
      "Processing tree 285/500...\n",
      "Processing tree 286/500...\n",
      "Processing tree 287/500...\n",
      "Processing tree 288/500...\n",
      "Processing tree 289/500...\n",
      "Processing tree 290/500...\n",
      "Processing tree 291/500...\n",
      "Processing tree 292/500...\n",
      "Processing tree 293/500...\n",
      "Processing tree 294/500...\n",
      "Processing tree 295/500...\n",
      "Processing tree 296/500...\n",
      "Processing tree 297/500...\n",
      "Processing tree 298/500...\n",
      "Processing tree 299/500...\n",
      "Processing tree 300/500...\n",
      "Processing tree 301/500...\n",
      "Processing tree 302/500...\n",
      "Processing tree 303/500...\n",
      "Processing tree 304/500...\n",
      "Processing tree 305/500...\n",
      "Processing tree 306/500...\n",
      "Processing tree 307/500...\n",
      "Processing tree 308/500...\n",
      "Processing tree 309/500...\n",
      "Processing tree 310/500...\n",
      "Processing tree 311/500...\n",
      "Processing tree 312/500...\n",
      "Processing tree 313/500...\n",
      "Processing tree 314/500...\n",
      "Processing tree 315/500...\n",
      "Processing tree 316/500...\n",
      "Processing tree 317/500...\n",
      "Processing tree 318/500...\n",
      "Processing tree 319/500...\n",
      "Processing tree 320/500...\n",
      "Processing tree 321/500...\n",
      "Processing tree 322/500...\n",
      "Processing tree 323/500...\n",
      "Processing tree 324/500...\n",
      "Processing tree 325/500...\n",
      "Processing tree 326/500...\n",
      "Processing tree 327/500...\n",
      "Processing tree 328/500...\n",
      "Processing tree 329/500...\n",
      "Processing tree 330/500...\n",
      "Processing tree 331/500...\n",
      "Processing tree 332/500...\n",
      "Processing tree 333/500...\n",
      "Processing tree 334/500...\n",
      "Processing tree 335/500...\n",
      "Processing tree 336/500...\n",
      "Processing tree 337/500...\n",
      "Processing tree 338/500...\n",
      "Processing tree 339/500...\n",
      "Processing tree 340/500...\n",
      "Processing tree 341/500...\n",
      "Processing tree 342/500...\n",
      "Processing tree 343/500...\n",
      "Processing tree 344/500...\n",
      "Processing tree 345/500...\n",
      "Processing tree 346/500...\n",
      "Processing tree 347/500...\n",
      "Processing tree 348/500...\n",
      "Processing tree 349/500...\n",
      "Processing tree 350/500...\n",
      "Processing tree 351/500...\n",
      "Processing tree 352/500...\n",
      "Processing tree 353/500...\n",
      "Processing tree 354/500...\n",
      "Processing tree 355/500...\n",
      "Processing tree 356/500...\n",
      "Processing tree 357/500...\n",
      "Processing tree 358/500...\n",
      "Processing tree 359/500...\n",
      "Processing tree 360/500...\n",
      "Processing tree 361/500...\n",
      "Processing tree 362/500...\n",
      "Processing tree 363/500...\n",
      "Processing tree 364/500...\n",
      "Processing tree 365/500...\n",
      "Processing tree 366/500...\n",
      "Processing tree 367/500...\n",
      "Processing tree 368/500...\n",
      "Processing tree 369/500...\n",
      "Processing tree 370/500...\n",
      "Processing tree 371/500...\n",
      "Processing tree 372/500...\n",
      "Processing tree 373/500...\n",
      "Processing tree 374/500...\n",
      "Processing tree 375/500...\n",
      "Processing tree 376/500...\n",
      "Processing tree 377/500...\n",
      "Processing tree 378/500...\n",
      "Processing tree 379/500...\n",
      "Processing tree 380/500...\n",
      "Processing tree 381/500...\n",
      "Processing tree 382/500...\n",
      "Processing tree 383/500...\n",
      "Processing tree 384/500...\n",
      "Processing tree 385/500...\n",
      "Processing tree 386/500...\n",
      "Processing tree 387/500...\n",
      "Processing tree 388/500...\n",
      "Processing tree 389/500...\n",
      "Processing tree 390/500...\n",
      "Processing tree 391/500...\n",
      "Processing tree 392/500...\n",
      "Processing tree 393/500...\n",
      "Processing tree 394/500...\n",
      "Processing tree 395/500...\n",
      "Processing tree 396/500...\n",
      "Processing tree 397/500...\n",
      "Processing tree 398/500...\n",
      "Processing tree 399/500...\n",
      "Processing tree 400/500...\n",
      "Processing tree 401/500...\n",
      "Processing tree 402/500...\n",
      "Processing tree 403/500...\n",
      "Processing tree 404/500...\n",
      "Processing tree 405/500...\n",
      "Processing tree 406/500...\n",
      "Processing tree 407/500...\n",
      "Processing tree 408/500...\n",
      "Processing tree 409/500...\n",
      "Processing tree 410/500...\n",
      "Processing tree 411/500...\n",
      "Processing tree 412/500...\n",
      "Processing tree 413/500...\n",
      "Processing tree 414/500...\n",
      "Processing tree 415/500...\n",
      "Processing tree 416/500...\n",
      "Processing tree 417/500...\n",
      "Processing tree 418/500...\n",
      "Processing tree 419/500...\n",
      "Processing tree 420/500...\n",
      "Processing tree 421/500...\n",
      "Processing tree 422/500...\n",
      "Processing tree 423/500...\n",
      "Processing tree 424/500...\n",
      "Processing tree 425/500...\n",
      "Processing tree 426/500...\n",
      "Processing tree 427/500...\n",
      "Processing tree 428/500...\n",
      "Processing tree 429/500...\n",
      "Processing tree 430/500...\n",
      "Processing tree 431/500...\n",
      "Processing tree 432/500...\n",
      "Processing tree 433/500...\n",
      "Processing tree 434/500...\n",
      "Processing tree 435/500...\n",
      "Processing tree 436/500...\n",
      "Processing tree 437/500...\n",
      "Processing tree 438/500...\n",
      "Processing tree 439/500...\n",
      "Processing tree 440/500...\n",
      "Processing tree 441/500...\n",
      "Processing tree 442/500...\n",
      "Processing tree 443/500...\n",
      "Processing tree 444/500...\n",
      "Processing tree 445/500...\n",
      "Processing tree 446/500...\n",
      "Processing tree 447/500...\n",
      "Processing tree 448/500...\n",
      "Processing tree 449/500...\n",
      "Processing tree 450/500...\n",
      "Processing tree 451/500...\n",
      "Processing tree 452/500...\n",
      "Processing tree 453/500...\n",
      "Processing tree 454/500...\n",
      "Processing tree 455/500...\n",
      "Processing tree 456/500...\n",
      "Processing tree 457/500...\n",
      "Processing tree 458/500...\n",
      "Processing tree 459/500...\n",
      "Processing tree 460/500...\n",
      "Processing tree 461/500...\n",
      "Processing tree 462/500...\n",
      "Processing tree 463/500...\n",
      "Processing tree 464/500...\n",
      "Processing tree 465/500...\n",
      "Processing tree 466/500...\n",
      "Processing tree 467/500...\n",
      "Processing tree 468/500...\n",
      "Processing tree 469/500...\n",
      "Processing tree 470/500...\n",
      "Processing tree 471/500...\n",
      "Processing tree 472/500...\n",
      "Processing tree 473/500...\n",
      "Processing tree 474/500...\n",
      "Processing tree 475/500...\n",
      "Processing tree 476/500...\n",
      "Processing tree 477/500...\n",
      "Processing tree 478/500...\n",
      "Processing tree 479/500...\n",
      "Processing tree 480/500...\n",
      "Processing tree 481/500...\n",
      "Processing tree 482/500...\n",
      "Processing tree 483/500...\n",
      "Processing tree 484/500...\n",
      "Processing tree 485/500...\n",
      "Processing tree 486/500...\n",
      "Processing tree 487/500...\n",
      "Processing tree 488/500...\n",
      "Processing tree 489/500...\n",
      "Processing tree 490/500...\n",
      "Processing tree 491/500...\n",
      "Processing tree 492/500...\n",
      "Processing tree 493/500...\n",
      "Processing tree 494/500...\n",
      "Processing tree 495/500...\n",
      "Processing tree 496/500...\n",
      "Processing tree 497/500...\n",
      "Processing tree 498/500...\n",
      "Processing tree 499/500...\n",
      "Processing tree 500/500...\n",
      "Decision trees saved as 'catboost_tree_run.txt'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Feature dependencies\n",
    "all_feature_dependencies = defaultdict(list)\n",
    "\n",
    "# Prepare feature names\n",
    "feature_names = df_selected.drop(columns=['Status_Binary']).columns.tolist()\n",
    "\n",
    "# Train CatBoost model\n",
    "print(\"\\nTraining CatBoost Model - Single Run\")\n",
    "\n",
    "cat_model = CatBoostClassifier(\n",
    "    depth=10, learning_rate=0.1, iterations=500,\n",
    "    random_seed=42, verbose=0\n",
    ")\n",
    "\n",
    "cat_model.fit(X, y)\n",
    "\n",
    "# **ðŸ”¹ Extract tree dump (all trees)**\n",
    "model_file = \"catboost_model.json\"\n",
    "cat_model.save_model(model_file, format=\"json\")\n",
    "\n",
    "# Load JSON model\n",
    "with open(model_file, \"r\") as file:\n",
    "    model_data = json.load(file)\n",
    "\n",
    "formatted_trees = []\n",
    "\n",
    "# **ðŸ”¹ Process each tree separately**\n",
    "for tree_index, tree in enumerate(model_data[\"oblivious_trees\"]):\n",
    "    formatted_tree = []\n",
    "    indent = \"    \"  # Indentation for formatting\n",
    "\n",
    "    print(f\"Processing tree {tree_index + 1}/{len(model_data['oblivious_trees'])}...\")\n",
    "\n",
    "    for split_index, split in enumerate(tree[\"splits\"]):\n",
    "        feature_index = split[\"float_feature_index\"]  # Feature index used in split\n",
    "        threshold = split[\"border\"]  # Threshold value for splitting\n",
    "        feature_name = feature_names[feature_index]  # Map index to actual feature name\n",
    "\n",
    "        # Format tree structure like XGBoost output\n",
    "        condition = f\"f{feature_index}<{threshold}\"\n",
    "        formatted_condition = f\"{feature_name} < {threshold}\"\n",
    "        formatted_line = f\"|--- Split {split_index}: {formatted_condition}\"\n",
    "        \n",
    "        formatted_tree.append(indent + formatted_line)\n",
    "\n",
    "    # **ðŸ”¹ Convert leaf values into class labels**\n",
    "    leaf_values = tree[\"leaf_values\"]\n",
    "    for leaf_index, value in enumerate(leaf_values):\n",
    "        logit = float(value)\n",
    "        probability = 1 / (1 + np.exp(-logit))  # Sigmoid function\n",
    "        predicted_class = 1 if probability > 0.5 else 0\n",
    "        formatted_tree.append(f\"{indent}|--- Leaf {leaf_index}: class {predicted_class}\")\n",
    "\n",
    "    formatted_trees.append(\"\\n\".join(formatted_tree))\n",
    "\n",
    "# **ðŸ”¹ Save formatted trees**\n",
    "output_file = \"catboost_tree_run.txt\"\n",
    "with open(output_file, \"w\") as file:\n",
    "    file.write(\"\\n\\n\".join(formatted_trees))\n",
    "\n",
    "print(f\"Decision trees saved as '{output_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updated non-renewal thresholds saved to 'non_renewal_thresholds_fixed_cat.txt'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load the tree rules from the file\n",
    "file_path = \"catboost_tree_run.txt\"\n",
    "\n",
    "with open(file_path, \"r\") as file:\n",
    "    tree_rules = file.readlines()\n",
    "\n",
    "# ðŸ”¹ Extract feature names dynamically from df_selected\n",
    "df_selected_columns = df_selected.columns.tolist()\n",
    "\n",
    "# Ensure exact feature name matching\n",
    "def match_feature_name(extracted_name):\n",
    "    for col in df_selected_columns:\n",
    "        if extracted_name.strip().lower() in col.lower():  \n",
    "            return col  \n",
    "    return extracted_name  \n",
    "\n",
    "# Dictionary to store feature dependencies and their frequency\n",
    "feature_threshold_counts = defaultdict(lambda: {\"<\": defaultdict(int), \">=\": defaultdict(int)})\n",
    "\n",
    "# Parse tree and extract rules leading to `class: 1`\n",
    "def extract_non_renewal_conditions(tree_rules):\n",
    "    current_path = []  # Track active decision path\n",
    "\n",
    "    for line in tree_rules:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        depth = line.count(\"|---\")\n",
    "        line_content = line.replace(\"|--- \", \"\").strip()\n",
    "\n",
    "        # Handle decision nodes (conditions)\n",
    "        match = re.search(r\"([A-Za-z0-9_ /-]+) (<|>=) ([0-9\\.]+)\", line_content)\n",
    "        if match:\n",
    "            feature, operator, threshold = match.groups()\n",
    "            threshold = float(threshold)\n",
    "\n",
    "            # Ensure full feature names\n",
    "            feature = match_feature_name(feature)\n",
    "\n",
    "            current_path = current_path[:depth]  # Trim path to current depth\n",
    "            current_path.append((feature, operator, threshold))  # Add new condition\n",
    "\n",
    "        # Handle leaf nodes (class: 1)\n",
    "        elif \"class: 1\" in line_content:\n",
    "            for feature, operator, threshold in current_path:\n",
    "                feature_threshold_counts[feature][operator][threshold] += 1\n",
    "\n",
    "    return feature_threshold_counts\n",
    "\n",
    "# Extract dependencies and their frequency\n",
    "feature_threshold_counts = extract_non_renewal_conditions(tree_rules)\n",
    "\n",
    "# Save reasons\n",
    "output_file = \"non_renewal_thresholds_fixed_cat.txt\"\n",
    "\n",
    "with open(output_file, \"w\") as file:\n",
    "    file.write(\"Key Non-Renewal Conditions (Auto-Extracted Feature Names):\\n\")\n",
    "    file.write(\"=\" * 100 + \"\\n\\n\")\n",
    "\n",
    "    for feature, threshold_data in feature_threshold_counts.items():\n",
    "        most_common_less_than = max(threshold_data[\"<\"], key=threshold_data[\"<\"].get, default=None)\n",
    "        most_common_greater_than = max(threshold_data[\">=\"], key=threshold_data[\">=\"].get, default=None)\n",
    "\n",
    "        less_than_count = threshold_data[\"<\"].get(most_common_less_than, 0)\n",
    "        greater_than_count = threshold_data[\">=\"].get(most_common_greater_than, 0)\n",
    "\n",
    "        file.write(f\" **{feature}** influences non-renewal due to:\\n\")\n",
    "\n",
    "        if most_common_less_than is not None:\n",
    "            file.write(f\"   - {feature} < {most_common_less_than} (Count: {less_than_count})\\n\")\n",
    "\n",
    "        if most_common_greater_than is not None:\n",
    "            file.write(f\"   - {feature} >= {most_common_greater_than} (Count: {greater_than_count})\\n\")\n",
    "\n",
    "        file.write(\"-\" * 100 + \"\\n\")\n",
    "\n",
    "print(f\"\\nUpdated non-renewal thresholds saved to '{output_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load the tree rules from the file\n",
    "file_path = \"catboost_tree_run.txt\"\n",
    "\n",
    "with open(file_path, \"r\") as file:\n",
    "    tree_rules = file.readlines()\n",
    "\n",
    "# Automatically get feature names from dataset\n",
    "df_selected_columns = [\n",
    "    \"Renewal Rate Status_Null\", \"age\", \"gst\", \"Tie Up_HYUNDAI\", \"Before GST Add-on GWP\", \"Vehicle IDV\",\n",
    "    \"Total OD Premium\", \"biztype_Roll Over\", \"Policy Tenure\", \"Applicable Discount with NCB\", \"Tie Up_MARUTI\",\n",
    "    \"Total TP Premium\", \"Approved\", \"Renewal Type_TY Onwards\", \"Renewal Type_SY onwards\", \"Product name _PrivateCarPolicy\",\n",
    "    \"biztype_Renewal Business\", \"Tie Up_MIBL OEM\", \"Tie Up_Non-OEM\", \"Tie Up_TATA PV\",\n",
    "    \"Product name _Standalone Own Damage Policy for Private Car\", \"Product name _Private Car Policy - Bundled Cover\",\n",
    "    \"Tie Up_EM Non OE Dealership\", \"New Customers_Yes\", \"Number of claims\", \"Customer Tenure\", \"Total Premium Payable\",\n",
    "    \"Tie Up_FORD\", \"Tie Up_HONDA PV\", \"Tie Up_SKODA\", \"Tie Up_VOLKSWAGEN\", \"Denied\", \"WITHDRAWN\",\n",
    "    \"Tie Up_EM Broker\", \"Claim Happened/Not_Yes\", \"NCB % Previous Year\", \"OEM\", \"Renewal Rate Status_Increase\",\n",
    "    \"Renewal Type_SY\", \"Not_Yes\", \"Tie Up_MIBL Others\"\n",
    "]  \n",
    "\n",
    "# Ensure exact feature name matching\n",
    "def match_feature_name(extracted_name):\n",
    "    for col in df_selected_columns:\n",
    "        if extracted_name in col:  \n",
    "            return col  \n",
    "    return extracted_name  \n",
    "\n",
    "# Dictionary to store feature dependencies and their frequency\n",
    "feature_threshold_counts = defaultdict(lambda: {\"<\": defaultdict(int), \">=\": defaultdict(int)})\n",
    "\n",
    "# Parse tree and extract rules leading to `class: 1`\n",
    "def extract_non_renewal_conditions(tree_rules):\n",
    "    current_path = []  \n",
    "\n",
    "    for line in tree_rules:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        depth = line.count(\"|---\")\n",
    "        line_content = line.replace(\"|--- \", \"\").strip()\n",
    "\n",
    "        # Handle decision nodes (conditions)\n",
    "        match = re.search(r\"([A-Za-z0-9_ /-]+) (<|>) ([0-9\\.]+) yes=(\\d+),no=(\\d+)\", line_content)\n",
    "        if match:\n",
    "            feature, operator, threshold, yes_branch, no_branch = match.groups()\n",
    "            threshold = float(threshold)\n",
    "\n",
    "            # Ensure full feature names\n",
    "            feature = match_feature_name(feature)\n",
    "\n",
    "            current_path.append((depth, feature, operator, threshold, yes_branch, no_branch))\n",
    "\n",
    "        # Handle leaf nodes (class 1)\n",
    "        elif \"class: 1\" in line_content:\n",
    "            for depth, feature, operator, threshold, yes_branch, no_branch in current_path:\n",
    "                if str(yes_branch) in line:  # `<` condition\n",
    "                    feature_threshold_counts[feature][\"<\"][threshold] += 1\n",
    "                elif str(no_branch) in line:  # `>=` condition\n",
    "                    feature_threshold_counts[feature][\">=\"][threshold] += 1\n",
    "\n",
    "    return feature_threshold_counts\n",
    "\n",
    "# Extract dependencies and their frequency\n",
    "feature_threshold_counts = extract_non_renewal_conditions(tree_rules)\n",
    "\n",
    "# Save reasons\n",
    "output_file = \"non_renewal_thresholds_fixed.txt\"\n",
    "\n",
    "with open(output_file, \"w\") as file:\n",
    "    file.write(\"Key Non-Renewal Conditions (Full Feature Names Fixed):\\n\")\n",
    "    file.write(\"=\" * 100 + \"\\n\\n\")\n",
    "\n",
    "    for feature, threshold_data in feature_threshold_counts.items():\n",
    "        most_common_less_than = max(threshold_data[\"<\"], key=threshold_data[\"<\"].get, default=None)\n",
    "        most_common_greater_than = max(threshold_data[\">=\"], key=threshold_data[\">=\"].get, default=None)\n",
    "\n",
    "        less_than_count = threshold_data[\"<\"].get(most_common_less_than, 0)\n",
    "        greater_than_count = threshold_data[\">=\"].get(most_common_greater_than, 0)\n",
    "\n",
    "        file.write(f\" **{feature}** influences non-renewal due to:\\n\")\n",
    "\n",
    "        if most_common_less_than is not None:\n",
    "            dominant_marker = \"\" if less_than_count > greater_than_count else \"\"\n",
    "            file.write(f\"   {dominant_marker}- {feature} < {most_common_less_than} (Count: {less_than_count})\\n\")\n",
    "\n",
    "        if most_common_greater_than is not None:\n",
    "            dominant_marker = \"\" if greater_than_count > less_than_count else \"\"\n",
    "            file.write(f\"   {dominant_marker}- {feature} >= {most_common_greater_than} (Count: {greater_than_count})\\n\")\n",
    "\n",
    "        file.write(\"-\" * 100 + \"\\n\")\n",
    "\n",
    "print(f\"\\nUpdated non-renewal thresholds saved to '{output_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updated non-renewal thresholds saved to 'non_renewal_thresholds_fixed.txt'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load the tree rules from the file\n",
    "file_path = \"catboost_tree_run.txt\"\n",
    "\n",
    "with open(file_path, \"r\") as file:\n",
    "    tree_rules = file.readlines()\n",
    "\n",
    "# Extract feature names dynamically from df_selected\n",
    "df_selected_columns = df_selected.columns.tolist()\n",
    "\n",
    "# ðŸ”¹ Ensure exact feature name matching\n",
    "def match_feature_name(extracted_name):\n",
    "    for col in df_selected_columns:\n",
    "        if extracted_name.strip().lower() in col.lower():  \n",
    "            return col  \n",
    "    return extracted_name  # Return original if no match found\n",
    "\n",
    "# ðŸ”¹ Dictionary to store feature dependencies and their frequency\n",
    "feature_threshold_counts = defaultdict(lambda: {\"<\": defaultdict(int), \">=\": defaultdict(int)})\n",
    "\n",
    "# ðŸ”¹ Parse tree and extract rules leading to `class: 1`\n",
    "def extract_non_renewal_conditions(tree_rules):\n",
    "    current_path = []  # Track active decision path\n",
    "    leaf_class_1_found = False  # Flag to track when a path leads to `class: 1`\n",
    "\n",
    "    for line in tree_rules:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        depth = line.count(\"|---\")\n",
    "        line_content = line.replace(\"|--- \", \"\").strip()\n",
    "\n",
    "        # ðŸ”¹ Handle decision nodes (conditions)\n",
    "        match = re.search(r\"([A-Za-z0-9_ /-]+) (<|>=) ([0-9\\.]+)\", line_content)\n",
    "        if match:\n",
    "            feature, operator, threshold = match.groups()\n",
    "            threshold = float(threshold)\n",
    "\n",
    "            # Ensure full feature names\n",
    "            feature = match_feature_name(feature)\n",
    "\n",
    "            # Trim path if going back up in tree depth\n",
    "            current_path = current_path[:depth]  \n",
    "            current_path.append((feature, operator, threshold))  # Add new condition\n",
    "\n",
    "        # ðŸ”¹ Handle leaf nodes (class: 1)\n",
    "        elif \"class: 1\" in line_content:\n",
    "            leaf_class_1_found = True  # Mark that we found a valid class: 1 leaf\n",
    "\n",
    "            for feature, operator, threshold in current_path:\n",
    "                feature_threshold_counts[feature][operator][threshold] += 1\n",
    "\n",
    "    return feature_threshold_counts, leaf_class_1_found\n",
    "\n",
    "# ðŸ”¹ Extract dependencies and their frequency\n",
    "feature_threshold_counts, leaf_class_1_found = extract_non_renewal_conditions(tree_rules)\n",
    "\n",
    "# ðŸ”¹ Save results\n",
    "output_file = \"non_renewal_thresholds_fixed.txt\"\n",
    "\n",
    "with open(output_file, \"w\") as file:\n",
    "    file.write(\"Key Non-Renewal Conditions (Auto-Extracted Feature Names):\\n\")\n",
    "    file.write(\"=\" * 100 + \"\\n\\n\")\n",
    "\n",
    "    if not leaf_class_1_found:\n",
    "        file.write(\"No significant non-renewal conditions found. Check if `class: 1` exists in the tree.\\n\")\n",
    "    else:\n",
    "        for feature, threshold_data in feature_threshold_counts.items():\n",
    "            most_common_less_than = max(threshold_data[\"<\"], key=threshold_data[\"<\"].get, default=None)\n",
    "            most_common_greater_than = max(threshold_data[\">=\"], key=threshold_data[\">=\"].get, default=None)\n",
    "\n",
    "            less_than_count = threshold_data[\"<\"].get(most_common_less_than, 0)\n",
    "            greater_than_count = threshold_data[\">=\"].get(most_common_greater_than, 0)\n",
    "\n",
    "            file.write(f\" **{feature}** influences non-renewal due to:\\n\")\n",
    "\n",
    "            if most_common_less_than is not None:\n",
    "                file.write(f\"   - {feature} < {most_common_less_than} (Count: {less_than_count})\\n\")\n",
    "\n",
    "            if most_common_greater_than is not None:\n",
    "                file.write(f\"   - {feature} >= {most_common_greater_than} (Count: {greater_than_count})\\n\")\n",
    "\n",
    "            file.write(\"-\" * 100 + \"\\n\")\n",
    "\n",
    "print(f\"\\nUpdated non-renewal thresholds saved to '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updated non-renewal thresholds saved to 'non_renewal_thresholds_fixed.txt'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# ðŸ”¹ Load the tree rules from the file\n",
    "file_path = \"catboost_tree_run.txt\"\n",
    "\n",
    "with open(file_path, \"r\") as file:\n",
    "    tree_rules = file.readlines()\n",
    "\n",
    "# ðŸ”¹ Extract feature names dynamically from df_selected\n",
    "df_selected_columns = df_selected.columns.tolist()\n",
    "\n",
    "# ðŸ”¹ Ensure exact feature name matching\n",
    "def match_feature_name(extracted_name):\n",
    "    for col in df_selected_columns:\n",
    "        if extracted_name.strip().lower() in col.lower():  \n",
    "            return col  \n",
    "    return extracted_name  # Return original if no match found\n",
    "\n",
    "# ðŸ”¹ Dictionary to store feature dependencies and their frequency\n",
    "feature_threshold_counts = defaultdict(lambda: {\"<\": defaultdict(int), \">=\": defaultdict(int)})\n",
    "\n",
    "# ðŸ”¹ Parse tree and extract rules leading to `class: 1` (\"Not Renewed\")\n",
    "def extract_non_renewal_conditions(tree_rules):\n",
    "    current_path = []  # Stack to track active decision path\n",
    "\n",
    "    for line in tree_rules:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        depth = line.count(\"|---\")  # Determine depth in the tree\n",
    "        line_content = line.replace(\"|--- \", \"\").strip()\n",
    "\n",
    "        # ðŸ”¹ Handle decision nodes (conditions)\n",
    "        match = re.search(r\"([A-Za-z0-9_ /-]+) (<|>=) ([0-9\\.]+)\", line_content)\n",
    "        if match:\n",
    "            feature, operator, threshold = match.groups()\n",
    "            threshold = float(threshold)\n",
    "\n",
    "            # Ensure full feature names\n",
    "            feature = match_feature_name(feature)\n",
    "\n",
    "            # Trim path if going back up in tree depth\n",
    "            current_path = current_path[:depth]  \n",
    "            current_path.append((feature, operator, threshold))  # Add new condition\n",
    "\n",
    "        # ðŸ”¹ Handle leaf nodes (`class: 1` -> Not Renewed)\n",
    "        elif \"class: 1\" in line_content:\n",
    "            # Record the full decision path leading to `class: 1`\n",
    "            for feature, operator, threshold in current_path:\n",
    "                feature_threshold_counts[feature][operator][threshold] += 1\n",
    "\n",
    "    return feature_threshold_counts\n",
    "\n",
    "# ðŸ”¹ Extract dependencies and their frequency\n",
    "feature_threshold_counts = extract_non_renewal_conditions(tree_rules)\n",
    "\n",
    "# ðŸ”¹ Save results\n",
    "output_file = \"non_renewal_thresholds_fixed.txt\"\n",
    "\n",
    "with open(output_file, \"w\") as file:\n",
    "    file.write(\"Key Non-Renewal Conditions (Auto-Extracted Feature Names):\\n\")\n",
    "    file.write(\"=\" * 100 + \"\\n\\n\")\n",
    "\n",
    "    if not feature_threshold_counts:\n",
    "        file.write(\"No significant non-renewal conditions found. Check if `class: 1` exists in the tree.\\n\")\n",
    "    else:\n",
    "        for feature, threshold_data in feature_threshold_counts.items():\n",
    "            most_common_less_than = max(threshold_data[\"<\"], key=threshold_data[\"<\"].get, default=None)\n",
    "            most_common_greater_than = max(threshold_data[\">=\"], key=threshold_data[\">=\"].get, default=None)\n",
    "\n",
    "            less_than_count = threshold_data[\"<\"].get(most_common_less_than, 0)\n",
    "            greater_than_count = threshold_data[\">=\"].get(most_common_greater_than, 0)\n",
    "\n",
    "            file.write(f\" **{feature}** contributes to non-renewal due to:\\n\")\n",
    "\n",
    "            if most_common_less_than is not None:\n",
    "                file.write(f\"   - {feature} < {most_common_less_than} (Count: {less_than_count})\\n\")\n",
    "\n",
    "            if most_common_greater_than is not None:\n",
    "                file.write(f\"   - {feature} >= {most_common_greater_than} (Count: {greater_than_count})\\n\")\n",
    "\n",
    "            file.write(\"-\" * 100 + \"\\n\")\n",
    "\n",
    "print(f\"\\nUpdated non-renewal thresholds saved to '{output_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updated non-renewal thresholds saved to 'non_renewal_thresholds_fixed.txt'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# ðŸ”¹ Load the tree rules from the file\n",
    "file_path = \"catboost_tree_run.txt\"\n",
    "\n",
    "with open(file_path, \"r\") as file:\n",
    "    tree_rules = file.readlines()\n",
    "\n",
    "# ðŸ”¹ Extract feature names dynamically from df_selected\n",
    "df_selected_columns = df_selected.columns.tolist()\n",
    "\n",
    "# ðŸ”¹ Ensure exact feature name matching\n",
    "def match_feature_name(extracted_name):\n",
    "    extracted_name = extracted_name.strip().lower()\n",
    "\n",
    "    for col in df_selected_columns:\n",
    "        if extracted_name in col.lower():  \n",
    "            return col  # Return full dataset feature name\n",
    "\n",
    "    print(f\"âš  Warning: Feature name `{extracted_name}` not found in dataset!\")\n",
    "    return extracted_name  # Return original if no match found\n",
    "\n",
    "# ðŸ”¹ Dictionary to store feature dependencies and their frequency\n",
    "feature_threshold_counts = defaultdict(lambda: {\"<\": defaultdict(int), \">=\": defaultdict(int)})\n",
    "\n",
    "# ðŸ”¹ Parse tree and extract rules leading to `class: 1` (\"Not Renewed\")\n",
    "def extract_non_renewal_conditions(tree_rules):\n",
    "    current_path = []  # Stack to track active decision path\n",
    "    class_1_found = False  # Flag to check if we detect class: 1\n",
    "\n",
    "    for line in tree_rules:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        depth = line.count(\"|---\")  # Determine depth in the tree\n",
    "        line_content = line.replace(\"|--- \", \"\").strip()\n",
    "\n",
    "        # ðŸ”¹ Handle decision nodes (conditions)\n",
    "        match = re.search(r\"([A-Za-z0-9_ /-]+) (<|>=) ([0-9\\.]+)\", line_content)\n",
    "        if match:\n",
    "            feature, operator, threshold = match.groups()\n",
    "            threshold = float(threshold)\n",
    "\n",
    "            # Ensure full feature names\n",
    "            feature = match_feature_name(feature)\n",
    "\n",
    "            # Trim path if going back up in tree depth\n",
    "            current_path = current_path[:depth]  \n",
    "            current_path.append((feature, operator, threshold))  # Add new condition\n",
    "\n",
    "        # ðŸ”¹ Handle leaf nodes (`class: 1` -> Not Renewed)\n",
    "        elif \"class: 1\" in line_content:\n",
    "            class_1_found = True\n",
    "            print(f\"ðŸš€ Found `class: 1` at depth {depth}, path: {current_path}\")\n",
    "\n",
    "            # Record the full decision path leading to `class: 1`\n",
    "            for feature, operator, threshold in current_path:\n",
    "                feature_threshold_counts[feature][operator][threshold] += 1\n",
    "\n",
    "    return feature_threshold_counts, class_1_found\n",
    "\n",
    "# ðŸ”¹ Extract dependencies and their frequency\n",
    "feature_threshold_counts, class_1_found = extract_non_renewal_conditions(tree_rules)\n",
    "\n",
    "# ðŸ”¹ Save results\n",
    "output_file = \"non_renewal_thresholds_fixed.txt\"\n",
    "\n",
    "with open(output_file, \"w\") as file:\n",
    "    file.write(\"Key Non-Renewal Conditions (Auto-Extracted Feature Names):\\n\")\n",
    "    file.write(\"=\" * 100 + \"\\n\\n\")\n",
    "\n",
    "    if not class_1_found:\n",
    "        file.write(\"No significant non-renewal conditions found. Check if `class: 1` exists in the tree.\\n\")\n",
    "    else:\n",
    "        for feature, threshold_data in feature_threshold_counts.items():\n",
    "            most_common_less_than = max(threshold_data[\"<\"], key=threshold_data[\"<\"].get, default=None)\n",
    "            most_common_greater_than = max(threshold_data[\">=\"], key=threshold_data[\">=\"].get, default=None)\n",
    "\n",
    "            less_than_count = threshold_data[\"<\"].get(most_common_less_than, 0)\n",
    "            greater_than_count = threshold_data[\">=\"].get(most_common_greater_than, 0)\n",
    "\n",
    "            file.write(f\" **{feature}** contributes to non-renewal due to:\\n\")\n",
    "\n",
    "            if most_common_less_than is not None:\n",
    "                file.write(f\"   - {feature} < {most_common_less_than} (Count: {less_than_count})\\n\")\n",
    "\n",
    "            if most_common_greater_than is not None:\n",
    "                file.write(f\"   - {feature} >= {most_common_greater_than} (Count: {greater_than_count})\\n\")\n",
    "\n",
    "            file.write(\"-\" * 100 + \"\\n\")\n",
    "\n",
    "print(f\"\\nUpdated non-renewal thresholds saved to '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CatBoostClassifier' object has no attribute 'get_tree_dumps'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m cat_model\u001b[38;5;241m.\u001b[39mfit(X, y)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Extract the tree dumps directly (each tree is represented as a string)\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m tree_dumps \u001b[38;5;241m=\u001b[39m \u001b[43mcat_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tree_dumps\u001b[49m()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Optionally, write the tree dumps to a file so you can process them later\u001b[39;00m\n\u001b[0;32m     11\u001b[0m dump_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcatboost_tree_run.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CatBoostClassifier' object has no attribute 'get_tree_dumps'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Train your CatBoostClassifier\n",
    "cat_model.fit(X, y)\n",
    "\n",
    "# Extract the tree dumps directly (each tree is represented as a string)\n",
    "tree_dumps = cat_model.get_tree_dumps()\n",
    "\n",
    "# Optionally, write the tree dumps to a file so you can process them later\n",
    "dump_file = \"catboost_tree_run.txt\"\n",
    "with open(dump_file, \"w\") as f:\n",
    "    for tree in tree_dumps:\n",
    "        f.write(tree + \"\\n\")  # separate trees by newlines\n",
    "\n",
    "# Load the tree rules from the file\n",
    "with open(dump_file, \"r\") as file:\n",
    "    tree_rules = file.readlines()\n",
    "\n",
    "# ðŸ”¹ Extract feature names dynamically from df_selected\n",
    "df_selected_columns = df_selected.columns.tolist()\n",
    "\n",
    "# Ensure exact feature name matching\n",
    "def match_feature_name(extracted_name):\n",
    "    for col in df_selected_columns:\n",
    "        if extracted_name.strip().lower() in col.lower():\n",
    "            return col\n",
    "    return extracted_name\n",
    "\n",
    "# Dictionary to store feature dependencies and their frequency\n",
    "feature_threshold_counts = defaultdict(lambda: {\"<\": defaultdict(int), \">=\": defaultdict(int)})\n",
    "\n",
    "# Parse tree dump and extract rules leading to `class: 1`\n",
    "def extract_non_renewal_conditions(tree_rules):\n",
    "    current_path = []  # Track active decision path\n",
    "    for line in tree_rules:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        depth = line.count(\"|---\")\n",
    "        line_content = line.replace(\"|--- \", \"\").strip()\n",
    "\n",
    "        # Handle decision nodes (conditions)\n",
    "        match = re.search(r\"([A-Za-z0-9_ /-]+) (<|>=) ([0-9\\.]+)\", line_content)\n",
    "        if match:\n",
    "            feature, operator, threshold = match.groups()\n",
    "            threshold = float(threshold)\n",
    "\n",
    "            # Ensure full feature names\n",
    "            feature = match_feature_name(feature)\n",
    "\n",
    "            current_path = current_path[:depth]  # Trim path to current depth\n",
    "            current_path.append((feature, operator, threshold))  # Add new condition\n",
    "\n",
    "        # Handle leaf nodes (e.g., indicating class: 1)\n",
    "        elif \"class: 1\" in line_content:\n",
    "            for feature, operator, threshold in current_path:\n",
    "                feature_threshold_counts[feature][operator][threshold] += 1\n",
    "\n",
    "    return feature_threshold_counts\n",
    "\n",
    "# Extract dependencies and their frequency from the tree dump\n",
    "feature_threshold_counts = extract_non_renewal_conditions(tree_rules)\n",
    "\n",
    "# Save the extracted rules to a file\n",
    "output_file = \"non_renewal_thresholds_fixed_cat.txt\"\n",
    "with open(output_file, \"w\") as file:\n",
    "    file.write(\"Key Non-Renewal Conditions (Auto-Extracted Feature Names):\\n\")\n",
    "    file.write(\"=\" * 100 + \"\\n\\n\")\n",
    "\n",
    "    for feature, threshold_data in feature_threshold_counts.items():\n",
    "        most_common_less_than = max(threshold_data[\"<\"], key=threshold_data[\"<\"].get, default=None)\n",
    "        most_common_greater_than = max(threshold_data[\">=\"], key=threshold_data[\">=\"].get, default=None)\n",
    "\n",
    "        less_than_count = threshold_data[\"<\"].get(most_common_less_than, 0)\n",
    "        greater_than_count = threshold_data[\">=\"].get(most_common_greater_than, 0)\n",
    "\n",
    "        file.write(f\" **{feature}** influences non-renewal due to:\\n\")\n",
    "\n",
    "        if most_common_less_than is not None:\n",
    "            file.write(f\"   - {feature} < {most_common_less_than} (Count: {less_than_count})\\n\")\n",
    "\n",
    "        if most_common_greater_than is not None:\n",
    "            file.write(f\"   - {feature} >= {most_common_greater_than} (Count: {greater_than_count})\\n\")\n",
    "\n",
    "        file.write(\"-\" * 100 + \"\\n\")\n",
    "\n",
    "print(f\"\\nUpdated non-renewal thresholds saved to '{output_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.7\n"
     ]
    }
   ],
   "source": [
    "import catboost\n",
    "print(catboost.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training CatBoost Model - Single Run\n",
      "\n",
      "Decision paths saved to 'decision_paths_by_class.txt'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Prepare feature names from your DataFrame (excluding target 'Status_Binary')\n",
    "feature_names = df_selected.drop(columns=['Status_Binary']).columns.tolist()\n",
    "\n",
    "# Train CatBoost model\n",
    "print(\"\\nTraining CatBoost Model - Single Run\")\n",
    "cat_model = CatBoostClassifier(\n",
    "    depth=10, learning_rate=0.1, iterations=500,\n",
    "    random_seed=42, verbose=0\n",
    ")\n",
    "cat_model.fit(X, y)\n",
    "\n",
    "# Save the model in JSON format\n",
    "model_file = \"catboost_model.json\"\n",
    "cat_model.save_model(model_file, format=\"json\")\n",
    "\n",
    "# Load the JSON model\n",
    "with open(model_file, \"r\") as file:\n",
    "    model_data = json.load(file)\n",
    "\n",
    "# Dictionaries to store the decision paths (\"reasons\") for each class\n",
    "reasons_for_class = {0: [], 1: []}\n",
    "\n",
    "# Process each tree from the JSON model\n",
    "for tree_index, tree in enumerate(model_data[\"oblivious_trees\"]):\n",
    "    splits = tree[\"splits\"]       # List of splits for this tree\n",
    "    num_splits = len(splits)      # d splits, so 2^d leaves\n",
    "    num_leaves = len(tree[\"leaf_values\"])\n",
    "    \n",
    "    # For each leaf in the tree, reconstruct the decision path\n",
    "    for leaf_index in range(num_leaves):\n",
    "        # Create a binary representation of the leaf index, padded to the number of splits\n",
    "        path_bin = format(leaf_index, f\"0{num_splits}b\")\n",
    "        conditions = []\n",
    "        \n",
    "        # Build conditions based on each bit in the path\n",
    "        for j, bit in enumerate(path_bin):\n",
    "            feature_index = splits[j][\"float_feature_index\"]\n",
    "            threshold = splits[j][\"border\"]\n",
    "            feature_name = feature_names[feature_index]\n",
    "            if bit == '0':\n",
    "                # Left branch: condition satisfied if feature < threshold\n",
    "                conditions.append(f\"{feature_name} < {threshold}\")\n",
    "            else:\n",
    "                # Right branch: condition satisfied if feature >= threshold\n",
    "                conditions.append(f\"{feature_name} >= {threshold}\")\n",
    "                \n",
    "        # Compute predicted probability for this leaf using the sigmoid of the logit\n",
    "        logit = float(tree[\"leaf_values\"][leaf_index])\n",
    "        probability = 1 / (1 + np.exp(-logit))\n",
    "        predicted_class = 1 if probability > 0.5 else 0\n",
    "        \n",
    "        # Combine the conditions into one readable decision path\n",
    "        decision_path = \" AND \".join(conditions)\n",
    "        reasons_for_class[predicted_class].append(decision_path)\n",
    "\n",
    "# Save the decision paths (reasons) to a file\n",
    "output_file = \"decision_paths_by_class.txt\"\n",
    "with open(output_file, \"w\") as file:\n",
    "    file.write(\"Reasons (Decision Paths) leading to Class 1:\\n\")\n",
    "    file.write(\"=\" * 50 + \"\\n\")\n",
    "    for reason in reasons_for_class[1]:\n",
    "        file.write(reason + \"\\n\")\n",
    "        \n",
    "    file.write(\"\\n\\nReasons (Decision Paths) leading to Class 0:\\n\")\n",
    "    file.write(\"=\" * 50 + \"\\n\")\n",
    "    for reason in reasons_for_class[0]:\n",
    "        file.write(reason + \"\\n\")\n",
    "\n",
    "print(f\"\\nDecision paths saved to '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main decision reasons saved to 'main_decision_reasons.txt'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Prepare feature names from your DataFrame (excluding target 'Status_Binary')\n",
    "feature_names = df_selected.drop(columns=['Status_Binary']).columns.tolist()\n",
    "\n",
    "# Train CatBoost model\n",
    "cat_model = CatBoostClassifier(\n",
    "    depth=10, learning_rate=0.1, iterations=500,\n",
    "    random_seed=42, verbose=0\n",
    ")\n",
    "cat_model.fit(X, y)\n",
    "\n",
    "# Save the model in JSON format\n",
    "model_file = \"catboost_model.json\"\n",
    "cat_model.save_model(model_file, format=\"json\")\n",
    "\n",
    "# Load the JSON model\n",
    "with open(model_file, \"r\") as file:\n",
    "    model_data = json.load(file)\n",
    "\n",
    "# Create dictionaries to count condition occurrences for each class\n",
    "condition_counts = {0: defaultdict(int), 1: defaultdict(int)}\n",
    "\n",
    "# Process each tree from the JSON model\n",
    "for tree in model_data[\"oblivious_trees\"]:\n",
    "    splits = tree[\"splits\"]       # List of splits for this tree\n",
    "    num_splits = len(splits)      # d splits, so 2^d leaves\n",
    "    num_leaves = len(tree[\"leaf_values\"])\n",
    "    \n",
    "    # For each leaf, reconstruct its decision path and count conditions\n",
    "    for leaf_index in range(num_leaves):\n",
    "        path_bin = format(leaf_index, f\"0{num_splits}b\")\n",
    "        conditions = []\n",
    "        for j, bit in enumerate(path_bin):\n",
    "            feature_index = splits[j][\"float_feature_index\"]\n",
    "            threshold = splits[j][\"border\"]\n",
    "            feature_name = feature_names[feature_index]\n",
    "            if bit == '0':\n",
    "                condition = f\"{feature_name} < {threshold}\"\n",
    "            else:\n",
    "                condition = f\"{feature_name} >= {threshold}\"\n",
    "            conditions.append(condition)\n",
    "        \n",
    "        # Determine the predicted class for this leaf using sigmoid\n",
    "        logit = float(tree[\"leaf_values\"][leaf_index])\n",
    "        probability = 1 / (1 + np.exp(-logit))\n",
    "        predicted_class = 1 if probability > 0.5 else 0\n",
    "        \n",
    "        # Increase count for each unique condition in the path\n",
    "        # (you can also choose to count combinations if needed)\n",
    "        for cond in set(conditions):\n",
    "            condition_counts[predicted_class][cond] += 1\n",
    "\n",
    "# Now, filter and output the main conditions (for example, top 10 for each class)\n",
    "top_n = 10\n",
    "main_reasons = {}\n",
    "for cls in (0, 1):\n",
    "    sorted_conditions = sorted(condition_counts[cls].items(), key=lambda x: x[1], reverse=True)\n",
    "    main_reasons[cls] = sorted_conditions[:top_n]\n",
    "\n",
    "# Write the main reasons to a file\n",
    "output_file = \"main_decision_reasons.txt\"\n",
    "with open(output_file, \"w\") as file:\n",
    "    for cls in (0, 1):\n",
    "        file.write(f\"Main Conditions for Class {cls}:\\n\")\n",
    "        file.write(\"=\" * 40 + \"\\n\")\n",
    "        for condition, count in main_reasons[cls]:\n",
    "            file.write(f\"{condition}  (Count: {count})\\n\")\n",
    "        file.write(\"\\n\")\n",
    "\n",
    "print(f\"Main decision reasons saved to '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

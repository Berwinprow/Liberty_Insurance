{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_curve, roc_auc_score, classification_report, log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import pandas as pd\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import random\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Database connection setup\n",
    "db_config = {\n",
    "    'host': 'localhost',\n",
    "    'database': 'postgres',\n",
    "    'user': 'postgres',\n",
    "    'password': 'kaviyam123',\n",
    "    'port': '5432'\n",
    "}\n",
    "\n",
    "connection_string = f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['database']}\"\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Step 1: Load data from PostgreSQL\n",
    "query = 'SELECT * FROM public.overall_cleaned_base_and_pr_ef_policyef;'\n",
    "data = pd.read_sql(query, con=engine)\n",
    "\n",
    "selected_columns = ['policy no', 'renewal type', 'product name', 'product name 2',  'biztype', 'policy end date', 'policy start date', \n",
    " 'age', 'manufacturer/make', 'model', 'variant', 'vehicle segment', 'fuel type', 'rto location', 'vehicle idv', 'ncb amount', 'Cleaned Reg no', \n",
    " 'before gst add-on gwp', 'total od premium', 'total tp premium', 'gst', 'total premium payable', \n",
    " 'ncb % previous year', 'applicable discount with ncb', 'Cleaned Branch Name 2', 'Cleaned State2', 'Cleaned Zone 2', 'tie up',\n",
    " 'Number of claims', 'approved', 'denied', 'corrected_name', 'customerid', 'Policy Status', 'Policy Tenure', 'Customer Tenure', 'New Customers', 'Claim Happaned/Not', \n",
    " 'Renewal Rate Status', 'withdrawn', 'chassis_engine_key', 'policy_wise_purchase']\n",
    "\n",
    "data = data[selected_columns]\n",
    "\n",
    "# Remove rows where 'Status' contains 'Open'\n",
    "data = data[data['Policy Status'].isin(['Renewed', 'Not Renewed'])]\n",
    "\n",
    "\n",
    "data['Policy Status'] = data['Policy Status'].apply(lambda x: 1 if x == 'Not Renewed' else 0)\n",
    "\n",
    "for column in data.columns:\n",
    "    if data[column].dtype == 'object':\n",
    "        data[column] = data[column].fillna('missing')\n",
    "    else:\n",
    "        data[column] = data[column].fillna(0)\n",
    "\n",
    "date_columns = ['policy start date', 'policy end date']\n",
    "\n",
    "for col in date_columns:\n",
    "    data[col] = pd.to_datetime(data[col], errors='coerce')\n",
    "\n",
    "# Extract year, month, and day as separate features for all date columns\n",
    "new_date_cols = {}\n",
    "for col in date_columns:\n",
    "    new_date_cols[f'{col}_YEAR'] = data[col].dt.year\n",
    "    new_date_cols[f'{col}_MONTH'] = data[col].dt.month\n",
    "    new_date_cols[f'{col}_DAY'] = data[col].dt.day\n",
    "\n",
    "data = pd.concat([data, pd.DataFrame(new_date_cols)], axis=1)\n",
    "\n",
    "# Drop date columns after splitting\n",
    "data = data.drop(columns=date_columns)\n",
    "\n",
    "# Define features and target\n",
    "features = [col for col in data.columns if col not in ['Policy Status']]\n",
    "\n",
    "# Define X (features) and y (target)\n",
    "X = data[features]\n",
    "y = data['Policy Status']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, log_loss, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "\n",
    "# Apply Random Oversampling to the training data\n",
    "X_train, y_train = ros.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply label encoding to categorical features for both train and test sets\n",
    "for column in X_train.columns:\n",
    "    if X_train[column].dtype == 'object':\n",
    "        # Initialize and fit the LabelEncoder on the training data\n",
    "        label_encoder = LabelEncoder()\n",
    "        X_train[column] = label_encoder.fit_transform(X_train[column].astype(str))\n",
    "\n",
    "        # Create a mapping dictionary from the LabelEncoder\n",
    "        mapping_dict = {label: i for i, label in enumerate(label_encoder.classes_)}\n",
    "        \n",
    "        # Track the next unique integer for unseen values in the test set\n",
    "        next_unique_value = [max(mapping_dict.values()) + 1]  \n",
    "\n",
    "        # Encode the test data\n",
    "        def encode_test_value(value):\n",
    "            if value in mapping_dict:\n",
    "                return mapping_dict[value]\n",
    "            else:\n",
    "                # Update the mapping_dict with a new unique value for unseen categories\n",
    "                mapping_dict[value] = next_unique_value[0]\n",
    "                next_unique_value[0] += 1\n",
    "                return mapping_dict[value]\n",
    "\n",
    "        # Apply the encoding to the test set\n",
    "        X_test[column] = X_test[column].apply(encode_test_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Initialize and train the Decision Tree classifier\n",
    "model = DecisionTreeClassifier(random_state=42, max_depth=5)\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Initialize and train the Decision Tree classifier\n",
    "model = DecisionTreeClassifier(random_state=42, max_depth=10)\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Initialize and train the Decision Tree classifier\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Initialize and train the Decision Tree classifier\n",
    "model = DecisionTreeClassifier(random_state=42, max_depth=40)\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "model = DecisionTreeClassifier(\n",
    "    random_state=42,\n",
    "    max_depth=8,                \n",
    "    min_samples_split=20,       \n",
    "    min_samples_leaf=10,        \n",
    "    max_leaf_nodes=50,          \n",
    "    ccp_alpha=0.01              \n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(random_state=42, max_depth=10)\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=8,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    max_leaf_nodes=50,\n",
    "    class_weight={0: 10, 1: 1},  \n",
    "    random_state=42\n",
    ")\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(random_state=42, max_depth=16)\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(random_state=42, max_depth=17)\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(random_state=42, max_depth=20, class_weight='balanced')\n",
    "\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=8,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    max_leaf_nodes=50,\n",
    "    class_weight={0: 5, 1: 1},  \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Random Forest model\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,             \n",
    "    max_depth=8,                  \n",
    "    min_samples_split=20,         \n",
    "    min_samples_leaf=10,          \n",
    "    max_leaf_nodes=50,            \n",
    "    class_weight='balanced',      \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(\n",
    "    n_estimators=50,  \n",
    "    max_depth=6,     \n",
    "    min_samples_split=50,\n",
    "    min_samples_leaf=20,\n",
    "    max_leaf_nodes=30,\n",
    "    class_weight={0: 5, 1: 1},\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB  \n",
    "from sklearn.metrics import accuracy_score, roc_curve, roc_auc_score, classification_report, log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize and train the Naive Bayes model\n",
    "model = GaussianNB() \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB  \n",
    "from sklearn.metrics import accuracy_score, roc_curve, roc_auc_score, classification_report, log_loss\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# XGBoost model\n",
    "model = xgb.XGBClassifier(\n",
    "    max_depth=6,                  \n",
    "    learning_rate=0.1,            \n",
    "    n_estimators=100,            \n",
    "    scale_pos_weight=len(y_train[y_train == 0]) / len(y_train[y_train == 1]), \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(\n",
    "    max_depth=5,                  \n",
    "    learning_rate=0.05,            \n",
    "    n_estimators=200,              \n",
    "    subsample=0.8,                 \n",
    "    colsample_bytree=0.8,         \n",
    "    scale_pos_weight=len(y_train[y_train == 0]) / len(y_train[y_train == 1]),  \n",
    "    gamma=0.1,                    \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.1,  # L1 regularization\n",
    "    reg_lambda=1.0,  # L2 regularization\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, classification_report, confusion_matrix, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# GBM model\n",
    "model = GradientBoostingClassifier(\n",
    "    max_depth=5,                   \n",
    "    learning_rate=0.05,             \n",
    "    n_estimators=200,               \n",
    "    subsample=0.8,                 \n",
    "    random_state=42                 \n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "model = GradientBoostingClassifier(\n",
    "    max_depth=5,\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=200,\n",
    "    subsample=0.8,\n",
    "    random_state=42,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    warm_start=False,\n",
    "    validation_fraction=0.1,\n",
    "    n_iter_no_change=10,\n",
    "    tol=1e-4\n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, classification_report, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# GBM model\n",
    "model = GradientBoostingClassifier(\n",
    "    max_depth=6,                    \n",
    "    learning_rate=0.1,              \n",
    "    n_estimators=100,              \n",
    "    random_state=42                 \n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, classification_report, confusion_matrix, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# AdaBoost model with a DecisionTreeClassifier as the base estimator\n",
    "model = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=6),  \n",
    "    learning_rate=0.1,                              \n",
    "    n_estimators=100,                               \n",
    "    random_state=42                                 \n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, classification_report, confusion_matrix, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# CatBoost model\n",
    "model = CatBoostClassifier(\n",
    "    depth=6,                      \n",
    "    learning_rate=0.1,            \n",
    "    iterations=100,               \n",
    "    random_seed=42,               \n",
    "    verbose=0                     \n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(\n",
    "    depth=10,                    \n",
    "    learning_rate=0.1,            \n",
    "    iterations=500,               \n",
    "    random_seed=42,               \n",
    "    verbose=0                     \n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, classification_report, confusion_matrix, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# LightGBM model\n",
    "model = LGBMClassifier(\n",
    "    max_depth=6,                     \n",
    "    learning_rate=0.1,               \n",
    "    n_estimators=100,                \n",
    "    random_state=42                  \n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [4, 6, 8, 10],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'gamma': [0, 0.1, 0.2, 0.3],\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "modelxgb = xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    modelxgb,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=50,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best parameters found: \", random_search.best_params_)\n",
    "\n",
    "# Use the best model for prediction\n",
    "model = random_search.best_estimator_\n",
    "\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_curve, roc_auc_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert data to numeric types\n",
    "X_train_numeric = np.array(X_train, dtype=np.float32)  \n",
    "X_test_numeric = np.array(X_test, dtype=np.float32)    \n",
    "y_train_numeric = np.array(y_train, dtype=np.float32)  \n",
    "y_test_numeric = np.array(y_test, dtype=np.float32)    \n",
    "\n",
    "# Reshape X_train and X_test to have a time step dimension (convert to 3D for RNN input)\n",
    "X_train_reshaped = np.expand_dims(X_train_numeric, axis=1)  \n",
    "X_test_reshaped = np.expand_dims(X_test_numeric, axis=1)\n",
    "\n",
    "# Define the Simple RNN model\n",
    "model = Sequential([\n",
    "    SimpleRNN(32, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), activation='relu', return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')  \n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model and store history\n",
    "history = model.fit(X_train_reshaped, y_train_numeric, epochs=20, batch_size=32, validation_data=(X_test_reshaped, y_test_numeric))\n",
    "\n",
    "# Make predictions\n",
    "y_pred_proba = model.predict(X_test_reshaped).ravel()\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Check for NaN values in y_pred_proba or y_test_numeric\n",
    "if np.isnan(y_pred_proba).any() or np.isnan(y_test_numeric).any():\n",
    "    print(\"NaN values found in predictions or test labels.\")\n",
    "    y_pred_proba = np.nan_to_num(y_pred_proba)  \n",
    "    y_test_numeric = np.nan_to_num(y_test_numeric)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test_numeric, y_pred)\n",
    "log_loss_value = log_loss(y_test_numeric, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test_numeric, y_pred_proba)\n",
    "report = classification_report(y_test_numeric, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "\n",
    "# Training accuracy and loss\n",
    "train_accuracy = history.history['accuracy'][-1]\n",
    "train_loss = history.history['loss'][-1]\n",
    "print(f\"Training Accuracy: {train_accuracy}\")\n",
    "print(f\"Training Loss: {train_loss}\")\n",
    "\n",
    "# Plot training & validation accuracy over epochs\n",
    "plt.figure()\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss over epochs\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test_numeric, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_curve, roc_auc_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(32, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), activation='relu', return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')  \n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model and store history\n",
    "history = model.fit(X_train_reshaped, y_train_numeric, epochs=20, batch_size=32, validation_data=(X_test_reshaped, y_test_numeric))\n",
    "\n",
    "# Make predictions\n",
    "y_pred_proba = model.predict(X_test_reshaped).ravel()\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Check for NaN values in y_pred_proba or y_test_numeric\n",
    "if np.isnan(y_pred_proba).any() or np.isnan(y_test_numeric).any():\n",
    "    print(\"NaN values found in predictions or test labels.\")\n",
    "    y_pred_proba = np.nan_to_num(y_pred_proba) \n",
    "    y_test_numeric = np.nan_to_num(y_test_numeric)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test_numeric, y_pred)\n",
    "log_loss_value = log_loss(y_test_numeric, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test_numeric, y_pred_proba)\n",
    "report = classification_report(y_test_numeric, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "\n",
    "# Training accuracy and loss\n",
    "train_accuracy = history.history['accuracy'][-1]\n",
    "train_loss = history.history['loss'][-1]\n",
    "print(f\"Training Accuracy: {train_accuracy}\")\n",
    "print(f\"Training Loss: {train_loss}\")\n",
    "\n",
    "# Plot training & validation accuracy over epochs\n",
    "plt.figure()\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss over epochs\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test_numeric, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_curve, roc_auc_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Define the GRU model\n",
    "model = Sequential([\n",
    "    GRU(32, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), activation='relu', return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')  \n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model and store history\n",
    "history = model.fit(X_train_reshaped, y_train_numeric, epochs=20, batch_size=32, validation_data=(X_test_reshaped, y_test_numeric))\n",
    "\n",
    "# Make predictions\n",
    "y_pred_proba = model.predict(X_test_reshaped).ravel()\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Check for NaN values in y_pred_proba or y_test_numeric\n",
    "if np.isnan(y_pred_proba).any() or np.isnan(y_test_numeric).any():\n",
    "    print(\"NaN values found in predictions or test labels.\")\n",
    "    y_pred_proba = np.nan_to_num(y_pred_proba)  \n",
    "    y_test_numeric = np.nan_to_num(y_test_numeric)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test_numeric, y_pred)\n",
    "log_loss_value = log_loss(y_test_numeric, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test_numeric, y_pred_proba)\n",
    "report = classification_report(y_test_numeric, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "\n",
    "# Training accuracy and loss\n",
    "train_accuracy = history.history['accuracy'][-1]\n",
    "train_loss = history.history['loss'][-1]\n",
    "print(f\"Training Accuracy: {train_accuracy}\")\n",
    "print(f\"Training Loss: {train_loss}\")\n",
    "\n",
    "# Plot training & validation accuracy over epochs\n",
    "plt.figure()\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss over epochs\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test_numeric, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_validate, StratifiedKFold\n",
    "\n",
    "# GBM model\n",
    "model = GradientBoostingClassifier(\n",
    "    max_depth=5,                   \n",
    "    learning_rate=0.05,             \n",
    "    n_estimators=200,               \n",
    "    subsample=0.8,                 \n",
    "    random_state=42                 \n",
    ")\n",
    "\n",
    "# Set up stratified k-fold cross-validation (ensuring class balance in folds)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation and capture metrics\n",
    "cv_results = cross_validate(model, X_train, y_train, cv=cv, scoring=['accuracy', 'roc_auc', 'neg_log_loss'], return_train_score=True)\n",
    "\n",
    "# Output the results of cross-validation\n",
    "print(\"Cross-Validation Results:\")\n",
    "print(f\"Mean Test Accuracy: {cv_results['test_accuracy'].mean():.4f}\")\n",
    "print(f\"Mean Test ROC AUC: {cv_results['test_roc_auc'].mean():.4f}\")\n",
    "print(f\"Mean Test Log Loss: {-cv_results['test_neg_log_loss'].mean():.4f}\")\n",
    "print(f\"Mean Train Accuracy: {cv_results['train_accuracy'].mean():.4f}\")\n",
    "print(f\"Mean Train ROC AUC: {cv_results['train_roc_auc'].mean():.4f}\")\n",
    "print(f\"Mean Train Log Loss: {-cv_results['train_neg_log_loss'].mean():.4f}\")\n",
    "\n",
    "# Now fit the model on the entire training dataset and evaluate it\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and evaluation metrics as before\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Output the evaluation metrics\n",
    "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test Log Loss: {log_loss_value:.4f}\")\n",
    "print(f\"Test ROC AUC: {roc_auc:.4f}\")\n",
    "print(f\"Classification Report:\\n{report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_validate, StratifiedKFold\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "    max_depth=5,                  \n",
    "    learning_rate=0.05,            \n",
    "    n_estimators=200,              \n",
    "    subsample=0.8,                 \n",
    "    colsample_bytree=0.8,         \n",
    "    scale_pos_weight=len(y[y == 0]) / len(y[y == 1]),  \n",
    "    gamma=0.1,                    \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Set up stratified k-fold cross-validation (ensuring class balance in folds)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation and capture metrics\n",
    "cv_results = cross_validate(model, X_train, y_train, cv=cv, scoring=['accuracy', 'roc_auc', 'neg_log_loss'], return_train_score=True)\n",
    "\n",
    "# Output the results of cross-validation\n",
    "print(\"Cross-Validation Results:\")\n",
    "print(f\"Mean Test Accuracy: {cv_results['test_accuracy'].mean():.4f}\")\n",
    "print(f\"Mean Test ROC AUC: {cv_results['test_roc_auc'].mean():.4f}\")\n",
    "print(f\"Mean Test Log Loss: {-cv_results['test_neg_log_loss'].mean():.4f}\")\n",
    "print(f\"Mean Train Accuracy: {cv_results['train_accuracy'].mean():.4f}\")\n",
    "print(f\"Mean Train ROC AUC: {cv_results['train_roc_auc'].mean():.4f}\")\n",
    "print(f\"Mean Train Log Loss: {-cv_results['train_neg_log_loss'].mean():.4f}\")\n",
    "\n",
    "# Now fit the model on the entire training dataset and evaluate it\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and evaluation metrics as before\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Output the evaluation metrics\n",
    "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test Log Loss: {log_loss_value:.4f}\")\n",
    "print(f\"Test ROC AUC: {roc_auc:.4f}\")\n",
    "print(f\"Classification Report:\\n{report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

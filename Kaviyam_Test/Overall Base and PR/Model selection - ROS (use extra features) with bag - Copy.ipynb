{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Step 1: Load data from PostgreSQL\u001b[39;00m\n\u001b[0;32m     26\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSELECT * FROM public.overall_cleaned_base_and_pr_ef_policyef;\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 27\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m selected_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolicy no\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct name\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct name 2\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbiztype\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolicy end date\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolicy start date\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrenewal type\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtie up\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvehicle segment\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     30\u001b[0m  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmanufacturer/make\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariant\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfuel type\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrto location\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCleaned Reg no\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchassis_engine_key\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     31\u001b[0m  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvehicle idv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mncb amount\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapplicable discount without ncb\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     35\u001b[0m  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRenewal Rate Status\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorrected_name\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupdated_old_policy_no\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     36\u001b[0m ]\n\u001b[0;32m     38\u001b[0m data \u001b[38;5;241m=\u001b[39m data[selected_columns]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\sql.py:734\u001b[0m, in \u001b[0;36mread_sql\u001b[1;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize, dtype_backend, dtype)\u001b[0m\n\u001b[0;32m    724\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pandas_sql\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[0;32m    725\u001b[0m         sql,\n\u001b[0;32m    726\u001b[0m         index_col\u001b[38;5;241m=\u001b[39mindex_col,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    731\u001b[0m         dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    732\u001b[0m     )\n\u001b[0;32m    733\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    735\u001b[0m \u001b[43m        \u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    736\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    737\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    738\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    739\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    740\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    741\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    742\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\sql.py:1853\u001b[0m, in \u001b[0;36mSQLDatabase.read_query\u001b[1;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[0;32m   1841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_query_iterator(\n\u001b[0;32m   1842\u001b[0m         result,\n\u001b[0;32m   1843\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexit_stack,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1850\u001b[0m         dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1851\u001b[0m     )\n\u001b[0;32m   1852\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1853\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetchall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1854\u001b[0m     frame \u001b[38;5;241m=\u001b[39m _wrap_result(\n\u001b[0;32m   1855\u001b[0m         data,\n\u001b[0;32m   1856\u001b[0m         columns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1861\u001b[0m         dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1862\u001b[0m     )\n\u001b[0;32m   1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m frame\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sqlalchemy\\engine\\result.py:1315\u001b[0m, in \u001b[0;36mResult.fetchall\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfetchall\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Sequence[Row[_TP]]:\n\u001b[0;32m   1313\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"A synonym for the :meth:`_engine.Result.all` method.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_allrows\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sqlalchemy\\engine\\result.py:548\u001b[0m, in \u001b[0;36mResultInternal._allrows\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    544\u001b[0m post_creational_filter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_creational_filter\n\u001b[0;32m    546\u001b[0m make_row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_row_getter\n\u001b[1;32m--> 548\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetchall_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    549\u001b[0m made_rows: List[_InterimRowType[_R]]\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m make_row:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sqlalchemy\\engine\\cursor.py:2130\u001b[0m, in \u001b[0;36mCursorResult._fetchall_impl\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_fetchall_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m-> 2130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcursor_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetchall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcursor\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sqlalchemy\\engine\\cursor.py:1140\u001b[0m, in \u001b[0;36mCursorFetchStrategy.fetchall\u001b[1;34m(self, result, dbapi_cursor)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rows\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1140\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdbapi_cursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sqlalchemy\\engine\\cursor.py:1081\u001b[0m, in \u001b[0;36mCursorFetchStrategy.handle_exception\u001b[1;34m(self, result, dbapi_cursor, err)\u001b[0m\n\u001b[0;32m   1075\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mhandle_exception\u001b[39m(\n\u001b[0;32m   1076\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1077\u001b[0m     result: CursorResult[Any],\n\u001b[0;32m   1078\u001b[0m     dbapi_cursor: Optional[DBAPICursor],\n\u001b[0;32m   1079\u001b[0m     err: \u001b[38;5;167;01mBaseException\u001b[39;00m,\n\u001b[0;32m   1080\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m-> 1081\u001b[0m     \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_dbapi_exception\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1082\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdbapi_cursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\n\u001b[0;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sqlalchemy\\engine\\base.py:2355\u001b[0m, in \u001b[0;36mConnection._handle_dbapi_exception\u001b[1;34m(self, e, statement, parameters, cursor, context, is_sub_exec)\u001b[0m\n\u001b[0;32m   2353\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2354\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m exc_info[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2355\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc_info[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mwith_traceback(exc_info[\u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m   2356\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   2357\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reentrant_error\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sqlalchemy\\engine\\cursor.py:1136\u001b[0m, in \u001b[0;36mCursorFetchStrategy.fetchall\u001b[1;34m(self, result, dbapi_cursor)\u001b[0m\n\u001b[0;32m   1130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfetchall\u001b[39m(\n\u001b[0;32m   1131\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1132\u001b[0m     result: CursorResult[Any],\n\u001b[0;32m   1133\u001b[0m     dbapi_cursor: DBAPICursor,\n\u001b[0;32m   1134\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m   1135\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1136\u001b[0m         rows \u001b[38;5;241m=\u001b[39m \u001b[43mdbapi_cursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetchall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1137\u001b[0m         result\u001b[38;5;241m.\u001b[39m_soft_close()\n\u001b[0;32m   1138\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m rows\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_curve, roc_auc_score, classification_report, log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import pandas as pd\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Database connection setup\n",
    "db_config = {\n",
    "    'host': 'localhost',\n",
    "    'database': 'postgres',\n",
    "    'user': 'postgres',\n",
    "    'password': 'kaviyam123',\n",
    "    'port': '5432'\n",
    "}\n",
    "\n",
    "connection_string = f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['database']}\"\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Step 1: Load data from PostgreSQL\n",
    "query = 'SELECT * FROM public.overall_cleaned_base_and_pr_ef_policyef;'\n",
    "data = pd.read_sql(query, con=engine)\n",
    "\n",
    "selected_columns = ['policy no', 'product name', 'product name 2',  'biztype', 'policy end date', 'policy start date', 'renewal type', 'tie up', 'vehicle segment',\n",
    " 'age', 'manufacturer/make', 'model', 'variant', 'fuel type', 'rto location', 'Cleaned Reg no', 'chassis_engine_key',\n",
    " 'vehicle idv', 'ncb amount', 'applicable discount without ncb', \n",
    " 'before gst add-on gwp', 'total od premium', 'total tp premium', 'gst', 'total premium payable', \n",
    " 'ncb % previous year', 'applicable discount with ncb', 'Cleaned Branch Name 2', 'Cleaned State2', 'Cleaned Zone 2', 'cleaned new vertical',\n",
    " 'Number of claims', 'approved', 'denied', 'withdrawn', 'customerid', 'Policy Status', 'Policy Tenure Month', 'Policy Tenure', 'Customer Tenure', 'New Customers', 'Claim Happaned/Not', \n",
    " 'Renewal Rate Status', 'corrected_name', 'updated_old_policy_no'\n",
    "]\n",
    "\n",
    "data = data[selected_columns]\n",
    "\n",
    "# Remove rows where 'Status' contains 'Open'\n",
    "data = data[data['Policy Status'].isin(['Renewed', 'Not Renewed'])]\n",
    "\n",
    "\n",
    "data['Policy Status'] = data['Policy Status'].apply(lambda x: 1 if x == 'Not Renewed' else 0)\n",
    "\n",
    "for column in data.columns:\n",
    "    if data[column].dtype == 'object':\n",
    "        data[column] = data[column].fillna('missing')\n",
    "    else:\n",
    "        data[column] = data[column].fillna(0)\n",
    "\n",
    "date_columns = ['policy start date', 'policy end date']\n",
    "\n",
    "for col in date_columns:\n",
    "    data[col] = pd.to_datetime(data[col], errors='coerce')\n",
    "\n",
    "# Extract year, month, and day as separate features for all date columns\n",
    "new_date_cols = {}\n",
    "for col in date_columns:\n",
    "    new_date_cols[f'{col}_YEAR'] = data[col].dt.year\n",
    "    new_date_cols[f'{col}_MONTH'] = data[col].dt.month\n",
    "    new_date_cols[f'{col}_DAY'] = data[col].dt.day\n",
    "\n",
    "data = pd.concat([data, pd.DataFrame(new_date_cols)], axis=1)\n",
    "\n",
    "# Drop date columns after splitting\n",
    "data = data.drop(columns=date_columns)\n",
    "\n",
    "# Define features and target\n",
    "features = [col for col in data.columns if col not in ['Policy Status']]\n",
    "\n",
    "# Define X (features) and y (target)\n",
    "X = data[features]\n",
    "y = data['Policy Status']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, log_loss, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "\n",
    "# Apply Random Oversampling to the training data\n",
    "X_train, y_train = ros.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply label encoding to categorical features for both train and test sets\n",
    "for column in X_train.columns:\n",
    "    if X_train[column].dtype == 'object':\n",
    "        # Initialize and fit the LabelEncoder on the training data\n",
    "        label_encoder = LabelEncoder()\n",
    "        X_train[column] = label_encoder.fit_transform(X_train[column].astype(str))\n",
    "\n",
    "        # Create a mapping dictionary from the LabelEncoder\n",
    "        mapping_dict = {label: i for i, label in enumerate(label_encoder.classes_)}\n",
    "        \n",
    "        # Track the next unique integer for unseen values in the test set\n",
    "        next_unique_value = [max(mapping_dict.values()) + 1]  \n",
    "\n",
    "        # Encode the test data\n",
    "        def encode_test_value(value):\n",
    "            if value in mapping_dict:\n",
    "                return mapping_dict[value]\n",
    "            else:\n",
    "                # Update the mapping_dict with a new unique value for unseen categories\n",
    "                mapping_dict[value] = next_unique_value[0]\n",
    "                next_unique_value[0] += 1\n",
    "                return mapping_dict[value]\n",
    "\n",
    "        # Apply the encoding to the test set\n",
    "        X_test[column] = X_test[column].apply(encode_test_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Initialize and train the Decision Tree classifier\n",
    "model = DecisionTreeClassifier(random_state=42, max_depth=5)\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Initialize and train the Decision Tree classifier\n",
    "base_model = DecisionTreeClassifier(random_state=42, max_depth=5)\n",
    "\n",
    "# Bagging Classifier (Fix: Use `estimator` instead of `base_estimator`)\n",
    "model = BaggingClassifier(\n",
    "    estimator=base_model,  # Corrected from `base_estimator`\n",
    "    n_estimators=5,        # Number of base models\n",
    "    max_samples=0.8,       # Use 80% of data per model\n",
    "    bootstrap=True,        # Enable bagging\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Initialize and train the Decision Tree classifier\n",
    "model = DecisionTreeClassifier(random_state=42, max_depth=10)\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Initialize and train the Decision Tree classifier\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "model = DecisionTreeClassifier(\n",
    "    random_state=42,\n",
    "    max_depth=8,                \n",
    "    min_samples_split=20,       \n",
    "    min_samples_leaf=10,        \n",
    "    max_leaf_nodes=50,          \n",
    "    ccp_alpha=0.01              \n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(random_state=42, max_depth=10)\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=8,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    max_leaf_nodes=50,\n",
    "    class_weight={0: 10, 1: 1},  \n",
    "    random_state=42\n",
    ")\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(random_state=42, max_depth=20, class_weight='balanced')\n",
    "\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=8,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    max_leaf_nodes=50,\n",
    "    class_weight={0: 5, 1: 1},  \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Random Forest model\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,             \n",
    "    max_depth=8,                  \n",
    "    min_samples_split=20,         \n",
    "    min_samples_leaf=10,          \n",
    "    max_leaf_nodes=50,            \n",
    "    class_weight='balanced',      \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(\n",
    "    n_estimators=50,  \n",
    "    max_depth=6,     \n",
    "    min_samples_split=50,\n",
    "    min_samples_leaf=20,\n",
    "    max_leaf_nodes=30,\n",
    "    class_weight={0: 5, 1: 1},\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB  \n",
    "from sklearn.metrics import accuracy_score, roc_curve, roc_auc_score, classification_report, log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize and train the Naive Bayes model\n",
    "model = GaussianNB() \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, classification_report, confusion_matrix, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# XGBoost model\n",
    "model = xgb.XGBClassifier(\n",
    "    max_depth=6,                  \n",
    "    learning_rate=0.1,            \n",
    "    n_estimators=100,            \n",
    "    scale_pos_weight=len(y_train[y_train == 0]) / len(y_train[y_train == 1]), \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "\n",
    "# XGBoost model\n",
    "base_model = xgb.XGBClassifier(\n",
    "    max_depth=6,                  \n",
    "    learning_rate=0.1,            \n",
    "    n_estimators=100,            \n",
    "    scale_pos_weight=len(y_train[y_train == 0]) / len(y_train[y_train == 1]), \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# Bagging Classifier (Fix: Use `estimator` instead of `base_estimator`)\n",
    "model = BaggingClassifier(\n",
    "    estimator=base_model,  # Corrected from `base_estimator`\n",
    "    n_estimators=5,        # Number of base models\n",
    "    max_samples=0.8,       # Use 80% of data per model\n",
    "    bootstrap=True,        # Enable bagging\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "base_model = xgb.XGBClassifier(\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Bagging Classifier (Fix: Use `estimator` instead of `base_estimator`)\n",
    "model = BaggingClassifier(\n",
    "    estimator=base_model,  # Corrected from `base_estimator`\n",
    "    n_estimators=5,        # Number of base models\n",
    "    max_samples=0.8,       # Use 80% of data per model\n",
    "    bootstrap=True,        # Enable bagging\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(\n",
    "    max_depth=5,                  \n",
    "    learning_rate=0.05,            \n",
    "    n_estimators=200,              \n",
    "    subsample=0.8,                 \n",
    "    colsample_bytree=0.8,         \n",
    "    scale_pos_weight=len(y_train[y_train == 0]) / len(y_train[y_train == 1]),  \n",
    "    gamma=0.1,                    \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_model = xgb.XGBClassifier(\n",
    "    max_depth=5,                  \n",
    "    learning_rate=0.05,            \n",
    "    n_estimators=200,              \n",
    "    subsample=0.8,                 \n",
    "    colsample_bytree=0.8,         \n",
    "    scale_pos_weight=len(y_train[y_train == 0]) / len(y_train[y_train == 1]),  \n",
    "    gamma=0.1,                    \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Bagging Classifier (Fix: Use `estimator` instead of `base_estimator`)\n",
    "model = BaggingClassifier(\n",
    "    estimator=base_model,  # Corrected from `base_estimator`\n",
    "    n_estimators=5,        # Number of base models\n",
    "    max_samples=0.8,       # Use 80% of data per model\n",
    "    bootstrap=True,        # Enable bagging\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.1,  # L1 regularization\n",
    "    reg_lambda=1.0,  # L2 regularization\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = xgb.XGBClassifier(\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.1,  # L1 regularization\n",
    "    reg_lambda=1.0,  # L2 regularization\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Bagging Classifier (Fix: Use `estimator` instead of `base_estimator`)\n",
    "model = BaggingClassifier(\n",
    "    estimator=base_model,  # Corrected from `base_estimator`\n",
    "    n_estimators=5,        # Number of base models\n",
    "    max_samples=0.8,       # Use 80% of data per model\n",
    "    bootstrap=True,        # Enable bagging\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, classification_report, confusion_matrix, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# GBM model\n",
    "model = GradientBoostingClassifier(\n",
    "    max_depth=5,                   \n",
    "    learning_rate=0.05,             \n",
    "    n_estimators=200,               \n",
    "    subsample=0.8,                 \n",
    "    random_state=42                 \n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "model = GradientBoostingClassifier(\n",
    "    max_depth=5,\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=200,\n",
    "    subsample=0.8,\n",
    "    random_state=42,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    warm_start=False,\n",
    "    validation_fraction=0.1,\n",
    "    n_iter_no_change=10,\n",
    "    tol=1e-4\n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, classification_report, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# GBM model\n",
    "model = GradientBoostingClassifier(\n",
    "    max_depth=6,                    \n",
    "    learning_rate=0.1,              \n",
    "    n_estimators=100,              \n",
    "    random_state=42                 \n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, classification_report, confusion_matrix, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# AdaBoost model with a DecisionTreeClassifier as the base estimator\n",
    "model = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=6),  \n",
    "    learning_rate=0.1,                              \n",
    "    n_estimators=100,                               \n",
    "    random_state=42                                 \n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, classification_report, confusion_matrix, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# CatBoost model\n",
    "model = CatBoostClassifier(\n",
    "    depth=6,                      \n",
    "    learning_rate=0.1,            \n",
    "    iterations=100,               \n",
    "    random_seed=42,               \n",
    "    verbose=0                     \n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(\n",
    "    depth=10,                    \n",
    "    learning_rate=0.1,            \n",
    "    iterations=500,               \n",
    "    random_seed=42,               \n",
    "    verbose=0                     \n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(\n",
    "    depth=8,                    \n",
    "    learning_rate=0.05,           \n",
    "    iterations=500,               \n",
    "    l2_leaf_reg=5,                # L2 regularization\n",
    "    bagging_temperature=0.5,      # Stronger bagging to reduce overfitting\n",
    "    random_seed=42,               \n",
    "    verbose=0                     \n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = CatBoostClassifier(\n",
    "    depth=6,                     \n",
    "    learning_rate=0.1,            \n",
    "    iterations=300,               \n",
    "    auto_class_weights=\"Balanced\",  # Handles imbalanced classes\n",
    "    subsample=0.8,                # Random subset of data for each tree\n",
    "    random_seed=42,               \n",
    "    verbose=0                     \n",
    ")\n",
    "\n",
    "\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, classification_report, confusion_matrix, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# LightGBM model\n",
    "model = LGBMClassifier(\n",
    "    max_depth=6,                     \n",
    "    learning_rate=0.1,               \n",
    "    n_estimators=100,                \n",
    "    random_state=42                  \n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test and training data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_log_loss = log_loss(y_train, y_train_pred_proba)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate confusion matrix for test data and compute class-specific accuracy\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred)\n",
    "class_0_accuracy_test = conf_matrix_test[0, 0] / conf_matrix_test[0].sum()\n",
    "class_1_accuracy_test = conf_matrix_test[1, 1] / conf_matrix_test[1].sum()\n",
    "\n",
    "# Calculate confusion matrix for training data and compute class-specific accuracy\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "class_0_accuracy_train = conf_matrix_train[0, 0] / conf_matrix_train[0].sum()\n",
    "class_1_accuracy_train = conf_matrix_train[1, 1] / conf_matrix_train[1].sum()\n",
    "\n",
    "# Print evaluation metrics for test and training data\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "print(f\"Class 0 Test Accuracy: {class_0_accuracy_test}\")\n",
    "print(f\"Class 1 Test Accuracy: {class_1_accuracy_test}\")\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Train Log Loss: {train_log_loss}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
    "print(f\"Train Classification Report:\\n{train_report}\")\n",
    "print(f\"Class 0 Train Accuracy: {class_0_accuracy_train}\")\n",
    "print(f\"Class 1 Train Accuracy: {class_1_accuracy_train}\")\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (test) (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color='green', lw=2, label=f'ROC curve (train) (area = {train_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_curve, roc_auc_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert data to numeric types\n",
    "X_train_numeric = np.array(X_train, dtype=np.float32)  \n",
    "X_test_numeric = np.array(X_test, dtype=np.float32)    \n",
    "y_train_numeric = np.array(y_train, dtype=np.float32)  \n",
    "y_test_numeric = np.array(y_test, dtype=np.float32)    \n",
    "\n",
    "# Reshape X_train and X_test to have a time step dimension (convert to 3D for RNN input)\n",
    "X_train_reshaped = np.expand_dims(X_train_numeric, axis=1)  \n",
    "X_test_reshaped = np.expand_dims(X_test_numeric, axis=1)\n",
    "\n",
    "# Define the Simple RNN model\n",
    "model = Sequential([\n",
    "    SimpleRNN(32, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), activation='relu', return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')  \n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model and store history\n",
    "history = model.fit(X_train_reshaped, y_train_numeric, epochs=20, batch_size=32, validation_data=(X_test_reshaped, y_test_numeric))\n",
    "\n",
    "# Make predictions\n",
    "y_pred_proba = model.predict(X_test_reshaped).ravel()\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Check for NaN values in y_pred_proba or y_test_numeric\n",
    "if np.isnan(y_pred_proba).any() or np.isnan(y_test_numeric).any():\n",
    "    print(\"NaN values found in predictions or test labels.\")\n",
    "    y_pred_proba = np.nan_to_num(y_pred_proba)  \n",
    "    y_test_numeric = np.nan_to_num(y_test_numeric)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test_numeric, y_pred)\n",
    "log_loss_value = log_loss(y_test_numeric, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test_numeric, y_pred_proba)\n",
    "report = classification_report(y_test_numeric, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "\n",
    "# Training accuracy and loss\n",
    "train_accuracy = history.history['accuracy'][-1]\n",
    "train_loss = history.history['loss'][-1]\n",
    "print(f\"Training Accuracy: {train_accuracy}\")\n",
    "print(f\"Training Loss: {train_loss}\")\n",
    "\n",
    "# Plot training & validation accuracy over epochs\n",
    "plt.figure()\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss over epochs\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test_numeric, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_curve, roc_auc_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(32, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), activation='relu', return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')  \n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model and store history\n",
    "history = model.fit(X_train_reshaped, y_train_numeric, epochs=20, batch_size=32, validation_data=(X_test_reshaped, y_test_numeric))\n",
    "\n",
    "# Make predictions\n",
    "y_pred_proba = model.predict(X_test_reshaped).ravel()\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Check for NaN values in y_pred_proba or y_test_numeric\n",
    "if np.isnan(y_pred_proba).any() or np.isnan(y_test_numeric).any():\n",
    "    print(\"NaN values found in predictions or test labels.\")\n",
    "    y_pred_proba = np.nan_to_num(y_pred_proba) \n",
    "    y_test_numeric = np.nan_to_num(y_test_numeric)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test_numeric, y_pred)\n",
    "log_loss_value = log_loss(y_test_numeric, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test_numeric, y_pred_proba)\n",
    "report = classification_report(y_test_numeric, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "\n",
    "# Training accuracy and loss\n",
    "train_accuracy = history.history['accuracy'][-1]\n",
    "train_loss = history.history['loss'][-1]\n",
    "print(f\"Training Accuracy: {train_accuracy}\")\n",
    "print(f\"Training Loss: {train_loss}\")\n",
    "\n",
    "# Plot training & validation accuracy over epochs\n",
    "plt.figure()\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss over epochs\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test_numeric, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_curve, roc_auc_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Define the GRU model\n",
    "model = Sequential([\n",
    "    GRU(32, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), activation='relu', return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')  \n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model and store history\n",
    "history = model.fit(X_train_reshaped, y_train_numeric, epochs=20, batch_size=32, validation_data=(X_test_reshaped, y_test_numeric))\n",
    "\n",
    "# Make predictions\n",
    "y_pred_proba = model.predict(X_test_reshaped).ravel()\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Check for NaN values in y_pred_proba or y_test_numeric\n",
    "if np.isnan(y_pred_proba).any() or np.isnan(y_test_numeric).any():\n",
    "    print(\"NaN values found in predictions or test labels.\")\n",
    "    y_pred_proba = np.nan_to_num(y_pred_proba)  \n",
    "    y_test_numeric = np.nan_to_num(y_test_numeric)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test_numeric, y_pred)\n",
    "log_loss_value = log_loss(y_test_numeric, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test_numeric, y_pred_proba)\n",
    "report = classification_report(y_test_numeric, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Log Loss: {log_loss_value}\")\n",
    "print(f\"Test ROC AUC: {roc_auc}\")\n",
    "print(f\"Test Classification Report:\\n{report}\")\n",
    "\n",
    "# Training accuracy and loss\n",
    "train_accuracy = history.history['accuracy'][-1]\n",
    "train_loss = history.history['loss'][-1]\n",
    "print(f\"Training Accuracy: {train_accuracy}\")\n",
    "print(f\"Training Loss: {train_loss}\")\n",
    "\n",
    "# Plot training & validation accuracy over epochs\n",
    "plt.figure()\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss over epochs\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test_numeric, y_pred_proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_validate, StratifiedKFold\n",
    "\n",
    "# GBM model\n",
    "model = GradientBoostingClassifier(\n",
    "    max_depth=5,                   \n",
    "    learning_rate=0.05,             \n",
    "    n_estimators=200,               \n",
    "    subsample=0.8,                 \n",
    "    random_state=42                 \n",
    ")\n",
    "\n",
    "# Set up stratified k-fold cross-validation (ensuring class balance in folds)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation and capture metrics\n",
    "cv_results = cross_validate(model, X_train, y_train, cv=cv, scoring=['accuracy', 'roc_auc', 'neg_log_loss'], return_train_score=True)\n",
    "\n",
    "# Output the results of cross-validation\n",
    "print(\"Cross-Validation Results:\")\n",
    "print(f\"Mean Test Accuracy: {cv_results['test_accuracy'].mean():.4f}\")\n",
    "print(f\"Mean Test ROC AUC: {cv_results['test_roc_auc'].mean():.4f}\")\n",
    "print(f\"Mean Test Log Loss: {-cv_results['test_neg_log_loss'].mean():.4f}\")\n",
    "print(f\"Mean Train Accuracy: {cv_results['train_accuracy'].mean():.4f}\")\n",
    "print(f\"Mean Train ROC AUC: {cv_results['train_roc_auc'].mean():.4f}\")\n",
    "print(f\"Mean Train Log Loss: {-cv_results['train_neg_log_loss'].mean():.4f}\")\n",
    "\n",
    "# Now fit the model on the entire training dataset and evaluate it\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and evaluation metrics as before\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Output the evaluation metrics\n",
    "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test Log Loss: {log_loss_value:.4f}\")\n",
    "print(f\"Test ROC AUC: {roc_auc:.4f}\")\n",
    "print(f\"Classification Report:\\n{report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_validate, StratifiedKFold\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "    max_depth=5,                  \n",
    "    learning_rate=0.05,            \n",
    "    n_estimators=200,              \n",
    "    subsample=0.8,                 \n",
    "    colsample_bytree=0.8,         \n",
    "    scale_pos_weight=len(y[y == 0]) / len(y[y == 1]),  \n",
    "    gamma=0.1,                    \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Set up stratified k-fold cross-validation (ensuring class balance in folds)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation and capture metrics\n",
    "cv_results = cross_validate(model, X_train, y_train, cv=cv, scoring=['accuracy', 'roc_auc', 'neg_log_loss'], return_train_score=True)\n",
    "\n",
    "# Output the results of cross-validation\n",
    "print(\"Cross-Validation Results:\")\n",
    "print(f\"Mean Test Accuracy: {cv_results['test_accuracy'].mean():.4f}\")\n",
    "print(f\"Mean Test ROC AUC: {cv_results['test_roc_auc'].mean():.4f}\")\n",
    "print(f\"Mean Test Log Loss: {-cv_results['test_neg_log_loss'].mean():.4f}\")\n",
    "print(f\"Mean Train Accuracy: {cv_results['train_accuracy'].mean():.4f}\")\n",
    "print(f\"Mean Train ROC AUC: {cv_results['train_roc_auc'].mean():.4f}\")\n",
    "print(f\"Mean Train Log Loss: {-cv_results['train_neg_log_loss'].mean():.4f}\")\n",
    "\n",
    "# Now fit the model on the entire training dataset and evaluate it\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and evaluation metrics as before\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Output the evaluation metrics\n",
    "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test Log Loss: {log_loss_value:.4f}\")\n",
    "print(f\"Test ROC AUC: {roc_auc:.4f}\")\n",
    "print(f\"Classification Report:\\n{report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

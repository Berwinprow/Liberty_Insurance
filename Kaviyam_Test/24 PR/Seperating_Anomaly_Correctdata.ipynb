{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Database connection setup\n",
    "db_config = {\n",
    "    'host': '10.10.10.71',\n",
    "    'database': 'postgres',\n",
    "    'user': 'postgres',\n",
    "    'password': 'kaviyam123',\n",
    "    'port': '5432'\n",
    "}\n",
    "connection_string = f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['database']}\"\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Step 1: Load data from PostgreSQL\n",
    "query = 'SELECT * FROM public.cleaned_overall_merged_base_pr_data;'\n",
    "df = pd.read_sql(query, con=engine)\n",
    "\n",
    "# Step 2: Define a function to clean text\n",
    "def clean_name(name):\n",
    "    if pd.isna(name):\n",
    "        return ''\n",
    "    return re.sub(r'[^a-zA-Z0-9]', '', str(name)).lower()\n",
    "\n",
    "# Step 3: Apply cleaning function to the necessary columns\n",
    "df['Cleaned_Insured name'] = df['Insured name '].apply(clean_name)\n",
    "df['Cleaned_New Branch Name 2'] = df['New Branch Name  2'].apply(clean_name)\n",
    "df['Cleaned_state2'] = df['state2'].apply(clean_name)\n",
    "\n",
    "# Step 4: Identify anomalies\n",
    "group_cols = ['Policy No']\n",
    "columns_to_check = ['Cleaned_Insured name', 'Cleaned_New Branch Name 2']\n",
    "\n",
    "# Add a column to store non-unique column names\n",
    "df['Non_unique_columns'] = ''\n",
    "\n",
    "# Create a boolean mask for anomalies\n",
    "mask = pd.Series(False, index=df.index)\n",
    "grouped = df.groupby(group_cols)\n",
    "\n",
    "for col in columns_to_check:\n",
    "    # Find groups with more than one unique value\n",
    "    unique_within_group = grouped[col].transform('nunique')\n",
    "    col_mask = unique_within_group > 1\n",
    "    mask |= col_mask\n",
    "    df.loc[col_mask, 'Non_unique_columns'] += col + ', '\n",
    "\n",
    "# Remove trailing commas from 'Non_unique_columns'\n",
    "df['Non_unique_columns'] = df['Non_unique_columns'].str.rstrip(', ')\n",
    "\n",
    "# Step 5: Separate anomalous and correct data\n",
    "anomalous_data = df[mask]\n",
    "correct_data = df[~mask]\n",
    "\n",
    "# Step 6: Save the results\n",
    "# Save anomalous data to CSV\n",
    "anomalous_data.to_csv('anomalous_data.csv', index=False)\n",
    "\n",
    "# Save correct data to the database or a CSV\n",
    "correct_data.to_sql('corrected_merged_data', engine, if_exists='replace', index=False)\n",
    "\n",
    "# Display summary\n",
    "print(f\"Anomalous data saved to 'anomalous_data.csv' with {len(anomalous_data)} rows.\")\n",
    "print(f\"Correct data saved to 'corrected_merged_data' table in the database with {len(correct_data)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data has been saved to the 'corrected_merged_claim_data' table in the database.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.types import Text, Integer, Float, DateTime\n",
    "\n",
    "# Database connection setup\n",
    "db_config = {\n",
    "    'host': '10.10.10.71',\n",
    "    'database': 'postgres',\n",
    "    'user': 'postgres',\n",
    "    'password': 'kaviyam123',\n",
    "    'port': '5432'\n",
    "}\n",
    "connection_string = f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['database']}\"\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Step 1: Load policy data from PostgreSQL\n",
    "policy_query = 'SELECT * FROM public.corrected_merged_data;'\n",
    "policy_data = pd.read_sql(policy_query, con=engine)\n",
    "\n",
    "# Step 2: Load claim data from Excel\n",
    "claim_data = pd.read_excel('unique_rows(claim).xlsx')\n",
    "\n",
    "# Step 3: Convert date columns to datetime format\n",
    "policy_data['Policy Start Date'] = pd.to_datetime(policy_data['Policy Start Date'], errors='coerce')\n",
    "policy_data['Policy End Date'] = pd.to_datetime(policy_data['Policy End Date'], errors='coerce')\n",
    "claim_data['Policy Start Date_claim'] = pd.to_datetime(claim_data['Policy Start Date_claim'], errors='coerce')\n",
    "claim_data['Policy End Date_claim'] = pd.to_datetime(claim_data['Policy End Date_claim'], errors='coerce')\n",
    "\n",
    "# Step 4: Merge the datasets on the specified columns\n",
    "merged_data = pd.merge(\n",
    "    policy_data,\n",
    "    claim_data,\n",
    "    how='left',  # Use 'left' to keep all rows in the policy data\n",
    "    left_on=['Policy No', 'Policy Start Date', 'Policy End Date'],\n",
    "    right_on=['Policy No (Str)', 'Policy Start Date_claim', 'Policy End Date_claim'],\n",
    "    suffixes=('_policy', '_claim')  # Avoid column conflicts\n",
    ")\n",
    "\n",
    "# Step 5: Map pandas dtypes to SQLAlchemy types dynamically\n",
    "def map_column_types(df):\n",
    "    dtype_mapping = {}\n",
    "    for col in df.columns:\n",
    "        if pd.api.types.is_integer_dtype(df[col]):\n",
    "            dtype_mapping[col] = Integer\n",
    "        elif pd.api.types.is_float_dtype(df[col]):\n",
    "            dtype_mapping[col] = Float\n",
    "        elif pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "            dtype_mapping[col] = DateTime\n",
    "        elif pd.api.types.is_object_dtype(df[col]) or pd.api.types.is_string_dtype(df[col]):\n",
    "            dtype_mapping[col] = Text\n",
    "        else:\n",
    "            dtype_mapping[col] = Text  # Default to Text for unsupported types\n",
    "    return dtype_mapping\n",
    "\n",
    "dtype_mapping = map_column_types(merged_data)\n",
    "\n",
    "# Step 6: Save the merged data to PostgreSQL\n",
    "merged_table_name = 'corrected_merged_claim_data'\n",
    "merged_data.to_sql(\n",
    "    merged_table_name,\n",
    "    con=engine,\n",
    "    if_exists='replace',\n",
    "    index=False,\n",
    "    dtype=dtype_mapping\n",
    ")\n",
    "\n",
    "# Step 7: Save the merged data to a CSV file (optional)\n",
    "# merged_data.to_csv('merged_policy_claim_data(Liberty).csv', index=False)\n",
    "\n",
    "# Display success message\n",
    "print(f\"Merged data has been saved to the '{merged_table_name}' table in the database.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Database connection setup\n",
    "db_config = {\n",
    "    'host': '10.10.10.71',\n",
    "    'database': 'postgres',\n",
    "    'user': 'postgres',\n",
    "    'password': 'kaviyam123',\n",
    "    'port': '5432'\n",
    "}\n",
    "connection_string = f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['database']}\"\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Step 1: Load data from PostgreSQL\n",
    "query = 'SELECT * FROM public.corrected_merged_claim_data;'\n",
    "db_data = pd.read_sql(query, con=engine)\n",
    "\n",
    "# Load the state-to-zone mapping from CSV\n",
    "zone_mapping = pd.read_csv('Indian_States_Direction_Final.csv')  # Replace with your file path\n",
    "\n",
    "# Ensure state names in both datasets are lowercase and stripped of spaces for matching\n",
    "db_data['Cleaned_state2'] = db_data['Cleaned_state2'].str.lower().str.strip()\n",
    "zone_mapping['State and UT'] = zone_mapping['State and UT'].str.lower().str.strip()\n",
    "\n",
    "# Step 2: Fill NULL values in Zone 2 based on Cleaned_state2\n",
    "state_zone_dict = dict(zip(zone_mapping['State and UT'], zone_mapping['Zone']))\n",
    "db_data['Zone 2'] = db_data.apply(\n",
    "    lambda row: state_zone_dict.get(row['Cleaned_state2'], row['Zone 2']) if pd.isna(row['Zone 2']) else row['Zone 2'], axis=1\n",
    ")\n",
    "\n",
    "# Step 3: Resolve conflicts for the same Policy No with different Zone 2 values\n",
    "db_data['Policy Start Date'] = pd.to_datetime(db_data['Policy Start Date'], errors='coerce')\n",
    "\n",
    "# Sort by Policy No and Policy Start Date (ascending)\n",
    "db_data.sort_values(by=['Policy No', 'Policy Start Date'], inplace=True)\n",
    "\n",
    "# Identify and resolve conflicting Zone 2 values\n",
    "def resolve_zone_conflicts(group):\n",
    "    if group['Zone 2'].nunique() > 1:\n",
    "        # Retain the Zone 2 value from the oldest Policy Start Date\n",
    "        oldest_zone = group.loc[group['Policy Start Date'].idxmin(), 'Zone 2']\n",
    "        group['Zone 2'] = oldest_zone\n",
    "    return group\n",
    "\n",
    "db_data = db_data.groupby('Policy No').apply(resolve_zone_conflicts)\n",
    "\n",
    "# Step 4: Save the corrected data back to PostgreSQL\n",
    "db_data.to_sql('corrected_merged_claim_data_final', con=engine, if_exists='replace', index=False)\n",
    "\n",
    "print(\"The corrected data has been saved to the 'corrected_merged_claim_data_final' table in the database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

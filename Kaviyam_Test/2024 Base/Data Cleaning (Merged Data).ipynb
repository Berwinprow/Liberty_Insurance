{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Database connection setup\n",
    "db_config = {\n",
    "    'host': 'localhost',         \n",
    "    'database': 'postgres',\n",
    "    'user': 'postgres',        \n",
    "    'password': 'abc', \n",
    "    'port': '5432'               \n",
    "}\n",
    "\n",
    "# Create connection string\n",
    "connection_string = f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['database']}\"\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Step 1: Fetch data from the database\n",
    "query = \"SELECT * FROM merged_2023_2024_base;\" \n",
    "df = pd.read_sql(query, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Convert relevant columns to datetime format\n",
    "df['Policy Start Date'] = pd.to_datetime(df['Policy Start Date'])\n",
    "df['Policy End Date'] = pd.to_datetime(df['Policy End Date'])\n",
    "\n",
    "# Step 3: Identify duplicates based on specific columns\n",
    "duplicates = df[df.duplicated(subset=['Policy Number', 'Policy Start Date', 'Policy End Date'], keep=False)]\n",
    "\n",
    "# Step 4: Prioritize duplicates based on null values and BOOKED column\n",
    "def prioritize_rows(group):\n",
    "    # Add a null count column and sort by it and BOOKED\n",
    "    group = group.assign(null_count=group.isnull().sum(axis=1))\n",
    "    group = group.sort_values(by=['null_count', 'BOOKED'], ascending=[True, False])\n",
    "    return group.iloc[0]\n",
    "\n",
    "cleaned_duplicates = duplicates.groupby(['Policy Number', 'Policy Start Date', 'Policy End Date']).apply(prioritize_rows).reset_index(drop=True)\n",
    "\n",
    "# Step 5: Remove duplicates from the original dataframe\n",
    "df_cleaned = pd.concat([df, cleaned_duplicates]).drop_duplicates(keep=False)\n",
    "\n",
    "# Step 6: Combine removed duplicates for the output file\n",
    "removed_rows = pd.concat([df, df_cleaned]).drop_duplicates(keep=False)\n",
    "\n",
    "# Step 7: Save results to CSV files (optional)\n",
    "removed_rows.to_csv('duplicates_data (Merged data).csv', index=False)\n",
    "df_cleaned.to_csv('cleaned_Merged_Base_dataset.csv', index=False)\n",
    "\n",
    "# Optional Step: Save cleaned data back to the database\n",
    "df_cleaned.to_sql('cleaned_Merged_Base_Data', con=engine, if_exists='replace', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_8468\\3731658580.py:32: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  cleaned_duplicates = duplicates.groupby(['Policy No', 'Policy Start Date', 'Policy End Date']).apply(prioritize_rows).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial row count: 1529265\n",
      "Duplicate row count: 737497\n",
      "Cleaned row count: 1160517\n",
      "Removed row count: 1106243\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Database connection setup\n",
    "db_config = {\n",
    "    'host': 'localhost',         \n",
    "    'database': 'postgres',\n",
    "    'user': 'postgres',        \n",
    "    'password': 'abc', \n",
    "    'port': '5432'               \n",
    "}\n",
    "connection_string = f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['database']}\"\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Step 1: Fetch data\n",
    "query = \"SELECT * FROM merged_2023_2024_base;\" \n",
    "df = pd.read_sql(query, con=engine)\n",
    "\n",
    "# Step 2: Convert dates\n",
    "df['Policy Start Date'] = pd.to_datetime(df['Policy Start Date'])\n",
    "df['Policy End Date'] = pd.to_datetime(df['Policy End Date'])\n",
    "\n",
    "# Step 3: Identify duplicates\n",
    "duplicates = df[df.duplicated(subset=['Policy No', 'Policy Start Date', 'Policy End Date'], keep=False)]\n",
    "\n",
    "# Step 4: Prioritize duplicates\n",
    "def prioritize_rows(group):\n",
    "    group = group.assign(null_count=group.isnull().sum(axis=1))\n",
    "    group = group.sort_values(by=['null_count', 'BOOKED'], ascending=[True, False])\n",
    "    return group.iloc[0]\n",
    "\n",
    "cleaned_duplicates = duplicates.groupby(['Policy No', 'Policy Start Date', 'Policy End Date']).apply(prioritize_rows).reset_index(drop=True)\n",
    "\n",
    "# Step 5: Remove duplicates from the original dataframe\n",
    "df_cleaned = df.loc[~df.index.isin(cleaned_duplicates.index)]\n",
    "\n",
    "# Step 6: Combine removed duplicates for the output\n",
    "removed_rows = pd.concat([duplicates, cleaned_duplicates]).drop_duplicates(keep=False)\n",
    "\n",
    "# Debug final counts\n",
    "print(f\"Initial row count: {len(df)}\")\n",
    "print(f\"Duplicate row count: {len(duplicates)}\")\n",
    "print(f\"Cleaned row count: {len(df_cleaned)}\")\n",
    "print(f\"Removed row count: {len(removed_rows)}\")\n",
    "# Step 7: Save results to CSV files (optional)\n",
    "# removed_rows.to_csv('duplicates_data (Merged data) 1.csv', index=False)\n",
    "# df_cleaned.to_csv('cleaned_Merged_Base_dataset 1.csv', index=False)\n",
    "\n",
    "# Optional Step: Save cleaned data back to the database\n",
    "df_cleaned.to_sql('cleaned_merged_base_data_check1', con=engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_8468\\845672825.py:37: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(prioritize_rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial row count: 1529265\n",
      "Duplicate row count: 737497\n",
      "Cleaned unique row count: 1160516\n",
      "Removed row count: 1106243\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "356"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Database connection setup\n",
    "db_config = {\n",
    "    'host': 'localhost',         \n",
    "    'database': 'postgres',\n",
    "    'user': 'postgres',        \n",
    "    'password': 'abc', \n",
    "    'port': '5432'               \n",
    "}\n",
    "connection_string = f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['database']}\"\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Step 1: Fetch data\n",
    "query = \"SELECT * FROM merged_2023_2024_base;\" \n",
    "df = pd.read_sql(query, con=engine)\n",
    "\n",
    "# Step 2: Convert dates to datetime format\n",
    "df['Policy Start Date'] = pd.to_datetime(df['Policy Start Date'])\n",
    "df['Policy End Date'] = pd.to_datetime(df['Policy End Date'])\n",
    "\n",
    "# Step 3: Identify duplicates\n",
    "duplicates = df[df.duplicated(subset=['Policy No', 'Policy Start Date', 'Policy End Date'], keep=False)]\n",
    "\n",
    "# Step 4: Prioritize duplicates\n",
    "def prioritize_rows(group):\n",
    "    # Add null count column for prioritization\n",
    "    group = group.assign(null_count=group.isnull().sum(axis=1))\n",
    "    # Sort by null_count (ascending) and BOOKED (descending)\n",
    "    group = group.sort_values(by=['null_count', 'BOOKED'], ascending=[True, False])\n",
    "    return group.iloc[0]  # Return the first row after sorting\n",
    "\n",
    "# Apply prioritization logic to each duplicate group\n",
    "cleaned_duplicates = (\n",
    "    duplicates.groupby(['Policy No', 'Policy Start Date', 'Policy End Date'])\n",
    "    .apply(prioritize_rows)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Step 5: Ensure uniqueness in the cleaned dataset\n",
    "df_cleaned = pd.concat([df, cleaned_duplicates]).drop_duplicates(\n",
    "    subset=['Policy No', 'Policy Start Date', 'Policy End Date'], keep='last'\n",
    ")\n",
    "\n",
    "# Step 6: Combine removed duplicates for the output\n",
    "removed_rows = pd.concat([df, df_cleaned]).drop_duplicates(keep=False)\n",
    "\n",
    "# Debug row counts\n",
    "print(f\"Initial row count: {len(df)}\")\n",
    "print(f\"Duplicate row count: {len(duplicates)}\")\n",
    "print(f\"Cleaned unique row count: {len(df_cleaned)}\")\n",
    "print(f\"Removed row count: {len(removed_rows)}\")\n",
    "\n",
    "# Optional Step: Save cleaned data back to database\n",
    "df_cleaned.to_sql('cleaned_merged_base_data_check', con=engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

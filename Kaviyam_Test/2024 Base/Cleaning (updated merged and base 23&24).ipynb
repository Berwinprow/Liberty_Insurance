{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Database connection setup\n",
    "db_config = {\n",
    "    'host': 'localhost',\n",
    "    'database': 'postgres',\n",
    "    'user': 'postgres',\n",
    "    'password': 'kaviyam123',\n",
    "    'port': '5432'\n",
    "}\n",
    "connection_string = f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['database']}\"\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Step 1: Load data\n",
    "query = \"SELECT * FROM test_data_12_12;\"\n",
    "df = pd.read_sql(query, con=engine)\n",
    "\n",
    "df['Policy Start Date'] = pd.to_datetime(df['Policy Start Date'], errors='coerce')\n",
    "df['Policy End Date'] = pd.to_datetime(df['Policy End Date'], errors='coerce')\n",
    "\n",
    "# Step 3: Deduplicate and prioritize\n",
    "duplicates = df[df.duplicated(subset=['Policy No', 'Policy Start Date', 'Policy End Date'], keep=False)]\n",
    "\n",
    "def prioritize_rows(group):\n",
    "    group = group.assign(null_count=group.isnull().sum(axis=1))\n",
    "    group = group.sort_values(by=['null_count', 'booked'], ascending=[True, False])\n",
    "    return group.iloc[0]\n",
    "\n",
    "cleaned_duplicates = (\n",
    "    duplicates.groupby(['Policy No', 'Policy Start Date', 'Policy End Date'])\n",
    "    .apply(prioritize_rows)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "df_cleaned = pd.concat([df, cleaned_duplicates]).drop_duplicates(subset=['Policy No', 'Policy Start Date', 'Policy End Date'], keep='last')\n",
    "\n",
    "df_cleaned['Policy End Date'] = pd.to_datetime(df_cleaned['Policy End Date'], errors='coerce')\n",
    "\n",
    "# Step 4: Handle NULL values in BOOKED\n",
    "today = pd.Timestamp(datetime.now().date())  # Ensure today is a pandas.Timestamp\n",
    "df_cleaned['booked'] = df_cleaned.apply(\n",
    "    lambda row: (\n",
    "        0 if pd.isnull(row['booked']) and pd.notnull(row['Policy End Date']) and row['Policy End Date'] < today \n",
    "        else '-' if pd.isnull(row['booked']) else row['booked']\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Step 5: Correct BOOKED values based on Type\n",
    "correction_count = 0\n",
    "\n",
    "def correct_booked(group):\n",
    "    global correction_count\n",
    "    type_a = group[group['Type'] == 'A']\n",
    "    type_b = group[group['Type'] == 'B']\n",
    "    \n",
    "    # Check if both Type A and Type B exist for the same Policy No\n",
    "    if not type_a.empty and not type_b.empty:\n",
    "        # Check if BOOKED is 0 for Type A\n",
    "        if (type_a.iloc[0]['booked'] == 0):\n",
    "            correction_count += 1\n",
    "            # Update BOOKED to 1 for Type A\n",
    "            group.loc[group['Type'] == 'A', 'booked'] = 1\n",
    "    return group\n",
    "\n",
    "# Apply the correction function to ensure Type A rows are properly updated\n",
    "type_a_b = df_cleaned[df_cleaned['Type'].isin(['A', 'B'])]\n",
    "type_a_b_grouped = type_a_b.groupby('Policy No')\n",
    "\n",
    "df_cleaned = type_a_b_grouped.apply(correct_booked).reset_index(drop=True)\n",
    "\n",
    "# Output the number of corrections made\n",
    "print(f\"Number of corrections made: {correction_count}\")\n",
    "\n",
    "# Optional Step: Save the cleaned dataset back to the database\n",
    "df_cleaned.to_sql('cleaned_test_data_12_12', con=engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Database connection setup\n",
    "db_config = {\n",
    "    'host': 'localhost',\n",
    "    'database': 'postgres',\n",
    "    'user': 'postgres',\n",
    "    'password': 'kaviyam123',\n",
    "    'port': '5432'\n",
    "}\n",
    "connection_string = f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['database']}\"\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Step 1: Load data from PostgreSQL\n",
    "query = \"SELECT * FROM public.cleaned_test_data_12_12;\"\n",
    "df = pd.read_sql(query, con=engine)\n",
    "\n",
    "# Function to clean names\n",
    "def clean_name(name):\n",
    "    return re.sub(r'[^a-zA-Z0-9]', '', str(name)).lower()\n",
    "\n",
    "# Clean and generate CustomerID\n",
    "df['Cleaned_Insured name'] = df['Insured name '].apply(clean_name)\n",
    "df['CustomerID_Base'] = (df['Cleaned_Insured name'].astype(str) + '_' +\n",
    "                         df['New Branch Name  2'].astype(str))\n",
    "df['CustomerID'] = (df.groupby('CustomerID_Base').ngroup() + 1000001).astype(str)\n",
    "\n",
    "# Convert dates to datetime\n",
    "df['Policy Start Date'] = pd.to_datetime(df['Policy Start Date'], errors='coerce')\n",
    "df['Policy End Date'] = pd.to_datetime(df['Policy End Date'], errors='coerce')\n",
    "\n",
    "# Map BOOKED values to Policy Status\n",
    "df['booked'] = df['booked'].astype(str).str.strip()  # Ensure values are strings and remove extra spaces\n",
    "df['booked'] = df['booked'].replace({'0.0': '0', '1.0': '1'})  # Normalize float-like strings to integers\n",
    "\n",
    "policy_status_map = {'0': 'Not Renewed', '1': 'Renewed', '-': 'Open'}\n",
    "df['Policy Status'] = df['booked'].map(policy_status_map)\n",
    "\n",
    "# Step 2: Calculate Policy Tenure (Months) for each individual policy\n",
    "df['Policy Tenure Month'] = ((df['Policy End Date'].dt.year - df['Policy Start Date'].dt.year) * 12 +\n",
    "                             (df['Policy End Date'].dt.month - df['Policy Start Date'].dt.month))\n",
    "\n",
    "# Calculate policy tenure in years (rounded)\n",
    "df['Policy Tenure'] = (df['Policy Tenure Month'] / 12).round(0)\n",
    "\n",
    "# Step 2: Extract Start Year for grouping\n",
    "df['Start Year'] = df['Policy Start Date'].dt.year\n",
    "\n",
    "# Extract the year from start and end dates for other calculations\n",
    "df['End Year'] = df['Policy End Date'].dt.year\n",
    "\n",
    "# Step 3: Group by CustomerID and Start Year to calculate min start date, max end date, and yearly tenure\n",
    "yearly_tenure = (\n",
    "    df.groupby(['CustomerID', 'Start Year'])\n",
    "    .agg({'Policy Start Date': 'min', 'Policy End Date': 'max'})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Calculate Yearly Tenure (Months)\n",
    "yearly_tenure['Yearly Tenure (Months)'] = (\n",
    "    (yearly_tenure['Policy End Date'].dt.year - yearly_tenure['Policy Start Date'].dt.year) * 12 +\n",
    "    (yearly_tenure['Policy End Date'].dt.month - yearly_tenure['Policy Start Date'].dt.month)\n",
    ")\n",
    "\n",
    "# Step 4: Calculate Cumulative Tenure\n",
    "yearly_tenure['Cumulative Tenure (Months)'] = (\n",
    "    yearly_tenure.groupby('CustomerID')['Yearly Tenure (Months)']\n",
    "    .cumsum()\n",
    ")\n",
    "\n",
    "# Convert Cumulative Tenure to years and calculate Customer Tenure\n",
    "yearly_tenure['Tenure Decimal'] = yearly_tenure['Cumulative Tenure (Months)'] / 12\n",
    "yearly_tenure['Customer Tenure'] = yearly_tenure['Tenure Decimal'].round(0)\n",
    "\n",
    "# Select relevant columns for mapping back to original data\n",
    "tenure_mapping = yearly_tenure[['CustomerID', 'Start Year', 'Cumulative Tenure (Months)', 'Tenure Decimal', 'Customer Tenure']]\n",
    "\n",
    "# Step 5: Map back to the original data\n",
    "df = df.merge(tenure_mapping, on=['CustomerID', 'Start Year'], how='left')\n",
    "\n",
    "# Step 8: Add New Customers column\n",
    "df['FirstPolicyYear'] = df.groupby('CustomerID')['Start Year'].transform('min')\n",
    "df['New_Customer_ID'] = df.apply(\n",
    "    lambda row: f\"{row['FirstPolicyYear']}_{row['CustomerID']}\" if row['Start Year'] == row['FirstPolicyYear'] else '',\n",
    "    axis=1\n",
    ")\n",
    "df['New Customers'] = df['New_Customer_ID'].apply(lambda x: 'Yes' if x else 'No')\n",
    "\n",
    "# Step 10: Calculate year-wise churn status\n",
    "def calculate_churn_status(group):\n",
    "    unique_statuses = group.unique()\n",
    "    if len(unique_statuses) == 1 and unique_statuses[0] == 'Not Renewed':\n",
    "        return 'Yes'\n",
    "    else:\n",
    "        return 'No'\n",
    "\n",
    "df['Churn Label'] = df.groupby(['CustomerID', 'End Year'])['Policy Status'].transform(lambda x: calculate_churn_status(x))\n",
    "\n",
    "from sqlalchemy import text\n",
    "\n",
    "# Save the processed data into PostgreSQL\n",
    "processed_table_name = 'overall_policy_level_data_EF'  # Target table name\n",
    "\n",
    "# Create a connection to the database\n",
    "with engine.connect() as connection:\n",
    "    # Drop the table if it exists\n",
    "    drop_query = f\"DROP TABLE IF EXISTS {processed_table_name};\"\n",
    "    connection.execute(text(drop_query))  # Execute the drop statement\n",
    "    print(f\"Table {processed_table_name} dropped successfully.\")\n",
    "\n",
    "    # Load the new data into the table\n",
    "    df.to_sql(processed_table_name, con=engine, if_exists='replace', index=False)\n",
    "    print(f\"Data loaded into {processed_table_name} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_18180\\532672589.py:35: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(prioritize_rows)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_18180\\532672589.py:67: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_cleaned = type_a_b_grouped.apply(correct_booked).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of corrections made: 49\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "332"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Database connection setup\n",
    "db_config = {\n",
    "    'host': 'localhost',\n",
    "    'database': 'postgres',\n",
    "    'user': 'postgres',\n",
    "    'password': 'kaviyam123',\n",
    "    'port': '5432'\n",
    "}\n",
    "connection_string = f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['database']}\"\n",
    "\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Step 1: Load data\n",
    "query = \"SELECT * FROM test_data_12_12;\"\n",
    "df = pd.read_sql(query, con=engine)\n",
    "\n",
    "# Step 2: Parse datetime columns (simplified)\n",
    "df['Policy Start Date'] = pd.to_datetime(df['Policy Start Date'], errors='coerce')\n",
    "df['Policy End Date'] = pd.to_datetime(df['Policy End Date'], errors='coerce')\n",
    "\n",
    "# Step 3: Deduplicate and prioritize\n",
    "duplicates = df[df.duplicated(subset=['Policy No', 'Policy Start Date', 'Policy End Date'], keep=False)]\n",
    "\n",
    "def prioritize_rows(group):\n",
    "    group = group.assign(null_count=group.isnull().sum(axis=1))\n",
    "    group = group.sort_values(by=['null_count', 'booked'], ascending=[True, False])\n",
    "    return group.iloc[0]\n",
    "\n",
    "cleaned_duplicates = (\n",
    "    duplicates.groupby(['Policy No', 'Policy Start Date', 'Policy End Date'])\n",
    "    .apply(prioritize_rows)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "df_cleaned = pd.concat([df, cleaned_duplicates]).drop_duplicates(subset=['Policy No', 'Policy Start Date', 'Policy End Date'], keep='last')\n",
    "\n",
    "# Step 4: Handle NULL values in BOOKED\n",
    "today = pd.Timestamp(datetime.now().date())\n",
    "\n",
    "df_cleaned['booked'] = df_cleaned['booked'].fillna(\n",
    "    df_cleaned['Policy End Date'].apply(\n",
    "        lambda x: 0 if pd.notnull(x) and x < today else '-'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Step 5: Correct BOOKED values based on Type\n",
    "correction_count = 0\n",
    "\n",
    "def correct_booked(group):\n",
    "    global correction_count\n",
    "    type_a = group[group['Type'] == 'A']\n",
    "    type_b = group[group['Type'] == 'B']\n",
    "    \n",
    "    if not type_a.empty and not type_b.empty:\n",
    "        if (type_a.iloc[0]['booked'] == 0):\n",
    "            correction_count += 1\n",
    "            group.loc[group['Type'] == 'A', 'booked'] = 1\n",
    "    return group\n",
    "\n",
    "type_a_b = df_cleaned[df_cleaned['Type'].isin(['A', 'B'])]\n",
    "type_a_b_grouped = type_a_b.groupby('Policy No')\n",
    "\n",
    "df_cleaned = type_a_b_grouped.apply(correct_booked).reset_index(drop=True)\n",
    "\n",
    "# Output the number of corrections made\n",
    "print(f\"Number of corrections made: {correction_count}\")\n",
    "\n",
    "# Save the cleaned dataset back to the database\n",
    "df_cleaned.to_sql('cleaned_test_data_12_12', con=engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table overall_policy_level_data_EF dropped successfully.\n",
      "Data loaded into overall_policy_level_data_EF successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Database connection setup\n",
    "db_config = {\n",
    "    'host': 'localhost',\n",
    "    'database': 'postgres',\n",
    "    'user': 'postgres',\n",
    "    'password': 'kaviyam123',\n",
    "    'port': '5432'\n",
    "}\n",
    "connection_string = f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['database']}\"\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Step 1: Load data from PostgreSQL\n",
    "query = \"SELECT * FROM public.cleaned_test_data_12_12;\"\n",
    "df = pd.read_sql(query, con=engine)\n",
    "\n",
    "# Function to clean names\n",
    "def clean_name(name):\n",
    "    return re.sub(r'[^a-zA-Z0-9]', '', str(name)).lower()\n",
    "\n",
    "# Clean and generate CustomerID\n",
    "df['Cleaned_Insured name'] = df['Insured name '].apply(clean_name)\n",
    "df['CustomerID_Base'] = (df['Cleaned_Insured name'].astype(str) + '_' +\n",
    "                         df['New Branch Name  2'].astype(str))\n",
    "df['CustomerID'] = (df.groupby('CustomerID_Base').ngroup() + 1000001).astype(str)\n",
    "\n",
    "# Convert dates to datetime\n",
    "df['Policy Start Date'] = pd.to_datetime(df['Policy Start Date'], errors='coerce')\n",
    "df['Policy End Date'] = pd.to_datetime(df['Policy End Date'], errors='coerce')\n",
    "\n",
    "# Map BOOKED values to Policy Status\n",
    "df['booked'] = df['booked'].astype(str).str.strip()  # Ensure values are strings and remove extra spaces\n",
    "df['booked'] = df['booked'].replace({'0.0': '0', '1.0': '1'})  # Normalize float-like strings to integers\n",
    "\n",
    "policy_status_map = {'0': 'Not Renewed', '1': 'Renewed', '-': 'Open'}\n",
    "df['Policy Status'] = df['booked'].map(policy_status_map)\n",
    "\n",
    "# Step 2: Calculate Policy Tenure (Months) for each individual policy\n",
    "df['Policy Tenure Month'] = ((df['Policy End Date'].dt.year - df['Policy Start Date'].dt.year) * 12 +\n",
    "                             (df['Policy End Date'].dt.month - df['Policy Start Date'].dt.month))\n",
    "\n",
    "# Calculate policy tenure in years (rounded)\n",
    "df['Policy Tenure'] = (df['Policy Tenure Month'] / 12).round(0)\n",
    "\n",
    "# Step 2: Extract Start Year for grouping\n",
    "df['Start Year'] = df['Policy Start Date'].dt.year\n",
    "\n",
    "# Extract the year from start and end dates for other calculations\n",
    "df['End Year'] = df['Policy End Date'].dt.year\n",
    "\n",
    "# Step 3: Group by CustomerID and Start Year to calculate min start date, max end date, and yearly tenure\n",
    "yearly_tenure = (\n",
    "    df.groupby(['CustomerID', 'Start Year'])\n",
    "    .agg({'Policy Start Date': 'min', 'Policy End Date': 'max'})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Calculate Yearly Tenure (Months)\n",
    "yearly_tenure['Yearly Tenure (Months)'] = (\n",
    "    (yearly_tenure['Policy End Date'].dt.year - yearly_tenure['Policy Start Date'].dt.year) * 12 +\n",
    "    (yearly_tenure['Policy End Date'].dt.month - yearly_tenure['Policy Start Date'].dt.month)\n",
    ")\n",
    "\n",
    "# Step 4: Calculate Cumulative Tenure\n",
    "yearly_tenure['Cumulative Tenure (Months)'] = (\n",
    "    yearly_tenure.groupby('CustomerID')['Yearly Tenure (Months)']\n",
    "    .cumsum()\n",
    ")\n",
    "\n",
    "# Convert Cumulative Tenure to years and calculate Customer Tenure\n",
    "yearly_tenure['Tenure Decimal'] = yearly_tenure['Cumulative Tenure (Months)'] / 12\n",
    "yearly_tenure['Customer Tenure'] = yearly_tenure['Tenure Decimal'].round(0)\n",
    "\n",
    "# Select relevant columns for mapping back to original data\n",
    "tenure_mapping = yearly_tenure[['CustomerID', 'Start Year', 'Cumulative Tenure (Months)', 'Tenure Decimal', 'Customer Tenure']]\n",
    "\n",
    "# Step 5: Map back to the original data\n",
    "df = df.merge(tenure_mapping, on=['CustomerID', 'Start Year'], how='left')\n",
    "\n",
    "# Step 8: Add New Customers column\n",
    "df['FirstPolicyYear'] = df.groupby('CustomerID')['Start Year'].transform('min')\n",
    "df['New_Customer_ID'] = df.apply(\n",
    "    lambda row: f\"{row['FirstPolicyYear']}_{row['CustomerID']}\" if row['Start Year'] == row['FirstPolicyYear'] else '',\n",
    "    axis=1\n",
    ")\n",
    "df['New Customers'] = df['New_Customer_ID'].apply(lambda x: 'Yes' if x else 'No')\n",
    "\n",
    "# Step 10: Calculate year-wise churn status\n",
    "def calculate_churn_status(group):\n",
    "    unique_statuses = group.unique()\n",
    "    if len(unique_statuses) == 1 and unique_statuses[0] == 'Not Renewed':\n",
    "        return 'Yes'\n",
    "    else:\n",
    "        return 'No'\n",
    "\n",
    "df['Churn Label'] = df.groupby(['CustomerID', 'End Year'])['Policy Status'].transform(lambda x: calculate_churn_status(x))\n",
    "\n",
    "from sqlalchemy import text\n",
    "\n",
    "# Save the processed data into PostgreSQL\n",
    "processed_table_name = 'overall_policy_level_data_EF'  # Target table name\n",
    "\n",
    "# Create a connection to the database\n",
    "with engine.connect() as connection:\n",
    "    # Drop the table if it exists\n",
    "    drop_query = f\"DROP TABLE IF EXISTS {processed_table_name};\"\n",
    "    connection.execute(text(drop_query))  # Execute the drop statement\n",
    "    print(f\"Table {processed_table_name} dropped successfully.\")\n",
    "\n",
    "    # Load the new data into the table\n",
    "    df.to_sql(processed_table_name, con=engine, if_exists='replace', index=False)\n",
    "    print(f\"Data loaded into {processed_table_name} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

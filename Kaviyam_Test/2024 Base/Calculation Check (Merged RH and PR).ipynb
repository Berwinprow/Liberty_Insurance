{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matching Policy Nos: 160852\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Define database connection parameters\n",
    "db_params = {\n",
    "    'dbname': 'postgres',\n",
    "    'user': 'postgres',\n",
    "    'password': 'kaviyam123',\n",
    "    'host': 'localhost',\n",
    "    'port': '5432'\n",
    "}\n",
    "\n",
    "excel_df = pd.read_excel('cleaned_PR dataset.xlsx')\n",
    "\n",
    "# Rename column to match with database for easier comparison\n",
    "excel_df.rename(columns={'Initial Policy No': 'Policy No'}, inplace=True)\n",
    "\n",
    "# Create a database connection\n",
    "try:\n",
    "    engine = create_engine(f\"postgresql://{db_params['user']}:{db_params['password']}@{db_params['host']}:{db_params['port']}/{db_params['dbname']}\")\n",
    "    with engine.connect() as conn:\n",
    "        # Load Policy No from the database\n",
    "        db_query = \"SELECT DISTINCT \\\"Policy No\\\" FROM public.check_test_data_06_12;\"\n",
    "        db_df = pd.read_sql(db_query, conn)\n",
    "\n",
    "        # Find matching Policy Nos\n",
    "        matching_policies = excel_df['Policy No'].isin(db_df['Policy No'])\n",
    "        match_count = matching_policies.sum()\n",
    "\n",
    "        print(f\"Number of matching Policy Nos: {match_count}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "finally:\n",
    "    engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2\n",
      "  Downloading psycopg2-2.9.10-cp312-cp312-win_amd64.whl.metadata (5.0 kB)\n",
      "Downloading psycopg2-2.9.10-cp312-cp312-win_amd64.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/1.2 MB 682.7 kB/s eta 0:00:02\n",
      "   ------- -------------------------------- 0.2/1.2 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.5/1.2 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 0.7/1.2 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 0.9/1.2 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.2/1.2 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.2/1.2 MB 3.9 MB/s eta 0:00:00\n",
      "Installing collected packages: psycopg2\n",
      "Successfully installed psycopg2-2.9.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_8692\\1484288109.py:34: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(prioritize_rows)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_8692\\1484288109.py:68: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_cleaned = type_a_b_grouped.apply(correct_booked).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of corrections made: 49\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "404"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime\n",
    "\n",
    "# Database connection setup\n",
    "db_config = {\n",
    "    'host': 'localhost',\n",
    "    'database': 'postgres',\n",
    "    'user': 'postgres',\n",
    "    'password': 'kaviyam123',\n",
    "    'port': '5432'\n",
    "}\n",
    "connection_string = f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['database']}\"\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Step 1: Load data\n",
    "query = \"SELECT * FROM test_data_06_12;\"\n",
    "df = pd.read_sql(query, con=engine)\n",
    "\n",
    "# Step 2: Convert dates to datetime format\n",
    "df['Policy Start Date'] = pd.to_datetime(df['Policy Start Date'])\n",
    "df['Policy End Date'] = pd.to_datetime(df['Policy End Date'])\n",
    "\n",
    "# Step 3: Deduplicate and prioritize\n",
    "duplicates = df[df.duplicated(subset=['Policy No', 'Policy Start Date', 'Policy End Date'], keep=False)]\n",
    "\n",
    "def prioritize_rows(group):\n",
    "    group = group.assign(null_count=group.isnull().sum(axis=1))\n",
    "    group = group.sort_values(by=['null_count', 'booked'], ascending=[True, False])\n",
    "    return group.iloc[0]\n",
    "\n",
    "cleaned_duplicates = (\n",
    "    duplicates.groupby(['Policy No', 'Policy Start Date', 'Policy End Date'])\n",
    "    .apply(prioritize_rows)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "df_cleaned = pd.concat([df, cleaned_duplicates]).drop_duplicates(subset=['Policy No', 'Policy Start Date', 'Policy End Date'], keep='last')\n",
    "\n",
    "# Step 4: Handle NULL values in BOOKED\n",
    "today = datetime.now().date()\n",
    "df_cleaned['booked'] = df_cleaned.apply(\n",
    "    lambda row: 0 if pd.isnull(row['booked']) and row['Policy End Date'].date() < today else ('-' if pd.isnull(row['booked']) else row['booked']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Step 5: Correct BOOKED values based on Type\n",
    "correction_count = 0\n",
    "\n",
    "def correct_booked(group):\n",
    "    global correction_count\n",
    "    type_a = group[group['Type'] == 'A']\n",
    "    type_b = group[group['Type'] == 'B']\n",
    "    \n",
    "    # Check if both Type A and Type B exist for the same Policy No\n",
    "    if not type_a.empty and not type_b.empty:\n",
    "        # Check if BOOKED is 0 for Type A\n",
    "        if (type_a.iloc[0]['booked'] == 0):\n",
    "            correction_count += 1\n",
    "            # Update BOOKED to 1 for Type A\n",
    "            group.loc[group['Type'] == 'A', 'booked'] = 1\n",
    "    return group\n",
    "\n",
    "# Apply the correction function to ensure Type A rows are properly updated\n",
    "type_a_b = df_cleaned[df_cleaned['Type'].isin(['A', 'B'])]\n",
    "type_a_b_grouped = type_a_b.groupby('Policy No')\n",
    "\n",
    "df_cleaned = type_a_b_grouped.apply(correct_booked).reset_index(drop=True)\n",
    "\n",
    "# Output the number of corrections made\n",
    "print(f\"Number of corrections made: {correction_count}\")\n",
    "\n",
    "# Optional Step: Save the cleaned dataset back to the database\n",
    "df_cleaned.to_sql('cleaned_test_data_06_12', con=engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m policy_no_with_b \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mType\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPolicy No\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Add the `Match` column to the original DataFrame\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMatch\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPolicy No\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMatches B\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m policy_no_with_b \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Step 3: Separate rows with non-numeric `Total Premium Payable`\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Function to identify non-numeric values in the `Total Premium Payable` column\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_non_numeric\u001b[39m(value):\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4918\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4919\u001b[0m         func,\n\u001b[0;32m   4920\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[0;32m   4921\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[0;32m   4922\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   4923\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m-> 4924\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[0;32m   1508\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[0;32m   1509\u001b[0m )\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[7], line 24\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     21\u001b[0m policy_no_with_b \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mType\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPolicy No\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Add the `Match` column to the original DataFrame\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMatch\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPolicy No\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMatches B\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m policy_no_with_b \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Step 3: Separate rows with non-numeric `Total Premium Payable`\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Function to identify non-numeric values in the `Total Premium Payable` column\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_non_numeric\u001b[39m(value):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Database connection setup\n",
    "db_config = {\n",
    "    'host': 'localhost',\n",
    "    'database': 'postgres',\n",
    "    'user': 'postgres',\n",
    "    'password': 'kaviyam123',\n",
    "    'port': '5432'\n",
    "}\n",
    "connection_string = f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['database']}\"\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Step 1: Load data from PostgreSQL\n",
    "query = \"SELECT * FROM cleaned_test_data_06_12;\"\n",
    "df = pd.read_sql(query, con=engine)\n",
    "\n",
    "# Step 2: Create a `Match` column for all rows in the dataset\n",
    "# Identify `Policy No` that have corresponding rows with `Type = B`\n",
    "policy_no_with_b = df[df['Type'] == 'B']['Policy No'].unique()\n",
    "\n",
    "# Add the `Match` column to the original DataFrame\n",
    "df['Match'] = df['Policy No'].apply(lambda x: 'Matches B' if x in policy_no_with_b else None)\n",
    "\n",
    "# Step 3: Separate rows with non-numeric `Total Premium Payable`\n",
    "# Function to identify non-numeric values in the `Total Premium Payable` column\n",
    "def is_non_numeric(value):\n",
    "    try:\n",
    "        float(value)  # Attempt to convert to a float\n",
    "        return False  # It's numeric\n",
    "    except ValueError:\n",
    "        return True  # It's non-numeric\n",
    "\n",
    "# Add a helper column to identify non-numeric rows\n",
    "df['is_non_numeric'] = df['Total Premium Payable '].apply(is_non_numeric)\n",
    "\n",
    "# Create a DataFrame for non-numeric rows\n",
    "non_numeric_df = df[df['is_non_numeric']].drop(columns=['is_non_numeric'])\n",
    "\n",
    "# Step 4: Save the data back to PostgreSQL and export as CSV\n",
    "# Save the full dataset with the `Match` column\n",
    "# df.to_sql('updated_data_with_match', con=engine, if_exists='replace', index=False)\n",
    "\n",
    "# Save the non-numeric rows to a separate table\n",
    "# non_numeric_df.to_sql('non_numeric_total_premium_with_match', con=engine, if_exists='replace', index=False)\n",
    "\n",
    "# Export to CSV for validation (optional)\n",
    "# df.to_csv('updated_data_with_match.csv', index=False)\n",
    "non_numeric_df.to_csv('non_numeric_total_premium_with_match.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "bad operand type for unary ~: 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m df\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mstrip(), inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Use vectorized operation to identify non-numeric values\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m non_numeric_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal Premium Payable\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39misdigit()\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Create a DataFrame for non-numeric rows\u001b[39;00m\n\u001b[0;32m     34\u001b[0m non_numeric_df \u001b[38;5;241m=\u001b[39m df[non_numeric_mask]\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:1571\u001b[0m, in \u001b[0;36mNDFrame.__invert__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize:\n\u001b[0;32m   1568\u001b[0m     \u001b[38;5;66;03m# inv fails with 0 len\u001b[39;00m\n\u001b[0;32m   1569\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m-> 1571\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mapply(operator\u001b[38;5;241m.\u001b[39minvert)\n\u001b[0;32m   1572\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m   1573\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__invert__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:361\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[0;32m    358\u001b[0m             kwargs[k] \u001b[38;5;241m=\u001b[39m obj[b\u001b[38;5;241m.\u001b[39mmgr_locs\u001b[38;5;241m.\u001b[39mindexer]\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(f):\n\u001b[1;32m--> 361\u001b[0m     applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    363\u001b[0m     applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(b, f)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:393\u001b[0m, in \u001b[0;36mBlock.apply\u001b[1;34m(self, func, **kwargs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Block]:\n\u001b[0;32m    389\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;124;03m    apply the function to my values; return a block if we are not\u001b[39;00m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;124;03m    one\u001b[39;00m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 393\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    395\u001b[0m     result \u001b[38;5;241m=\u001b[39m maybe_coerce_values(result)\n\u001b[0;32m    396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_op_result(result)\n",
      "\u001b[1;31mTypeError\u001b[0m: bad operand type for unary ~: 'NoneType'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Database connection setup\n",
    "db_config = {\n",
    "    'host': 'localhost',\n",
    "    'database': 'postgres',\n",
    "    'user': 'postgres',\n",
    "    'password': 'kaviyam123', \n",
    "    'port': '5432'\n",
    "}\n",
    "connection_string = f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['database']}\"\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Step 1: Load data from PostgreSQL\n",
    "query = \"SELECT * FROM cleaned_test_data_06_12;\"\n",
    "df = pd.read_sql(query, con=engine)\n",
    "\n",
    "# Step 2: Create a `Match` column for all rows in the dataset\n",
    "# Identify `Policy No` that have corresponding rows with `Type = B`\n",
    "policy_no_with_b = df[df['Type'] == 'B']['Policy No'].unique()\n",
    "\n",
    "# Add the `Match` column to the original DataFrame\n",
    "df['Match'] = df['Policy No'].isin(policy_no_with_b).apply(lambda x: 'Matches B' if x else None)\n",
    "\n",
    "# Step 3: Efficiently separate rows with non-numeric `Total Premium Payable`\n",
    "# Ensure column name is stripped of extra spaces\n",
    "df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "\n",
    "# Use vectorized operation to identify non-numeric values\n",
    "non_numeric_mask = ~df['Total Premium Payable'].str.replace('.', '', 1).str.isdigit()\n",
    "\n",
    "# Create a DataFrame for non-numeric rows\n",
    "non_numeric_df = df[non_numeric_mask]\n",
    "\n",
    "# Step 4: Save the data back to PostgreSQL and export as CSV\n",
    "# Save the full dataset with the `Match` column\n",
    "# df.to_sql('updated_data_with_match', con=engine, if_exists='replace', index=False)\n",
    "\n",
    "# Save the non-numeric rows to a separate table\n",
    "# non_numeric_df.to_sql('non_numeric_total_premium_with_match', con=engine, if_exists='replace', index=False)\n",
    "\n",
    "# Export to CSV for validation (optional)\n",
    "# df.to_csv('updated_data_with_match.csv', index=False)\n",
    "non_numeric_df.to_csv('non_numeric_total_premium_with_match.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Database connection setup\n",
    "db_config = {\n",
    "    'host': 'localhost',\n",
    "    'database': 'postgres',\n",
    "    'user': 'postgres',\n",
    "    'password': 'kaviyam123',\n",
    "    'port': '5432'\n",
    "}\n",
    "connection_string = f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['database']}\"\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Step 1: Load data from PostgreSQL\n",
    "query = \"SELECT * FROM cleaned_test_data_06_12;\"\n",
    "df = pd.read_sql(query, con=engine)\n",
    "\n",
    "# Step 2: Create a `Match` column for all rows in the dataset\n",
    "policy_no_with_b = df[df['Type'] == 'B']['Policy No'].unique()\n",
    "df['Match'] = df['Policy No'].isin(policy_no_with_b).apply(lambda x: 'Matches B' if x else None)\n",
    "\n",
    "# Step 3: Clean column names\n",
    "df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "\n",
    "# Step 4: Handle missing values and identify non-numeric rows\n",
    "# Fill missing values with a placeholder string\n",
    "df['Total Premium Payable'] = df['Total Premium Payable'].fillna('')\n",
    "\n",
    "# Use vectorized operations to identify non-numeric rows\n",
    "non_numeric_mask = ~df['Total Premium Payable'].str.replace('.', '', 1).str.isdigit()\n",
    "\n",
    "# Create a DataFrame for non-numeric rows\n",
    "non_numeric_df = df[non_numeric_mask]\n",
    "\n",
    "# Step 5: Save the data back to PostgreSQL and export as CSV\n",
    "# df.to_sql('updated_data_with_match', con=engine, if_exists='replace', index=False)\n",
    "# non_numeric_df.to_sql('non_numeric_total_premium_with_match', con=engine, if_exists='replace', index=False)\n",
    "\n",
    "# df.to_csv('updated_data_with_match.csv', index=False)\n",
    "non_numeric_df.to_csv('non_numeric_total_premium_with_match.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The distinct count of Policy No across both sources is: 1164060\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Database connection setup\n",
    "db_config = {\n",
    "    'host': 'localhost',\n",
    "    'database': 'postgres',\n",
    "    'user': 'postgres',\n",
    "    'password': 'kaviyam123',\n",
    "    'port': '5432'\n",
    "}\n",
    "connection_string = f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['database']}\"\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Step 1: Retrieve Policy No from PostgreSQL\n",
    "sql_query = 'SELECT DISTINCT \"Policy No\" FROM public.check_test_data_06_12;'\n",
    "sql_data = pd.read_sql(sql_query, con=engine)\n",
    "\n",
    "# Step 2: Read the Excel file\n",
    "excel_data = pd.read_excel(\"cleaned_PR dataset - Copy.xlsx\", usecols=[\"Policy No\"])\n",
    "\n",
    "# Step 3: Combine and find distinct Policy Nos\n",
    "combined_data = pd.concat([sql_data, excel_data]).drop_duplicates()\n",
    "\n",
    "# Step 4: Count distinct Policy Nos\n",
    "distinct_count = combined_data[\"Policy No\"].nunique()\n",
    "\n",
    "# Display the result\n",
    "print(f\"The distinct count of Policy No across both sources is: {distinct_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The distinct count of Policy No across both sources is: 1164060\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Database connection setup\n",
    "db_config = {\n",
    "    'host': 'localhost',\n",
    "    'database': 'postgres',\n",
    "    'user': 'postgres',\n",
    "    'password': 'kaviyam123',\n",
    "    'port': '5432'\n",
    "}\n",
    "connection_string = f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['database']}\"\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Step 1: Retrieve Policy No from PostgreSQL\n",
    "sql_query = 'SELECT DISTINCT \"Policy No\" FROM cleaned_merged_base_data_check;'\n",
    "sql_data = pd.read_sql(sql_query, con=engine)\n",
    "\n",
    "# Step 2: Read the Excel file\n",
    "excel_data = pd.read_excel(\"cleaned_PR dataset - Copy.xlsx\", usecols=[\"Policy No\"])\n",
    "\n",
    "# Step 3: Combine and find distinct Policy Nos\n",
    "combined_data = pd.concat([sql_data, excel_data]).drop_duplicates()\n",
    "\n",
    "# Step 4: Count distinct Policy Nos\n",
    "distinct_count = combined_data[\"Policy No\"].nunique()\n",
    "\n",
    "# Display the result\n",
    "print(f\"The distinct count of Policy No across both sources is: {distinct_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of matching Policy Nos between both sources is: 157233\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Database connection setup\n",
    "db_config = {\n",
    "    'host': 'localhost',\n",
    "    'database': 'postgres',\n",
    "    'user': 'postgres',\n",
    "    'password': 'kaviyam123',\n",
    "    'port': '5432'\n",
    "}\n",
    "connection_string = f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['database']}\"\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Step 1: Retrieve Policy No from PostgreSQL\n",
    "sql_query = 'SELECT DISTINCT \"Policy No\" FROM cleaned_merged_base_data_check;'\n",
    "sql_data = pd.read_sql(sql_query, con=engine)\n",
    "\n",
    "# Step 2: Read the Excel file\n",
    "excel_data = pd.read_excel(\"cleaned_PR dataset - Copy.xlsx\", usecols=[\"Policy No\"])\n",
    "\n",
    "# Step 3: Find matching policies\n",
    "# Convert to sets for easy comparison\n",
    "sql_policy_set = set(sql_data[\"Policy No\"])\n",
    "excel_policy_set = set(excel_data[\"Policy No\"])\n",
    "\n",
    "# Find intersection (matching policies)\n",
    "matching_policies = sql_policy_set.intersection(excel_policy_set)\n",
    "\n",
    "# Step 4: Count matches\n",
    "matching_count = len(matching_policies)\n",
    "\n",
    "# Display the result\n",
    "print(f\"The number of matching Policy Nos between both sources is: {matching_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

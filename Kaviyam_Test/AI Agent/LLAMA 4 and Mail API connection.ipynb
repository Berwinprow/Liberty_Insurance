{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb620bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kaviyam\\AppData\\Local\\Temp\\ipykernel_8348\\2645428228.py:60: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(f\"SELECT * FROM {SOURCE_TABLE}\", conn)\n",
      "C:\\Users\\Kaviyam\\AppData\\Local\\Temp\\ipykernel_8348\\2645428228.py:86: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeds = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
      "2025-08-18 10:41:51,069 | INFO | Use pytorch device_name: cuda:0\n",
      "2025-08-18 10:41:51,071 | INFO | Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n",
      "2025-08-18 10:42:00,977 | INFO | Loading faiss with AVX2 support.\n",
      "2025-08-18 10:42:01,408 | INFO | Successfully loaded faiss with AVX2 support.\n",
      "2025-08-18 10:42:01,426 | INFO | Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-18 10:42:02,440 | INFO | HTTP Request: GET https://checkip.amazonaws.com/ \"HTTP/1.1 200 \"\n",
      "2025-08-18 10:42:02,472 | INFO | HTTP Request: GET https://checkip.amazonaws.com/ \"HTTP/1.1 200 \"\n",
      "2025-08-18 10:42:02,573 | INFO | HTTP Request: GET http://127.0.0.1:7860/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-08-18 10:42:02,941 | INFO | HTTP Request: HEAD http://127.0.0.1:7860/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-18 10:42:03,301 | INFO | HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-08-18 10:42:03,324 | INFO | HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "C:\\Users\\Kaviyam\\AppData\\Local\\Temp\\ipykernel_8348\\2645428228.py:93: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = schema_retriever.get_relevant_documents(question)\n",
      "2025-08-18 10:45:05,973 | INFO | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-18 10:45:06,026 | INFO | Generated SQL:\n",
      "SELECT churn_top_3_reasons FROM bi_dwh.prediction_agent_data \n",
      "WHERE UPPER(REPLACE(REGEXP_REPLACE(policy_no::text, '^''', ''), ' ', '')) = '201130140823100001101000';\n",
      "2025-08-18 10:45:06,026 | INFO | Corrected SQL:\n",
      "SELECT * FROM bi_dwh.prediction_agent_data\n",
      "WHERE UPPER(REPLACE(REGEXP_REPLACE(policy_no::text, '^''', ''), ' ', '')) = '201130140823100001101000';\n",
      "2025-08-18 10:45:07,639 | INFO | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-18 10:45:57,318 | INFO | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-18 10:45:57,335 | INFO | Generated SQL:\n",
      "SELECT * FROM bi_dwh.prediction_agent_data \n",
      "WHERE UPPER(REPLACE(REGEXP_REPLACE(policy_no::text, '^''', ''), ' ', '')) = '201130140823100001101000';\n",
      "2025-08-18 10:45:57,338 | INFO | Corrected SQL:\n",
      "SELECT * FROM bi_dwh.prediction_agent_data\n",
      "WHERE UPPER(REPLACE(REGEXP_REPLACE(policy_no::text, '^''', ''), ' ', '')) = '201130140823100001101000';\n",
      "2025-08-18 10:45:59,202 | INFO | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-18 10:46:21,721 | INFO | file_cache is only supported with oauth2client<4.0.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import logging\n",
    "import difflib\n",
    "from typing import TypedDict, Optional, Tuple, Dict, Any, List\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "\n",
    "# ---------------- Gmail imports (NEW) ----------------\n",
    "import base64\n",
    "from email.mime.text import MIMEText\n",
    "from typing import Optional as _Opt\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "# ----------------------------------------------------\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\")\n",
    "\n",
    "# ========== ENV ==========\n",
    "load_dotenv()\n",
    "DB_HOST = os.getenv(\"DB_HOST\")\n",
    "DB_PORT = os.getenv(\"DB_PORT\", \"5432\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\")\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\")\n",
    "SOURCE_TABLE = os.getenv(\"SOURCE_TABLE\", \"bi_dwh.prediction_agent_data\")\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "GROQ_MODEL = \"meta-llama/llama-4-maverick-17b-128e-instruct\"\n",
    "\n",
    "# ---- Gmail env (NEW) ----\n",
    "GMAIL_CREDENTIALS_FILE = os.getenv(\"GMAIL_CREDENTIALS_FILE\", \"credentials.json\")\n",
    "GMAIL_TOKEN_FILE = os.getenv(\"GMAIL_TOKEN_FILE\", \"token.json\")\n",
    "SENDER_EMAIL = os.getenv(\"SENDER_EMAIL\", \"\")\n",
    "DEFAULT_TO_EMAIL = os.getenv(\"DEFAULT_TO_EMAIL\", \"\")\n",
    "# ------------------------\n",
    "\n",
    "CUSTOMER_ID_COL = \"customerid\"\n",
    "POLICY_NO_COL = \"policy_no\"\n",
    "\n",
    "# ========== DB ==========\n",
    "def fetch_customer_data() -> pd.DataFrame:\n",
    "    conn = psycopg2.connect(\n",
    "        host=DB_HOST, port=DB_PORT, dbname=DB_NAME, user=DB_USER, password=DB_PASSWORD\n",
    "    )\n",
    "    try:\n",
    "        df = pd.read_sql(f\"SELECT * FROM {SOURCE_TABLE}\", conn)\n",
    "    finally:\n",
    "        conn.close()\n",
    "    df.columns = df.columns.str.lower()\n",
    "    return df\n",
    "\n",
    "# LLM\n",
    "llm = ChatGroq(model=GROQ_MODEL, api_key=GROQ_API_KEY, temperature=0)\n",
    "\n",
    "# Load table once (columns + sample fallback)\n",
    "df_all_customers = fetch_customer_data()\n",
    "all_cols = list(df_all_customers.columns)\n",
    "\n",
    "# ========== Tiny schema RAG (embed only column names + one sample value) ==========\n",
    "def build_schema_docs(df: pd.DataFrame) -> FAISS:\n",
    "    texts = []\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            sample_val = df[col].dropna().iloc[0]\n",
    "        except Exception:\n",
    "            sample_val = None\n",
    "        desc = f\"{col}: column in {SOURCE_TABLE}\"\n",
    "        if sample_val is not None:\n",
    "            desc += f\" | sample_value={str(sample_val)[:120]}\"\n",
    "        texts.append(desc)\n",
    "    docs = [Document(page_content=t) for t in texts]\n",
    "    embeds = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "    return FAISS.from_documents(docs, embeds)\n",
    "\n",
    "schema_vs = build_schema_docs(df_all_customers)\n",
    "schema_retriever = schema_vs.as_retriever(search_kwargs={\"k\": 8})\n",
    "\n",
    "def get_schema_snippets(question: str) -> str:\n",
    "    docs = schema_retriever.get_relevant_documents(question)\n",
    "    return \"\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "# ========== Lightweight in-process memory for follow-ups ==========\n",
    "MEMORY: Dict[str, Any] = {\n",
    "    \"last_kind\": None,        # \"CUSTOMER\" | \"POLICY\" | None\n",
    "    \"last_ident\": None,       # the actual id string\n",
    "    \"last_context\": None,     # flattened row context string\n",
    "    \"last_status\": None,      # \"renewed\" | \"not_renewed\" | \"unknown\"\n",
    "}\n",
    "\n",
    "FOLLOWUP_HINTS = {\n",
    "    \"for this\", \"for that\", \"for above\", \"same\", \"this policy\", \"that policy\",\n",
    "    \"this customer\", \"that customer\", \"for this one\",\n",
    "    \"use the same\", \"continue with this\"\n",
    "}\n",
    "\n",
    "def looks_like_followup(text: str) -> bool:\n",
    "    q = text.lower()\n",
    "    return any(h in q for h in FOLLOWUP_HINTS)\n",
    "\n",
    "# ========== SQL helpers ==========\n",
    "def normalize_policy_for_compare(p: str) -> str:\n",
    "    s = str(p).strip()\n",
    "    if s.startswith((\"'\", '\"')):\n",
    "        s = s[1:]\n",
    "    s = s.replace(\" \", \"\")\n",
    "    return s.upper()\n",
    "\n",
    "def extract_identifier(question: str) -> Tuple[Optional[str], Optional[str]]:\n",
    "    q = question.strip()\n",
    "\n",
    "    # explicit \"policy …\"\n",
    "    m_pol_explicit = re.search(\n",
    "        r\"(?:policy(?:\\s*no)?\\.?\\s*#?:?\\s*[\\\"']?)([A-Za-z0-9\\-]{6,40})\",\n",
    "        q, flags=re.IGNORECASE\n",
    "    )\n",
    "    if m_pol_explicit:\n",
    "        return \"POLICY\", m_pol_explicit.group(1)\n",
    "\n",
    "    # customer id (≥5 consecutive digits) – checked BEFORE the fallback\n",
    "    m_cust = re.search(r\"(?:customer\\s*)?(\\d{5,})\", q, flags=re.IGNORECASE)\n",
    "    if m_cust:\n",
    "        return \"CUSTOMER\", m_cust.group(1)\n",
    "\n",
    "    # long-token fallback IF it contains at least one digit (looks number-ish)\n",
    "    m_long = re.search(\n",
    "        r\"[\\\"']?([A-Za-z0-9\\-]{10,40}\\d[A-Za-z0-9\\-]*)\",\n",
    "        q,\n",
    "    )\n",
    "    if m_long:\n",
    "        return \"POLICY\", m_long.group(1)\n",
    "\n",
    "    return None, None\n",
    "\n",
    "def fuzzy_in(text: str, targets, cutoff=0.8) -> bool:\n",
    "    words = [w for w in re.findall(r\"\\w+\", text) if len(w) > 2]\n",
    "    for w in words:\n",
    "        for t in targets:\n",
    "            if len(t) > 2 and difflib.SequenceMatcher(None, w, t).ratio() >= cutoff:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def is_whole_word(word: str, text: str) -> bool:\n",
    "    return re.search(rf\"\\b{re.escape(word)}\\b\", text) is not None\n",
    "\n",
    "def classify_intent(question: str) -> str:\n",
    "    q = question.strip().lower().rstrip(string.punctuation)\n",
    "\n",
    "    greeting_phrases = [\"hi\", \"hello\", \"hey\", \"what can you do\", \"who are you\", \"help\"]\n",
    "    if (fuzzy_in(q, greeting_phrases, 0.8) or any(is_whole_word(p, q) for p in greeting_phrases)):\n",
    "        return \"GREETING\"\n",
    "\n",
    "    email_phrases = [\n",
    "        \"email\", \"e-mail\", \"mail\", \"sms\", \"whatsapp\", \"draft\",\n",
    "        \"compose\", \"send mail\", \"write an email\", \"write to\",\n",
    "        \"message template\", \"text message\"\n",
    "    ]\n",
    "    if (fuzzy_in(q, email_phrases, 0.75) or any(is_whole_word(p, q) for p in email_phrases)):\n",
    "        return \"EMAIL\"\n",
    "\n",
    "    reco_phrases = [\n",
    "        \"recommendation\", \"recommend\", \"strategy\", \"suggest\",\n",
    "        \"advice\", \"plan\", \"retain\", \"how to retain\",\n",
    "        \"next best action\", \"improve renewal\", \"reduce churn\"\n",
    "    ]\n",
    "    if (fuzzy_in(q, reco_phrases, 0.75) or any(is_whole_word(p, q) for p in reco_phrases)):\n",
    "        return \"RECOMMENDATION\"\n",
    "\n",
    "    return \"UNKNOWN\"\n",
    "\n",
    "def generate_sql_with_llm(question: str, kind: Optional[str], ident: Optional[str]) -> str:\n",
    "    schema_snippets = get_schema_snippets(question)\n",
    "    ident_str = ident or \"\"\n",
    "    kind_str = kind or \"UNKNOWN\"\n",
    "\n",
    "    norm_sql_expr = f\"UPPER(REPLACE(REGEXP_REPLACE({POLICY_NO_COL}::text, '^''', ''), ' ', ''))\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You write a single valid PostgreSQL SELECT for table {SOURCE_TABLE}. No markdown.\n",
    "\n",
    "Schema hints:\n",
    "{schema_snippets}\n",
    "\n",
    "Rules:\n",
    "- Use only table {SOURCE_TABLE}.\n",
    "- If the question is customer-specific -> filter as: WHERE {CUSTOMER_ID_COL} = '<id>'.\n",
    "- If the question is policy-specific -> compare normalized policy numbers:\n",
    "  Use: {norm_sql_expr} = '<NORMALIZED_ID>'  -- where NORMALIZED_ID strips leading quote and spaces then UPPER.\n",
    "- Never do JOINs or subqueries. One SELECT only.\n",
    "- Return *only* the final SQL (end with a semicolon).\n",
    "\n",
    "Question: {question}\n",
    "Kind detected: {kind_str}\n",
    "Identifier: {ident_str}\n",
    "\n",
    "Examples:\n",
    "-- customer\n",
    "SELECT * FROM {SOURCE_TABLE} WHERE {CUSTOMER_ID_COL} = '1467520';\n",
    "\n",
    "-- policy (normalize compare)\n",
    "SELECT * FROM {SOURCE_TABLE}\n",
    "WHERE {norm_sql_expr} = '201140020123100158101000';\n",
    "\"\"\"\n",
    "    sql = llm.invoke(prompt).content.strip()\n",
    "    m = re.search(r\"(SELECT\\s.+?;)\", sql, flags=re.IGNORECASE | re.DOTALL)\n",
    "    return m.group(1).strip() if m else sql\n",
    "\n",
    "def execute_sql_fetch_one(sql: str) -> Optional[Dict[str, Any]]:\n",
    "    conn = psycopg2.connect(\n",
    "        host=DB_HOST, port=DB_PORT, dbname=DB_NAME, user=DB_USER, password=DB_PASSWORD\n",
    "    )\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(sql)\n",
    "            row = cur.fetchone()\n",
    "            if row is None:\n",
    "                return None\n",
    "            cols = [desc[0].lower() for desc in cur.description]\n",
    "            return dict(zip(cols, row))\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# NEW: fetch all rows\n",
    "def execute_sql_fetch_all(sql: str) -> List[Dict[str, Any]]:\n",
    "    conn = psycopg2.connect(\n",
    "        host=DB_HOST, port=DB_PORT, dbname=DB_NAME, user=DB_USER, password=DB_PASSWORD\n",
    "    )\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(sql)\n",
    "            rows = cur.fetchall()\n",
    "            if not rows:\n",
    "                return []\n",
    "            cols = [desc[0].lower() for desc in cur.description]\n",
    "            return [dict(zip(cols, r)) for r in rows]\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# NEW: aggregate churn reasons across rows\n",
    "def aggregate_reasons(rows: List[Dict[str, Any]]) -> tuple[list[str], Dict[str, int]]:\n",
    "    reasons_counter = Counter()\n",
    "    reason_cols = [\"churn_main_reason\", \"churn_top_3_reasons\"]\n",
    "    for r in rows:\n",
    "        for c in reason_cols:\n",
    "            if c in r and r[c]:\n",
    "                val = str(r[c])\n",
    "                parts = [p.strip() for p in val.replace(\";\", \",\").split(\",\") if p.strip()]\n",
    "                for p in parts:\n",
    "                    reasons_counter[p] += 1\n",
    "    top3 = [reason for reason, _ in reasons_counter.most_common(3)]\n",
    "    return top3, dict(reasons_counter)\n",
    "\n",
    "def infer_status_from_values(value: object) -> Optional[str]:\n",
    "    v = str(value).strip().lower()\n",
    "    if v in {\"renewed\", \"yes\", \"true\"}:\n",
    "        return \"renewed\"\n",
    "    if v in {\"not renewed\", \"no\", \"false\", \"churn\", \"not_renewed\"}:\n",
    "        return \"not_renewed\"\n",
    "    return None\n",
    "\n",
    "def infer_status_from_context(ctx: str) -> str:\n",
    "    c = ctx.lower()\n",
    "    if \"is_churn: renewed\" in c or \"predicted status: renewed\" in c or \"status: renewed\" in c or (\"renewed\" in c and \"not renewed\" not in c):\n",
    "        return \"renewed\"\n",
    "    if \"is_churn: not renewed\" in c or \"predicted status: not renewed\" in c or \"status: not renewed\" in c or \"churn\" in c:\n",
    "        return \"not_renewed\"\n",
    "    return \"unknown\"\n",
    "\n",
    "# ========== State ==========\n",
    "class AgentState(TypedDict, total=False):\n",
    "    question: str\n",
    "    context: str\n",
    "    route: str\n",
    "    status: str\n",
    "    customer_id: Optional[str]\n",
    "    policy_no: Optional[str]\n",
    "    response: str\n",
    "\n",
    "def unknown_node(state: AgentState) -> AgentState:\n",
    "    state[\"response\"] = \"Sorry, I’m a retention-strategy agent and can’t answer that question.\"\n",
    "    return state\n",
    "\n",
    "# ========== Build context (with memory support) ==========\n",
    "def build_context_and_status(state: AgentState) -> AgentState:\n",
    "    q = state[\"question\"]\n",
    "\n",
    "    # Prefer identifiers set by router_node (which may have reused memory)\n",
    "    if state.get(\"customer_id\"):\n",
    "        kind, ident = \"CUSTOMER\", state[\"customer_id\"]\n",
    "    elif state.get(\"policy_no\"):\n",
    "        kind, ident = \"POLICY\", state[\"policy_no\"]\n",
    "    else:\n",
    "        kind, ident = extract_identifier(q)\n",
    "\n",
    "    state[\"customer_id\"] = ident if kind == \"CUSTOMER\" else None\n",
    "    state[\"policy_no\"] = ident if kind == \"POLICY\" else None\n",
    "\n",
    "    # No identifier → sample fallback; do not overwrite memory\n",
    "    if kind is None or ident is None:\n",
    "        sample = df_all_customers.head(3)\n",
    "        context = \"Sample records: \" + \"; \".join(\n",
    "            \", \".join(f\"{c}: {sample.iloc[i][c]}\" for c in all_cols)\n",
    "            for i in range(min(3, len(sample)))\n",
    "        )\n",
    "        state[\"context\"] = context\n",
    "        state[\"status\"] = infer_status_from_context(context)\n",
    "        return state\n",
    "\n",
    "    # Generate SQL\n",
    "    sql = generate_sql_with_llm(q, kind=kind, ident=ident)\n",
    "    logging.info(\"Generated SQL:\\n%s\", sql)\n",
    "\n",
    "    # -------- POLICY branch (single row as before) --------\n",
    "    if kind == \"POLICY\":\n",
    "        norm_val = normalize_policy_for_compare(ident)\n",
    "        guard_norm = f\"UPPER(REPLACE(REGEXP_REPLACE({POLICY_NO_COL}::text, '^''', ''), ' ', ''))\"\n",
    "        if guard_norm not in sql.upper():\n",
    "            sql = f\"\"\"\n",
    "SELECT * FROM {SOURCE_TABLE}\n",
    "WHERE {guard_norm} = '{norm_val}';\n",
    "\"\"\".strip()\n",
    "            logging.info(\"Corrected SQL:\\n%s\", sql)\n",
    "\n",
    "        row = execute_sql_fetch_one(sql)\n",
    "        if not row:\n",
    "            sample = df_all_customers.head(3)\n",
    "            context = f\"No exact row found for {kind}:{ident}. Sample records: \" + \"; \".join(\n",
    "                \", \".join(f\"{c}: {sample.iloc[i][c]}\" for c in all_cols)\n",
    "                for i in range(min(3, len(sample)))\n",
    "            )\n",
    "            state[\"context\"] = context\n",
    "            state[\"status\"] = infer_status_from_context(context)\n",
    "\n",
    "            MEMORY[\"last_kind\"] = kind\n",
    "            MEMORY[\"last_ident\"] = ident\n",
    "            MEMORY[\"last_context\"] = context\n",
    "            MEMORY[\"last_status\"] = state[\"status\"]\n",
    "            return state\n",
    "\n",
    "        context = \", \".join(f\"{k}: {row.get(k)}\" for k in all_cols if k in row)\n",
    "        state[\"context\"] = context\n",
    "\n",
    "        status = \"unknown\"\n",
    "        for col in [\"is_churn\", \"predicted status\", \"predicted_status\", \"status\"]:\n",
    "            key = col.lower()\n",
    "            if key in row:\n",
    "                s = infer_status_from_values(row[key])\n",
    "                if s:\n",
    "                    status = s\n",
    "                    break\n",
    "        if status == \"unknown\":\n",
    "            status = infer_status_from_context(context)\n",
    "        state[\"status\"] = status\n",
    "\n",
    "        # Persist memory\n",
    "        MEMORY[\"last_kind\"] = kind\n",
    "        MEMORY[\"last_ident\"] = ident\n",
    "        MEMORY[\"last_context\"] = context\n",
    "        MEMORY[\"last_status\"] = status\n",
    "        return state\n",
    "\n",
    "    # -------- CUSTOMER branch (NEW: fetch ALL rows + aggregate) --------\n",
    "    if kind == \"CUSTOMER\":\n",
    "        rows = execute_sql_fetch_all(sql)\n",
    "        if not rows:\n",
    "            sample = df_all_customers.head(3)\n",
    "            context = f\"No rows found for {kind}:{ident}. Sample records: \" + \"; \".join(\n",
    "                \", \".join(f\"{c}: {sample.iloc[i][c]}\" for c in all_cols)\n",
    "                for i in range(min(3, len(sample)))\n",
    "            )\n",
    "            state[\"context\"] = context\n",
    "            state[\"status\"] = infer_status_from_context(context)\n",
    "\n",
    "            MEMORY[\"last_kind\"] = kind\n",
    "            MEMORY[\"last_ident\"] = ident\n",
    "            MEMORY[\"last_context\"] = context\n",
    "            MEMORY[\"last_status\"] = state[\"status\"]\n",
    "            return state\n",
    "\n",
    "        # Aggregate reasons across rows\n",
    "        top3, counts = aggregate_reasons(rows)\n",
    "\n",
    "        # Customer-level status\n",
    "        found_statuses: List[str] = []\n",
    "        for r in rows:\n",
    "            for k in [\"is_churn\", \"predicted status\", \"predicted_status\", \"status\"]:\n",
    "                if k in r:\n",
    "                    s = infer_status_from_values(r.get(k))\n",
    "                    if s:\n",
    "                        found_statuses.append(s)\n",
    "\n",
    "        if any(s == \"not_renewed\" for s in found_statuses):\n",
    "            status = \"not_renewed\"\n",
    "        elif found_statuses and all(s == \"renewed\" for s in found_statuses):\n",
    "            status = \"renewed\"\n",
    "        else:\n",
    "            status = \"unknown\"\n",
    "\n",
    "        # Compact context: aggregate + a few sample rows\n",
    "        samples_str = \"; \".join(\n",
    "            \", \".join(f\"{c}: {sr.get(c)}\" for c in all_cols if c in sr)\n",
    "            for sr in rows[:3]\n",
    "        )\n",
    "        context = (\n",
    "            f\"customer_id: {ident}; policies_found: {len(rows)}; \"\n",
    "            f\"aggregated_top_3_reasons: {top3}; reason_counts: {counts}; \"\n",
    "            f\"samples: {samples_str}\"\n",
    "        )\n",
    "\n",
    "        state[\"context\"] = context\n",
    "        state[\"status\"] = status\n",
    "\n",
    "        # Persist memory\n",
    "        MEMORY[\"last_kind\"] = kind\n",
    "        MEMORY[\"last_ident\"] = ident\n",
    "        MEMORY[\"last_context\"] = context\n",
    "        MEMORY[\"last_status\"] = status\n",
    "        return state\n",
    "\n",
    "    return state\n",
    "\n",
    "# ========== Agents ==========\n",
    "def recommendation_agent(state: AgentState) -> str:\n",
    "    if state.get(\"status\") == \"renewed\" and (state.get(\"customer_id\") or state.get(\"policy_no\")):\n",
    "        return \"No action is needed. This customer is predicted to renew their policy.\"\n",
    "    prompt = f\"\"\"\n",
    "You are a professional customer retention strategist for a car insurance company.\n",
    "\n",
    "Your job is to analyze the situation based on the given context and provide a clear, helpful recommendation to improve customer retention.\n",
    "\n",
    "Use the following internal guidelines while crafting your answer (DO NOT mention these in your response):\n",
    "\n",
    "- Users may ask general or customer-specific questions.\n",
    "\n",
    "- If the question is customer-specific or policy-specific:\n",
    "  - Use is_churn to determine the customer’s or policy’s status.\n",
    "  - If the status is \"Not Renewed\", provide a smart, personalized recommendation to retain the customer.\n",
    "  - If the status is \"Renewed\", simply acknowledge that no action is needed.\n",
    "\n",
    "- For customer or policy specific queries:\n",
    "  - Carefully analyze the churn reasons provided, including both `churn_main_reason` and all values listed in `churn_top_3_reasons`.\n",
    "  - Your recommendation must address all churn reasons mentioned, not just one. For each reason, include a relevant counter-strategy or benefit.\n",
    "  - Additionally, consider the customer’s or policy’s purchase history, premium values, vehicle type, and policy type to enrich your recommendation.\n",
    "\n",
    "- If the question is based on churn reasons:\n",
    "  - Analyze the `churn_main_reason` and `churn_top_3_reasons` - give a relevant, reason-based retention strategy\n",
    "\n",
    "- If mentioning discounts related, stay in practical ranges (10–30%) unless strong justification; never exceed 30%.\n",
    "\n",
    "- Don't mention any `churned_customer_segment` related.\n",
    "\n",
    "Style rules (apply to every answer):\n",
    "- Deliver a concise, third-person strategy note—never an email.\n",
    "- Do not address the customer directly (“you”, “we”, “please”).\n",
    "- Do not ask for missing fields; work with what you have.\n",
    "- Keep the tone professional, helpful, and concise.\n",
    "- Format the recommendation in clear bullet points. Each point should focus on a distinct strategy or insight.\n",
    "\n",
    "Context:\n",
    "{state['context']}\n",
    "\n",
    "Question:\n",
    "{state['question']}\n",
    "\n",
    "Return only the recommendation.\n",
    "\"\"\".strip()\n",
    "    return llm.invoke(prompt).content.strip()\n",
    "\n",
    "def email_agent(state: AgentState) -> str:\n",
    "    if state.get(\"status\") == \"renewed\" and (state.get(\"customer_id\") or state.get(\"policy_no\")):\n",
    "        return \"No email is needed because this customer is predicted to renew.\"\n",
    "    prompt = f\"\"\"\n",
    "You are an expert in customer retention communication for car insurance.\n",
    "\n",
    "Write a concise, friendly, proactive email or SMS message.\n",
    "\n",
    "Follow these internal rules (DO NOT mention them in the output):\n",
    "- Do NOT mention churn predictions like \"you may not renew\" or \"we noticed you won't renew\".\n",
    "\n",
    "- If the question is about a specific customer wise or policy wise:\n",
    "    - Carefully review both `churn_main_reason` and all values in `churn_top_3_reasons`, and incorporate them into the message. If multiple reasons are given, the message should reflect a proactive solution or benefit for each one.\n",
    "    - Use their `churn_main_reason`, `churn_top_3_reasons`, `premium_amount`, `policy_type`, `vehicle_type`, etc. to personalize the message.\n",
    "    - Offer a relevant benefit like discounts, loyalty reward, etc.\n",
    "\n",
    "- If the request is about churn_reason (`churn_main_reason`, `churn_top_3_reasons`) related (e.g., \"Low Vehicle IDV\", \"Low discount with NCB\", etc.):\n",
    "    - Write a general reusable template for that reason.\n",
    "    - Do NOT include any specific customer details (name, policy number, tenure, etc.).\n",
    "    - Do NOT mention any specific vehicle make/model.\n",
    "    - Keep the message applicable to all customers with that reason.\n",
    "    - Provide a persuasive, benefit-focused message addressing the reason and offering a compelling renewal incentive.\n",
    "\n",
    "- Just assume retention is needed and offer clear value (e.g., discounts, benefits).\n",
    "\n",
    "- If mentioning discounts related, stay in practical ranges (10–30%) unless strong justification; never exceed 30%.\n",
    "\n",
    "- Don't mention any `churned_customer_segment` related.\n",
    "\n",
    "- Keep the tone friendly, persuasive, and action-oriented.\n",
    "\n",
    "- End with a clear next step: e.g., renew link or support contact.\n",
    "\n",
    "- Be brief and do not repeat context data unnecessarily.\n",
    "\n",
    "Context:\n",
    "{state['context']}\n",
    "\n",
    "Query:\n",
    "{state['question']}\n",
    "\n",
    "Return only the finished message.\n",
    "\"\"\".strip()\n",
    "    return llm.invoke(prompt).content.strip()\n",
    "\n",
    "def greeting_agent() -> str:\n",
    "    prompt = \"\"\"\n",
    "You are a helpful assistant for a car insurance retention team.\n",
    "Explain briefly what you can do: analyze customer contexts, provide renewal/retention recommendations, and draft concise email/SMS messages.\n",
    "Keep it short and professional.\n",
    "\"\"\"\n",
    "    return llm.invoke(prompt).content.strip()\n",
    "\n",
    "# ========== Router with follow-up memory ==========\n",
    "def router_node(state: AgentState) -> AgentState:\n",
    "    state[\"route\"] = classify_intent(state[\"question\"])\n",
    "\n",
    "    if state[\"route\"] not in {\"GREETING\", \"UNKNOWN\"}:\n",
    "        q = state[\"question\"]\n",
    "        kind, ident = extract_identifier(q)\n",
    "\n",
    "        # Reuse last identifier for follow-ups without an explicit id\n",
    "        if (kind is None or ident is None) and looks_like_followup(q):\n",
    "            if MEMORY.get(\"last_kind\") and MEMORY.get(\"last_ident\"):\n",
    "                kind = MEMORY[\"last_kind\"]\n",
    "                ident = MEMORY[\"last_ident\"]\n",
    "\n",
    "        if kind == \"CUSTOMER\":\n",
    "            state[\"customer_id\"] = ident\n",
    "            state[\"policy_no\"] = None\n",
    "        elif kind == \"POLICY\":\n",
    "            state[\"policy_no\"] = ident\n",
    "            state[\"customer_id\"] = None\n",
    "        else:\n",
    "            state[\"customer_id\"] = None\n",
    "            state[\"policy_no\"] = None\n",
    "\n",
    "        state = build_context_and_status(state)\n",
    "    return state\n",
    "\n",
    "def reco_node(state: AgentState) -> AgentState:\n",
    "    state[\"response\"] = recommendation_agent(state)\n",
    "    return state\n",
    "\n",
    "def email_node(state: AgentState) -> AgentState:\n",
    "    state[\"response\"] = email_agent(state)\n",
    "    return state\n",
    "\n",
    "def greet_node(state: AgentState) -> AgentState:\n",
    "    state[\"response\"] = greeting_agent()\n",
    "    return state\n",
    "\n",
    "# ========== Graph ==========\n",
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(\"router\", RunnableLambda(router_node))\n",
    "graph.add_node(\"recommendation_agent\", RunnableLambda(reco_node))\n",
    "graph.add_node(\"email_agent\", RunnableLambda(email_node))\n",
    "graph.add_node(\"greeting_agent\", RunnableLambda(greet_node))\n",
    "graph.add_node(\"unknown_agent\", RunnableLambda(unknown_node))\n",
    "\n",
    "graph.set_entry_point(\"router\")\n",
    "graph.add_conditional_edges(\n",
    "    \"router\",\n",
    "    lambda s: s[\"route\"],\n",
    "    {\n",
    "        \"RECOMMENDATION\": \"recommendation_agent\",\n",
    "        \"EMAIL\": \"email_agent\",\n",
    "        \"GREETING\": \"greeting_agent\",\n",
    "        \"UNKNOWN\": \"unknown_agent\",\n",
    "    },\n",
    ")\n",
    "graph.add_edge(\"recommendation_agent\", END)\n",
    "graph.add_edge(\"email_agent\", END)\n",
    "graph.add_edge(\"greeting_agent\", END)\n",
    "graph.add_edge(\"unknown_agent\", END)\n",
    "\n",
    "flow = graph.compile()\n",
    "\n",
    "# ========== NEW: last email store + Gmail sender ==========\n",
    "_LAST_EMAIL_SUBJ = \"\"\n",
    "_LAST_EMAIL_BODY = \"\"\n",
    "_LAST_ROUTE = \"UNKNOWN\"\n",
    "\n",
    "def _guess_subject_from_body(body: str) -> str:\n",
    "    # Minimal heuristic: first non-empty line, truncated\n",
    "    for line in (body or \"\").splitlines():\n",
    "        s = line.strip()\n",
    "        if s:\n",
    "            return s[:80]\n",
    "    return \"Policy Renewal Options\"\n",
    "\n",
    "# ========== UI glue ==========\n",
    "SUGGESTION_TEMPLATES = [\n",
    "    [\"Give a retention recommendation for policy \\\"Policy_no\\\".\"],\n",
    "    [\"Give a retention recommendation for customer \\\"Customer_id\\\".\"],\n",
    "    [\"Write an email for policy \\\"Policy_no\\\" focusing on renewal and value.\"],\n",
    "    [\"Write an email for customer \\\"Customer_id\\\" focusing on renewal and value.\"],\n",
    "    [\"Give a retention recommendation for the customers whose having \\\"Reason\\\" as a reason.\"],\n",
    "    [\"Write a mail draft for the customers whose having \\\"Reason\\\" as a reason.\"],\n",
    "    [\"For this policy, write a retention email draft.\"],\n",
    "    [\"For this customer, write a retention email draft.\"]\n",
    "]\n",
    "\n",
    "def chat_respond(message: str, history: list) -> str:\n",
    "    global _LAST_EMAIL_SUBJ, _LAST_EMAIL_BODY, _LAST_ROUTE\n",
    "    try:\n",
    "        out = flow.invoke({\"question\": message})\n",
    "        # Preserve last draft if this is an email route\n",
    "        _LAST_ROUTE = out.get(\"route\", \"UNKNOWN\")\n",
    "        if _LAST_ROUTE == \"EMAIL\":\n",
    "            body = out.get(\"response\", \"\")\n",
    "            _LAST_EMAIL_BODY = body\n",
    "            _LAST_EMAIL_SUBJ = _guess_subject_from_body(body)\n",
    "        return out.get(\"response\", \"No response.\")\n",
    "    except Exception as e:\n",
    "        _LAST_ROUTE = \"UNKNOWN\"\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# ========== Keep your ChatInterface exactly, but embed it in a Blocks wrapper (to show Send button) ==========\n",
    "with gr.Blocks(title=\"Retention Assistant\") as app:\n",
    "    # Your original ChatInterface (unchanged)\n",
    "    demo = gr.ChatInterface(\n",
    "        fn=chat_respond,\n",
    "        title=\"Retention Assistant\",\n",
    "        textbox=gr.Textbox(placeholder=\"Ask for a recommendation or request an email/SMS draft…\"),\n",
    "        retry_btn=\"Retry\",\n",
    "        undo_btn=\"Undo\",\n",
    "        clear_btn=\"Clear\",\n",
    "        examples=SUGGESTION_TEMPLATES,\n",
    "        cache_examples=False\n",
    "    )\n",
    "\n",
    "    # Small send panel (NEW)\n",
    "    with gr.Row():\n",
    "        to_box = gr.Textbox(label=\"To\", value=DEFAULT_TO_EMAIL, scale=4)\n",
    "        send_btn = gr.Button(\"Send Mail\", visible=True, scale=1)  # stays visible; guarded in handler\n",
    "    send_status = gr.Markdown(\"\")\n",
    "\n",
    "    # Click handler (NEW)\n",
    "    def _on_send(to_addr: str):\n",
    "        global _LAST_EMAIL_SUBJ, _LAST_EMAIL_BODY, _LAST_ROUTE\n",
    "        if _LAST_ROUTE != \"EMAIL\":\n",
    "            return \"No email draft detected. Ask for an email draft first.\"\n",
    "        if not to_addr:\n",
    "            return \"Please provide a 'To' address.\"\n",
    "        if not _LAST_EMAIL_BODY:\n",
    "            return \"No email body available.\"\n",
    "\n",
    "        # Build Gmail service & send\n",
    "        try:\n",
    "            # Build service (first time triggers OAuth → creates token.json)\n",
    "            scopes = [\"https://www.googleapis.com/auth/gmail.send\"]\n",
    "            creds = None\n",
    "            if os.path.exists(GMAIL_TOKEN_FILE):\n",
    "                creds = Credentials.from_authorized_user_file(GMAIL_TOKEN_FILE, scopes)\n",
    "            if not creds or not creds.valid:\n",
    "                if creds and creds.expired and creds.refresh_token:\n",
    "                    creds.refresh(Request())\n",
    "                else:\n",
    "                    flow = InstalledAppFlow.from_client_secrets_file(GMAIL_CREDENTIALS_FILE, scopes)\n",
    "                    try:\n",
    "                        creds = flow.run_local_server(port=0)\n",
    "                    except Exception:\n",
    "                        print(\"Falling back to console OAuth flow...\")\n",
    "                        creds = flow.run_console()\n",
    "                with open(GMAIL_TOKEN_FILE, \"w\") as f:\n",
    "                    f.write(creds.to_json())\n",
    "            service = build(\"gmail\", \"v1\", credentials=creds)\n",
    "\n",
    "            sender = SENDER_EMAIL or \"\"\n",
    "            if not sender:\n",
    "                return \"SENDER_EMAIL not set in environment.\"\n",
    "\n",
    "            msg = MIMEText(_LAST_EMAIL_BODY, _charset=\"utf-8\")\n",
    "            msg[\"to\"] = to_addr\n",
    "            msg[\"from\"] = sender\n",
    "            msg[\"subject\"] = _LAST_EMAIL_SUBJ or \"Policy Renewal Options\"\n",
    "\n",
    "            raw = base64.urlsafe_b64encode(msg.as_bytes()).decode(\"utf-8\")\n",
    "            sent = service.users().messages().send(userId=\"me\", body={\"raw\": raw}).execute()\n",
    "            return f\"Sent. Gmail message id: {sent.get('id','')}\"\n",
    "        except Exception as e:\n",
    "            return f\"Send failed: {e}\"\n",
    "\n",
    "    send_btn.click(_on_send, inputs=[to_box], outputs=[send_status])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9837a444",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kaviyam\\AppData\\Local\\Temp\\ipykernel_6328\\3806793786.py:58: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(f\"SELECT * FROM {SOURCE_TABLE}\", conn)\n",
      "C:\\Users\\Kaviyam\\AppData\\Local\\Temp\\ipykernel_6328\\3806793786.py:84: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeds = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
      "2025-08-20 14:20:44,933 | INFO | Use pytorch device_name: cuda:0\n",
      "2025-08-20 14:20:44,934 | INFO | Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n",
      "2025-08-20 14:20:50,865 | INFO | Loading faiss with AVX2 support.\n",
      "2025-08-20 14:20:51,102 | INFO | Successfully loaded faiss with AVX2 support.\n",
      "2025-08-20 14:20:51,115 | INFO | Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-20 14:20:51,981 | INFO | HTTP Request: GET http://127.0.0.1:7860/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-08-20 14:20:51,981 | INFO | HTTP Request: GET https://checkip.amazonaws.com/ \"HTTP/1.1 200 \"\n",
      "2025-08-20 14:20:52,015 | INFO | HTTP Request: GET https://checkip.amazonaws.com/ \"HTTP/1.1 200 \"\n",
      "2025-08-20 14:20:52,124 | INFO | HTTP Request: HEAD http://127.0.0.1:7860/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-20 14:20:52,945 | INFO | HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-08-20 14:20:52,974 | INFO | HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-08-20 14:21:15,767 | INFO | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "C:\\Users\\Kaviyam\\AppData\\Local\\Temp\\ipykernel_6328\\3806793786.py:91: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = schema_retriever.get_relevant_documents(question)\n",
      "2025-08-20 14:21:36,344 | INFO | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-20 14:21:36,363 | INFO | Generated SQL:\n",
      "SELECT claim_approval_rate FROM bi_dwh.prediction_agent_data \n",
      "WHERE UPPER(REPLACE(REGEXP_REPLACE(policy_no::text, '^''', ''), ' ', '')) = '201140020123704866505000';\n",
      "2025-08-20 14:21:36,366 | INFO | Corrected SQL:\n",
      "SELECT * FROM bi_dwh.prediction_agent_data\n",
      "WHERE UPPER(REPLACE(REGEXP_REPLACE(policy_no::text, '^''', ''), ' ', '')) = '201140020123704866505000';\n",
      "2025-08-20 14:21:39,208 | INFO | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-20 14:22:16,956 | INFO | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-20 14:22:16,956 | INFO | Generated SQL:\n",
      "SELECT * FROM bi_dwh.prediction_agent_data \n",
      "WHERE UPPER(REPLACE(REGEXP_REPLACE(policy_no::text, '^''', ''), ' ', '')) = '201140020123704866505000';\n",
      "2025-08-20 14:22:16,969 | INFO | Corrected SQL:\n",
      "SELECT * FROM bi_dwh.prediction_agent_data\n",
      "WHERE UPPER(REPLACE(REGEXP_REPLACE(policy_no::text, '^''', ''), ' ', '')) = '201140020123704866505000';\n",
      "2025-08-20 14:22:20,025 | INFO | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-20 14:22:38,599 | INFO | file_cache is only supported with oauth2client<4.0.0\n",
      "2025-08-20 14:49:54,027 | INFO | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-20 14:50:18,060 | INFO | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-20 14:50:18,066 | INFO | Generated SQL:\n",
      "SELECT * FROM bi_dwh.prediction_agent_data \n",
      "WHERE UPPER(REPLACE(REGEXP_REPLACE(policy_no::text, '^''', ''), ' ', '')) = '201140020123704866505000';\n",
      "2025-08-20 14:50:18,068 | INFO | Corrected SQL:\n",
      "SELECT * FROM bi_dwh.prediction_agent_data\n",
      "WHERE UPPER(REPLACE(REGEXP_REPLACE(policy_no::text, '^''', ''), ' ', '')) = '201140020123704866505000';\n",
      "2025-08-20 14:50:20,236 | INFO | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-20 14:50:31,629 | INFO | file_cache is only supported with oauth2client<4.0.0\n",
      "2025-08-20 14:51:08,087 | INFO | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-20 14:51:08,094 | INFO | Generated SQL:\n",
      "SELECT * FROM bi_dwh.prediction_agent_data \n",
      "WHERE UPPER(REPLACE(REGEXP_REPLACE(policy_no::text, '^''', ''), ' ', '')) = '201140020123704866505000';\n",
      "2025-08-20 14:51:08,094 | INFO | Corrected SQL:\n",
      "SELECT * FROM bi_dwh.prediction_agent_data\n",
      "WHERE UPPER(REPLACE(REGEXP_REPLACE(policy_no::text, '^''', ''), ' ', '')) = '201140020123704866505000';\n",
      "2025-08-20 14:51:10,175 | INFO | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import logging\n",
    "import difflib\n",
    "from typing import TypedDict, Optional, Tuple, Dict, Any, List\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Gmail imports\n",
    "import base64\n",
    "from email.mime.text import MIMEText\n",
    "from typing import Optional as _Opt\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\")\n",
    "\n",
    "# ENV \n",
    "load_dotenv()\n",
    "DB_HOST = os.getenv(\"DB_HOST\")\n",
    "DB_PORT = os.getenv(\"DB_PORT\", \"5432\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\")\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\")\n",
    "SOURCE_TABLE = os.getenv(\"SOURCE_TABLE\", \"bi_dwh.prediction_agent_data\")\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "GROQ_MODEL = \"meta-llama/llama-4-maverick-17b-128e-instruct\"\n",
    "\n",
    "# Gmail env\n",
    "GMAIL_CREDENTIALS_FILE = os.getenv(\"GMAIL_CREDENTIALS_FILE\", \"credentials.json\")\n",
    "GMAIL_TOKEN_FILE = os.getenv(\"GMAIL_TOKEN_FILE\", \"token.json\")\n",
    "SENDER_EMAIL = os.getenv(\"SENDER_EMAIL\", \"\")\n",
    "DEFAULT_TO_EMAIL = os.getenv(\"DEFAULT_TO_EMAIL\", \"\")\n",
    "\n",
    "CUSTOMER_ID_COL = \"customerid\"\n",
    "POLICY_NO_COL = \"policy_no\"\n",
    "\n",
    "# DB\n",
    "def fetch_customer_data() -> pd.DataFrame:\n",
    "    conn = psycopg2.connect(\n",
    "        host=DB_HOST, port=DB_PORT, dbname=DB_NAME, user=DB_USER, password=DB_PASSWORD\n",
    "    )\n",
    "    try:\n",
    "        df = pd.read_sql(f\"SELECT * FROM {SOURCE_TABLE}\", conn)\n",
    "    finally:\n",
    "        conn.close()\n",
    "    df.columns = df.columns.str.lower()\n",
    "    return df\n",
    "\n",
    "# LLM\n",
    "llm = ChatGroq(model=GROQ_MODEL, api_key=GROQ_API_KEY, temperature=0)\n",
    "\n",
    "# Load table once (columns + sample fallback)\n",
    "df_all_customers = fetch_customer_data()\n",
    "all_cols = list(df_all_customers.columns)\n",
    "\n",
    "# Tiny schema RAG (embed only column names + one sample value)\n",
    "def build_schema_docs(df: pd.DataFrame) -> FAISS:\n",
    "    texts = []\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            sample_val = df[col].dropna().iloc[0]\n",
    "        except Exception:\n",
    "            sample_val = None\n",
    "        desc = f\"{col}: column in {SOURCE_TABLE}\"\n",
    "        if sample_val is not None:\n",
    "            desc += f\" | sample_value={str(sample_val)[:120]}\"\n",
    "        texts.append(desc)\n",
    "    docs = [Document(page_content=t) for t in texts]\n",
    "    embeds = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "    return FAISS.from_documents(docs, embeds)\n",
    "\n",
    "schema_vs = build_schema_docs(df_all_customers)\n",
    "schema_retriever = schema_vs.as_retriever(search_kwargs={\"k\": 8})\n",
    "\n",
    "def get_schema_snippets(question: str) -> str:\n",
    "    docs = schema_retriever.get_relevant_documents(question)\n",
    "    return \"\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "# Lightweight in-process memory for follow-ups\n",
    "MEMORY: Dict[str, Any] = {\n",
    "    \"last_kind\": None,        # \"CUSTOMER\" | \"POLICY\" | None\n",
    "    \"last_ident\": None,       # the actual id string\n",
    "    \"last_context\": None,     # flattened row context string\n",
    "    \"last_status\": None,      # \"renewed\" | \"not_renewed\" | \"unknown\"\n",
    "}\n",
    "\n",
    "FOLLOWUP_HINTS = {\n",
    "    \"for this\", \"for that\", \"for above\", \"same\", \"this policy\", \"that policy\",\n",
    "    \"this customer\", \"that customer\", \"for this one\",\n",
    "    \"use the same\", \"continue with this\"\n",
    "}\n",
    "\n",
    "def looks_like_followup(text: str) -> bool:\n",
    "    q = text.lower()\n",
    "    return any(h in q for h in FOLLOWUP_HINTS)\n",
    "\n",
    "# SQL helpers\n",
    "def normalize_policy_for_compare(p: str) -> str:\n",
    "    s = str(p).strip()\n",
    "    if s.startswith((\"'\", '\"')):\n",
    "        s = s[1:]\n",
    "    s = s.replace(\" \", \"\")\n",
    "    return s.upper()\n",
    "\n",
    "def extract_identifier(question: str) -> Tuple[Optional[str], Optional[str]]:\n",
    "    q = question.strip()\n",
    "\n",
    "    # explicit \"policy …\"\n",
    "    m_pol_explicit = re.search(\n",
    "        r\"(?:policy(?:\\s*no)?\\.?\\s*#?:?\\s*[\\\"']?)([A-Za-z0-9\\-]{6,40})\",\n",
    "        q, flags=re.IGNORECASE\n",
    "    )\n",
    "    if m_pol_explicit:\n",
    "        return \"POLICY\", m_pol_explicit.group(1)\n",
    "\n",
    "    # customer id (≥5 consecutive digits)\n",
    "    m_cust = re.search(r\"(?:customer\\s*)?(\\d{5,})\", q, flags=re.IGNORECASE)\n",
    "    if m_cust:\n",
    "        return \"CUSTOMER\", m_cust.group(1)\n",
    "\n",
    "    # long-token fallback if contains at least one digit\n",
    "    m_long = re.search(\n",
    "        r\"[\\\"']?([A-Za-z0-9\\-]{10,40}\\d[A-Za-z0-9\\-]*)\",\n",
    "        q,\n",
    "    )\n",
    "    if m_long:\n",
    "        return \"POLICY\", m_long.group(1)\n",
    "\n",
    "    return None, None\n",
    "\n",
    "def fuzzy_in(text: str, targets, cutoff=0.8) -> bool:\n",
    "    words = [w for w in re.findall(r\"\\w+\", text) if len(w) > 2]\n",
    "    for w in words:\n",
    "        for t in targets:\n",
    "            if len(t) > 2 and difflib.SequenceMatcher(None, w, t).ratio() >= cutoff:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def is_whole_word(word: str, text: str) -> bool:\n",
    "    return re.search(rf\"\\b{re.escape(word)}\\b\", text) is not None\n",
    "\n",
    "def classify_intent(question: str) -> str:\n",
    "    q = question.strip().lower().rstrip(string.punctuation)\n",
    "\n",
    "    greeting_phrases = [\"hi\", \"hello\", \"hey\", \"what can you do\", \"who are you\", \"help\"]\n",
    "    if (fuzzy_in(q, greeting_phrases, 0.8) or any(is_whole_word(p, q) for p in greeting_phrases)):\n",
    "        return \"GREETING\"\n",
    "\n",
    "    email_phrases = [\n",
    "        \"email\", \"e-mail\", \"mail\", \"sms\", \"whatsapp\", \"draft\",\n",
    "        \"compose\", \"send mail\", \"write an email\", \"write to\",\n",
    "        \"message template\", \"text message\"\n",
    "    ]\n",
    "    if (fuzzy_in(q, email_phrases, 0.75) or any(is_whole_word(p, q) for p in email_phrases)):\n",
    "        return \"EMAIL\"\n",
    "\n",
    "    reco_phrases = [\n",
    "        \"recommendation\", \"recommend\", \"strategy\", \"suggest\",\n",
    "        \"advice\", \"plan\", \"retain\", \"how to retain\",\n",
    "        \"next best action\", \"improve renewal\", \"reduce churn\"\n",
    "    ]\n",
    "    if (fuzzy_in(q, reco_phrases, 0.75) or any(is_whole_word(p, q) for p in reco_phrases)):\n",
    "        return \"RECOMMENDATION\"\n",
    "\n",
    "    return \"UNKNOWN\"\n",
    "\n",
    "def generate_sql_with_llm(question: str, kind: Optional[str], ident: Optional[str]) -> str:\n",
    "    schema_snippets = get_schema_snippets(question)\n",
    "    ident_str = ident or \"\"\n",
    "    kind_str = kind or \"UNKNOWN\"\n",
    "\n",
    "    norm_sql_expr = f\"UPPER(REPLACE(REGEXP_REPLACE({POLICY_NO_COL}::text, '^''', ''), ' ', ''))\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You write a single valid PostgreSQL SELECT for table {SOURCE_TABLE}. No markdown.\n",
    "\n",
    "Schema hints:\n",
    "{schema_snippets}\n",
    "\n",
    "Rules:\n",
    "- Use only table {SOURCE_TABLE}.\n",
    "- If the question is customer-specific -> filter as: WHERE {CUSTOMER_ID_COL} = '<id>'.\n",
    "- If the question is policy-specific -> compare normalized policy numbers:\n",
    "  Use: {norm_sql_expr} = '<NORMALIZED_ID>'  -- where NORMALIZED_ID strips leading quote and spaces then UPPER.\n",
    "- Never do JOINs or subqueries. One SELECT only.\n",
    "- Return *only* the final SQL (end with a semicolon).\n",
    "\n",
    "Question: {question}\n",
    "Kind detected: {kind_str}\n",
    "Identifier: {ident_str}\n",
    "\n",
    "Examples:\n",
    "-- customer\n",
    "SELECT * FROM {SOURCE_TABLE} WHERE {CUSTOMER_ID_COL} = '1467520';\n",
    "\n",
    "-- policy (normalize compare)\n",
    "SELECT * FROM {SOURCE_TABLE}\n",
    "WHERE {norm_sql_expr} = '201140020123100158101000';\n",
    "\"\"\"\n",
    "    sql = llm.invoke(prompt).content.strip()\n",
    "    m = re.search(r\"(SELECT\\s.+?;)\", sql, flags=re.IGNORECASE | re.DOTALL)\n",
    "    return m.group(1).strip() if m else sql\n",
    "\n",
    "def execute_sql_fetch_one(sql: str) -> Optional[Dict[str, Any]]:\n",
    "    conn = psycopg2.connect(\n",
    "        host=DB_HOST, port=DB_PORT, dbname=DB_NAME, user=DB_USER, password=DB_PASSWORD\n",
    "    )\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(sql)\n",
    "            row = cur.fetchone()\n",
    "            if row is None:\n",
    "                return None\n",
    "            cols = [desc[0].lower() for desc in cur.description]\n",
    "            return dict(zip(cols, row))\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "def execute_sql_fetch_all(sql: str) -> List[Dict[str, Any]]:\n",
    "    conn = psycopg2.connect(\n",
    "        host=DB_HOST, port=DB_PORT, dbname=DB_NAME, user=DB_USER, password=DB_PASSWORD\n",
    "    )\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(sql)\n",
    "            rows = cur.fetchall()\n",
    "            if not rows:\n",
    "                return []\n",
    "            cols = [desc[0].lower() for desc in cur.description]\n",
    "            return [dict(zip(cols, r)) for r in rows]\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "def aggregate_reasons(rows: List[Dict[str, Any]]) -> tuple[list[str], Dict[str, int]]:\n",
    "    reasons_counter = Counter()\n",
    "    reason_cols = [\"churn_main_reason\", \"churn_top_3_reasons\"]\n",
    "    for r in rows:\n",
    "        for c in reason_cols:\n",
    "            if c in r and r[c]:\n",
    "                val = str(r[c])\n",
    "                parts = [p.strip() for p in val.replace(\";\", \",\").split(\",\") if p.strip()]\n",
    "                for p in parts:\n",
    "                    reasons_counter[p] += 1\n",
    "    top3 = [reason for reason, _ in reasons_counter.most_common(3)]\n",
    "    return top3, dict(reasons_counter)\n",
    "\n",
    "def infer_status_from_values(value: object) -> Optional[str]:\n",
    "    v = str(value).strip().lower()\n",
    "    if v in {\"renewed\", \"yes\", \"true\"}:\n",
    "        return \"renewed\"\n",
    "    if v in {\"not renewed\", \"no\", \"false\", \"churn\", \"not_renewed\"}:\n",
    "        return \"not_renewed\"\n",
    "    return None\n",
    "\n",
    "def infer_status_from_context(ctx: str) -> str:\n",
    "    c = ctx.lower()\n",
    "    if \"is_churn: renewed\" in c or \"predicted status: renewed\" in c or \"status: renewed\" in c or (\"renewed\" in c and \"not renewed\" not in c):\n",
    "        return \"renewed\"\n",
    "    if \"is_churn: not renewed\" in c or \"predicted status: not renewed\" in c or \"status: not renewed\" in c or \"churn\" in c:\n",
    "        return \"not_renewed\"\n",
    "    return \"unknown\"\n",
    "\n",
    "# State\n",
    "class AgentState(TypedDict, total=False):\n",
    "    question: str\n",
    "    context: str\n",
    "    route: str\n",
    "    status: str\n",
    "    customer_id: Optional[str]\n",
    "    policy_no: Optional[str]\n",
    "    response: str\n",
    "\n",
    "def unknown_node(state: AgentState) -> AgentState:\n",
    "    state[\"response\"] = \"Sorry, I’m a retention-strategy agent and can’t answer that question.\"\n",
    "    return state\n",
    "\n",
    "# Build context (with memory support)\n",
    "def build_context_and_status(state: AgentState) -> AgentState:\n",
    "    q = state[\"question\"]\n",
    "\n",
    "    if state.get(\"customer_id\"):\n",
    "        kind, ident = \"CUSTOMER\", state[\"customer_id\"]\n",
    "    elif state.get(\"policy_no\"):\n",
    "        kind, ident = \"POLICY\", state[\"policy_no\"]\n",
    "    else:\n",
    "        kind, ident = extract_identifier(q)\n",
    "\n",
    "    state[\"customer_id\"] = ident if kind == \"CUSTOMER\" else None\n",
    "    state[\"policy_no\"] = ident if kind == \"POLICY\" else None\n",
    "\n",
    "    if kind is None or ident is None:\n",
    "        sample = df_all_customers.head(3)\n",
    "        context = \"Sample records: \" + \"; \".join(\n",
    "            \", \".join(f\"{c}: {sample.iloc[i][c]}\" for c in all_cols)\n",
    "            for i in range(min(3, len(sample)))\n",
    "        )\n",
    "        state[\"context\"] = context\n",
    "        state[\"status\"] = infer_status_from_context(context)\n",
    "        return state\n",
    "\n",
    "    sql = generate_sql_with_llm(q, kind=kind, ident=ident)\n",
    "    logging.info(\"Generated SQL:\\n%s\", sql)\n",
    "\n",
    "    if kind == \"POLICY\":\n",
    "        norm_val = normalize_policy_for_compare(ident)\n",
    "        guard_norm = f\"UPPER(REPLACE(REGEXP_REPLACE({POLICY_NO_COL}::text, '^''', ''), ' ', ''))\"\n",
    "        if guard_norm not in sql.upper():\n",
    "            sql = f\"\"\"\n",
    "SELECT * FROM {SOURCE_TABLE}\n",
    "WHERE {guard_norm} = '{norm_val}';\n",
    "\"\"\".strip()\n",
    "            logging.info(\"Corrected SQL:\\n%s\", sql)\n",
    "\n",
    "        row = execute_sql_fetch_one(sql)\n",
    "        if not row:\n",
    "            sample = df_all_customers.head(3)\n",
    "            context = f\"No exact row found for {kind}:{ident}. Sample records: \" + \"; \".join(\n",
    "                \", \".join(f\"{c}: {sample.iloc[i][c]}\" for c in all_cols)\n",
    "                for i in range(min(3, len(sample)))\n",
    "            )\n",
    "            state[\"context\"] = context\n",
    "            state[\"status\"] = infer_status_from_context(context)\n",
    "\n",
    "            MEMORY[\"last_kind\"] = kind\n",
    "            MEMORY[\"last_ident\"] = ident\n",
    "            MEMORY[\"last_context\"] = context\n",
    "            MEMORY[\"last_status\"] = state[\"status\"]\n",
    "            return state\n",
    "\n",
    "        context = \", \".join(f\"{k}: {row.get(k)}\" for k in all_cols if k in row)\n",
    "        state[\"context\"] = context\n",
    "\n",
    "        status = \"unknown\"\n",
    "        for col in [\"is_churn\", \"predicted status\", \"predicted_status\", \"status\"]:\n",
    "            key = col.lower()\n",
    "            if key in row:\n",
    "                s = infer_status_from_values(row[key])\n",
    "                if s:\n",
    "                    status = s\n",
    "                    break\n",
    "        if status == \"unknown\":\n",
    "            status = infer_status_from_context(context)\n",
    "        state[\"status\"] = status\n",
    "\n",
    "        MEMORY[\"last_kind\"] = kind\n",
    "        MEMORY[\"last_ident\"] = ident\n",
    "        MEMORY[\"last_context\"] = context\n",
    "        MEMORY[\"last_status\"] = status\n",
    "        return state\n",
    "\n",
    "    if kind == \"CUSTOMER\":\n",
    "        rows = execute_sql_fetch_all(sql)\n",
    "        if not rows:\n",
    "            sample = df_all_customers.head(3)\n",
    "            context = f\"No rows found for {kind}:{ident}. Sample records: \" + \"; \".join(\n",
    "                \", \".join(f\"{c}: {sample.iloc[i][c]}\" for c in all_cols)\n",
    "                for i in range(min(3, len(sample)))\n",
    "            )\n",
    "            state[\"context\"] = context\n",
    "            state[\"status\"] = infer_status_from_context(context)\n",
    "\n",
    "            MEMORY[\"last_kind\"] = kind\n",
    "            MEMORY[\"last_ident\"] = ident\n",
    "            MEMORY[\"last_context\"] = context\n",
    "            MEMORY[\"last_status\"] = state[\"status\"]\n",
    "            return state\n",
    "\n",
    "        top3, counts = aggregate_reasons(rows)\n",
    "\n",
    "        found_statuses: List[str] = []\n",
    "        for r in rows:\n",
    "            for k in [\"is_churn\", \"predicted status\", \"predicted_status\", \"status\"]:\n",
    "                if k in r:\n",
    "                    s = infer_status_from_values(r.get(k))\n",
    "                    if s:\n",
    "                        found_statuses.append(s)\n",
    "\n",
    "        if any(s == \"not_renewed\" for s in found_statuses):\n",
    "            status = \"not_renewed\"\n",
    "        elif found_statuses and all(s == \"renewed\" for s in found_statuses):\n",
    "            status = \"renewed\"\n",
    "        else:\n",
    "            status = \"unknown\"\n",
    "\n",
    "        samples_str = \"; \".join(\n",
    "            \", \".join(f\"{c}: {sr.get(c)}\" for c in all_cols if c in sr)\n",
    "            for sr in rows[:3]\n",
    "        )\n",
    "        context = (\n",
    "            f\"customer_id: {ident}; policies_found: {len(rows)}; \"\n",
    "            f\"aggregated_top_3_reasons: {top3}; reason_counts: {counts}; \"\n",
    "            f\"samples: {samples_str}\"\n",
    "        )\n",
    "\n",
    "        state[\"context\"] = context\n",
    "        state[\"status\"] = status\n",
    "\n",
    "        MEMORY[\"last_kind\"] = kind\n",
    "        MEMORY[\"last_ident\"] = ident\n",
    "        MEMORY[\"last_context\"] = context\n",
    "        MEMORY[\"last_status\"] = status\n",
    "        return state\n",
    "\n",
    "    return state\n",
    "\n",
    "# Agents\n",
    "def recommendation_agent(state: AgentState) -> str:\n",
    "    if state.get(\"status\") == \"renewed\" and (state.get(\"customer_id\") or state.get(\"policy_no\")):\n",
    "        return \"No action is needed. This customer is predicted to renew their policy.\"\n",
    "    prompt = f\"\"\"\n",
    "You are a professional customer retention strategist for a car insurance company.\n",
    "\n",
    "Your job is to analyze the situation based on the given context and provide a clear, helpful recommendation to improve customer retention.\n",
    "\n",
    "Use the following internal guidelines while crafting your answer (DO NOT mention these in your response):\n",
    "\n",
    "- Users may ask general or customer-specific questions.\n",
    "\n",
    "- If the question is customer-specific or policy-specific:\n",
    "  - Use is_churn to determine the customer’s or policy’s status.\n",
    "  - If the status is \"Not Renewed\", provide a smart, personalized recommendation to retain the customer.\n",
    "  - If the status is \"Renewed\", simply acknowledge that no action is needed.\n",
    "\n",
    "- For customer or policy specific queries:\n",
    "  - Carefully analyze the churn reasons provided, including both `churn_main_reason` and all values listed in `churn_top_3_reasons`.\n",
    "  - Your recommendation must address all churn reasons mentioned, not just one. For each reason, include a relevant counter-strategy or benefit.\n",
    "  - Additionally, consider the customer’s or policy’s purchase history, premium values, vehicle type, and policy type to enrich your recommendation.\n",
    "\n",
    "- If the question is based on churn reasons:\n",
    "  - Analyze the `churn_main_reason` and `churn_top_3_reasons` - give a relevant, reason-based retention strategy\n",
    "\n",
    "- If mentioning discounts related, stay in practical ranges (10–30%) unless strong justification; never exceed 30%.\n",
    "\n",
    "- Don't mention any `churned_customer_segment` related.\n",
    "\n",
    "Style rules (apply to every answer):\n",
    "- Deliver a concise, third-person strategy note—never an email.\n",
    "- Do not address the customer directly (“you”, “we”, “please”).\n",
    "- Do not ask for missing fields; work with what you have.\n",
    "- Keep the tone professional, helpful, and concise.\n",
    "- Format the recommendation in clear bullet points. Each point should focus on a distinct strategy or insight.\n",
    "\n",
    "Context:\n",
    "{state['context']}\n",
    "\n",
    "Question:\n",
    "{state['question']}\n",
    "\n",
    "Return only the recommendation.\n",
    "\"\"\".strip()\n",
    "    return llm.invoke(prompt).content.strip()\n",
    "\n",
    "def email_agent(state: AgentState) -> str:\n",
    "    if state.get(\"status\") == \"renewed\" and (state.get(\"customer_id\") or state.get(\"policy_no\")):\n",
    "        return \"No email is needed because this customer is predicted to renew.\"\n",
    "    prompt = f\"\"\"\n",
    "You are an expert in customer retention communication for car insurance.\n",
    "\n",
    "Write a concise, friendly, proactive email or SMS message.\n",
    "\n",
    "Follow these internal rules (DO NOT mention them in the output):\n",
    "- Do NOT mention churn predictions like \"you may not renew\" or \"we noticed you won't renew\".\n",
    "\n",
    "- If the question is about a specific customer wise or policy wise:\n",
    "    - Carefully review both `churn_main_reason` and all values in `churn_top_3_reasons`, and incorporate them into the message. If multiple reasons are given, the message should reflect a proactive solution or benefit for each one.\n",
    "    - Use their `churn_main_reason`, `churn_top_3_reasons`, `premium_amount`, `policy_type`, `vehicle_type`, etc. to personalize the message.\n",
    "    - Offer a relevant benefit like discounts, loyalty reward, etc.\n",
    "\n",
    "- If the request is about churn_reason (`churn_main_reason`, `churn_top_3_reasons`) related (e.g., \"Low Vehicle IDV\", \"Low discount with NCB\", etc.):\n",
    "    - Write a general reusable template for that reason.\n",
    "    - Do NOT include any specific customer details (name, policy number, tenure, etc.).\n",
    "    - Do NOT mention any specific vehicle make/model.\n",
    "    - Keep the message applicable to all customers with that reason.\n",
    "    - Provide a persuasive, benefit-focused message addressing the reason and offering a compelling renewal incentive.\n",
    "\n",
    "- Just assume retention is needed and offer clear value (e.g., discounts, benefits).\n",
    "\n",
    "- If mentioning discounts related, stay in practical ranges (10–30%) unless strong justification; never exceed 30%.\n",
    "\n",
    "- Don't mention any `churned_customer_segment` related.\n",
    "\n",
    "- Keep the tone friendly, persuasive, and action-oriented.\n",
    "\n",
    "- End with a clear next step: e.g., renew link or support contact.\n",
    "\n",
    "- Be brief and do not repeat context data unnecessarily.\n",
    "\n",
    "Context:\n",
    "{state['context']}\n",
    "\n",
    "Query:\n",
    "{state['question']}\n",
    "\n",
    "Return only the finished message.\n",
    "\"\"\".strip()\n",
    "    return llm.invoke(prompt).content.strip()\n",
    "\n",
    "def greeting_agent() -> str:\n",
    "    prompt = \"\"\"\n",
    "You are a helpful assistant for a car insurance retention team.\n",
    "Explain briefly what you can do: analyze customer contexts, provide renewal/retention recommendations, and draft concise email/SMS messages.\n",
    "Keep it short and professional.\n",
    "\"\"\"\n",
    "    return llm.invoke(prompt).content.strip()\n",
    "\n",
    "# Router with follow-up memory\n",
    "def router_node(state: AgentState) -> AgentState:\n",
    "    state[\"route\"] = classify_intent(state[\"question\"])\n",
    "\n",
    "    if state[\"route\"] not in {\"GREETING\", \"UNKNOWN\"}:\n",
    "        q = state[\"question\"]\n",
    "        kind, ident = extract_identifier(q)\n",
    "\n",
    "        # Reuse last identifier for follow-ups without an explicit id\n",
    "        if (kind is None or ident is None) and looks_like_followup(q):\n",
    "            if MEMORY.get(\"last_kind\") and MEMORY.get(\"last_ident\"):\n",
    "                kind = MEMORY[\"last_kind\"]\n",
    "                ident = MEMORY[\"last_ident\"]\n",
    "\n",
    "        if kind == \"CUSTOMER\":\n",
    "            state[\"customer_id\"] = ident\n",
    "            state[\"policy_no\"] = None\n",
    "        elif kind == \"POLICY\":\n",
    "            state[\"policy_no\"] = ident\n",
    "            state[\"customer_id\"] = None\n",
    "        else:\n",
    "            state[\"customer_id\"] = None\n",
    "            state[\"policy_no\"] = None\n",
    "\n",
    "        state = build_context_and_status(state)\n",
    "    return state\n",
    "\n",
    "def reco_node(state: AgentState) -> AgentState:\n",
    "    state[\"response\"] = recommendation_agent(state)\n",
    "    return state\n",
    "\n",
    "def email_node(state: AgentState) -> AgentState:\n",
    "    state[\"response\"] = email_agent(state)\n",
    "    return state\n",
    "\n",
    "def greet_node(state: AgentState) -> AgentState:\n",
    "    state[\"response\"] = greeting_agent()\n",
    "    return state\n",
    "\n",
    "# Graph\n",
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(\"router\", RunnableLambda(router_node))\n",
    "graph.add_node(\"recommendation_agent\", RunnableLambda(reco_node))\n",
    "graph.add_node(\"email_agent\", RunnableLambda(email_node))\n",
    "graph.add_node(\"greeting_agent\", RunnableLambda(greet_node))\n",
    "graph.add_node(\"unknown_agent\", RunnableLambda(unknown_node))\n",
    "\n",
    "graph.set_entry_point(\"router\")\n",
    "graph.add_conditional_edges(\n",
    "    \"router\",\n",
    "    lambda s: s[\"route\"],\n",
    "    {\n",
    "        \"RECOMMENDATION\": \"recommendation_agent\",\n",
    "        \"EMAIL\": \"email_agent\",\n",
    "        \"GREETING\": \"greeting_agent\",\n",
    "        \"UNKNOWN\": \"unknown_agent\",\n",
    "    },\n",
    ")\n",
    "graph.add_edge(\"recommendation_agent\", END)\n",
    "graph.add_edge(\"email_agent\", END)\n",
    "graph.add_edge(\"greeting_agent\", END)\n",
    "graph.add_edge(\"unknown_agent\", END)\n",
    "\n",
    "flow = graph.compile()\n",
    "\n",
    "# Email subject/body extraction (NEW)\n",
    "SUBJECT_FALLBACK = \"Policy Renewal Options\"\n",
    "SUBJECT_RE = re.compile(r\"^\\s*subject\\s*:\\s*(.+)$\", re.IGNORECASE)\n",
    "\n",
    "def extract_subject_and_clean_body(text: str) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    - If a line starts with 'Subject:' (any case), use everything after ':' as the subject\n",
    "      and remove that line from the body.\n",
    "    - Else: use the first non-empty line as subject and remove it from the body.\n",
    "    - Trim quotes/spaces and cap length to ~78 chars.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return SUBJECT_FALLBACK, \"\"\n",
    "\n",
    "    lines = text.splitlines()\n",
    "    subject: Optional[str] = None\n",
    "    body_lines: list[str] = []\n",
    "\n",
    "    for line in lines:\n",
    "        m = SUBJECT_RE.match(line)\n",
    "        if m and subject is None:\n",
    "            subject = m.group(1).strip()\n",
    "            continue  # skip this line from body\n",
    "        body_lines.append(line)\n",
    "\n",
    "    if subject is None:\n",
    "        # choose first non-empty line as subject, then remove it\n",
    "        for idx, line in enumerate(body_lines):\n",
    "            if line.strip():\n",
    "                subject = line.strip().strip(\"'\").strip('\"')\n",
    "                body_lines = body_lines[idx+1:]\n",
    "                break\n",
    "\n",
    "    if not subject:\n",
    "        subject = SUBJECT_FALLBACK\n",
    "\n",
    "    if subject.lower().startswith(\"subject:\"):\n",
    "        subject = subject.split(\":\", 1)[1].strip()\n",
    "\n",
    "    subject = subject[:78]  # safe header length\n",
    "\n",
    "    # strip leading blank lines from body\n",
    "    while body_lines and not body_lines[0].strip():\n",
    "        body_lines.pop(0)\n",
    "\n",
    "    clean_body = \"\\n\".join(body_lines).rstrip()\n",
    "    return subject, clean_body\n",
    "\n",
    "# NEW: last email store\n",
    "_LAST_EMAIL_SUBJ = \"\"\n",
    "_LAST_EMAIL_BODY = \"\"\n",
    "_LAST_ROUTE = \"UNKNOWN\"\n",
    "\n",
    "# UI glue\n",
    "SUGGESTION_TEMPLATES = [\n",
    "    [\"Give a retention recommendation for policy \\\"Policy_no\\\".\"],\n",
    "    [\"Give a retention recommendation for customer \\\"Customer_id\\\".\"],\n",
    "    [\"Write an email for policy \\\"Policy_no\\\" focusing on renewal and value.\"],\n",
    "    [\"Write an email for customer \\\"Customer_id\\\" focusing on renewal and value.\"],\n",
    "    [\"Give a retention recommendation for the customers whose having \\\"Reason\\\" as a reason.\"],\n",
    "    [\"Write a mail draft for the customers whose having \\\"Reason\\\" as a reason.\"],\n",
    "    [\"For this policy, write a retention email draft.\"],\n",
    "    [\"For this customer, write a retention email draft.\"]\n",
    "]\n",
    "\n",
    "def chat_respond(message: str, history: list) -> str:\n",
    "    global _LAST_EMAIL_SUBJ, _LAST_EMAIL_BODY, _LAST_ROUTE\n",
    "    try:\n",
    "        out = flow.invoke({\"question\": message})\n",
    "        _LAST_ROUTE = out.get(\"route\", \"UNKNOWN\")\n",
    "        if _LAST_ROUTE == \"EMAIL\":\n",
    "            raw_body = out.get(\"response\", \"\") or \"\"\n",
    "            subj, clean = extract_subject_and_clean_body(raw_body)\n",
    "            _LAST_EMAIL_SUBJ = subj or SUBJECT_FALLBACK\n",
    "            _LAST_EMAIL_BODY = clean\n",
    "        return out.get(\"response\", \"No response.\")\n",
    "    except Exception as e:\n",
    "        _LAST_ROUTE = \"UNKNOWN\"\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "with gr.Blocks(title=\"Retention Assistant\") as app:\n",
    "    demo = gr.ChatInterface(\n",
    "        fn=chat_respond,\n",
    "        title=\"Retention Assistant\",\n",
    "        textbox=gr.Textbox(placeholder=\"Ask for a recommendation or request an email/SMS draft…\"),\n",
    "        retry_btn=\"Retry\",\n",
    "        undo_btn=\"Undo\",\n",
    "        clear_btn=\"Clear\",\n",
    "        examples=SUGGESTION_TEMPLATES,\n",
    "        cache_examples=False\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "        to_box = gr.Textbox(label=\"To\", value=DEFAULT_TO_EMAIL, scale=4)\n",
    "        send_btn = gr.Button(\"Send Mail\", visible=True, scale=1)\n",
    "    send_status = gr.Markdown(\"\")\n",
    "\n",
    "    def _on_send(to_addr: str):\n",
    "        global _LAST_EMAIL_SUBJ, _LAST_EMAIL_BODY, _LAST_ROUTE\n",
    "        if _LAST_ROUTE != \"EMAIL\":\n",
    "            return \"No email draft detected. Ask for an email draft first.\"\n",
    "        if not to_addr:\n",
    "            return \"Please provide a 'To' address.\"\n",
    "        if not _LAST_EMAIL_BODY:\n",
    "            return \"No email body available.\"\n",
    "\n",
    "        try:\n",
    "            scopes = [\"https://www.googleapis.com/auth/gmail.send\"]\n",
    "            creds = None\n",
    "            if os.path.exists(GMAIL_TOKEN_FILE):\n",
    "                creds = Credentials.from_authorized_user_file(GMAIL_TOKEN_FILE, scopes)\n",
    "            if not creds or not creds.valid:\n",
    "                if creds and creds.expired and creds.refresh_token:\n",
    "                    creds.refresh(Request())\n",
    "                else:\n",
    "                    flow = InstalledAppFlow.from_client_secrets_file(GMAIL_CREDENTIALS_FILE, scopes)\n",
    "                    try:\n",
    "                        creds = flow.run_local_server(port=0)\n",
    "                    except Exception:\n",
    "                        print(\"Falling back to console OAuth flow...\")\n",
    "                        creds = flow.run_console()\n",
    "                with open(GMAIL_TOKEN_FILE, \"w\") as f:\n",
    "                    f.write(creds.to_json())\n",
    "            service = build(\"gmail\", \"v1\", credentials=creds)\n",
    "\n",
    "            sender = SENDER_EMAIL or \"\"\n",
    "            if not sender:\n",
    "                return \"SENDER_EMAIL not set in environment.\"\n",
    "\n",
    "            # Use cleaned subject and body\n",
    "            msg = MIMEText(_LAST_EMAIL_BODY, _subtype=\"plain\", _charset=\"utf-8\")\n",
    "            msg[\"to\"] = to_addr\n",
    "            msg[\"from\"] = sender\n",
    "            msg[\"subject\"] = _LAST_EMAIL_SUBJ or SUBJECT_FALLBACK\n",
    "\n",
    "            raw = base64.urlsafe_b64encode(msg.as_bytes()).decode(\"utf-8\")\n",
    "            sent = service.users().messages().send(userId=\"me\", body={\"raw\": raw}).execute()\n",
    "            return f\"Sent. Gmail message id: {sent.get('id','')}\"\n",
    "        except Exception as e:\n",
    "            return f\"Send failed: {e}\"\n",
    "\n",
    "    send_btn.click(_on_send, inputs=[to_box], outputs=[send_status])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc1b2ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load data from Excel file\n",
    "df = pd.read_excel(\"2024 PR_tie_up.xlsx\")\n",
    "\n",
    "# Step 2: Convert date columns to datetime\n",
    "for col in ['Policy Start Date', 'Policy End Date']:\n",
    "    df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "\n",
    "# Step 3: Define the function to handle duplicates\n",
    "def prioritize_duplicates(group):\n",
    "    # Exclude rows with negative Net Premium values\n",
    "    positive_premium = group[group['Net Premium'] >= 0]\n",
    "    \n",
    "    if not positive_premium.empty:\n",
    "        # If all rows have positive Net Premium, select the one with the highest value\n",
    "        return positive_premium.loc[positive_premium['Net Premium'].idxmax()]\n",
    "    else:\n",
    "        # If all values are the same, select any one\n",
    "        return group.iloc[0]\n",
    "\n",
    "# Step 4: Handle duplicates\n",
    "cleaned_duplicates = (\n",
    "    df.groupby(['Policy No', 'Policy Start Date', 'Policy End Date'], group_keys=False)\n",
    "    .apply(prioritize_duplicates)\n",
    ")\n",
    "\n",
    "# Drop duplicates and merge cleaned data\n",
    "cleaned_duplicates = cleaned_duplicates.dropna()\n",
    "df_cleaned = pd.concat([df.drop_duplicates(subset=['Policy No', 'Policy Start Date', 'Policy End Date']), cleaned_duplicates])\n",
    "df_cleaned = df_cleaned.drop_duplicates(subset=['Policy No', 'Policy Start Date', 'Policy End Date'], keep='last')\n",
    "\n",
    "# Step 5: Save the cleaned dataset to CSV\n",
    "df_cleaned.to_csv(\"cleaned_2024_PR_tie_up.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_20392\\2052456785.py:30: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(prioritize_duplicates)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved to 'cleaned_2024_PR_tie_up.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load data from Excel file\n",
    "df = pd.read_excel(\"2024 PR_tie_up.xlsx\")\n",
    "\n",
    "# Step 2: Convert date columns to datetime\n",
    "date_columns = ['Policy Start Date', 'Policy End Date', 'Policy Issue date']\n",
    "for col in date_columns:\n",
    "    df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "\n",
    "# Step 3: Define the function to handle duplicates\n",
    "def prioritize_duplicates(group):\n",
    "    # Step 3a: Select the row(s) with the latest Policy Issue Date\n",
    "    latest_issue_date = group['Policy Issue date'].max()\n",
    "    latest_rows = group[group['Policy Issue date'] == latest_issue_date]\n",
    "    \n",
    "    # Step 3b: Exclude rows with negative Net Premium values\n",
    "    positive_premium_rows = latest_rows[latest_rows['Net Premium'] >= 0]\n",
    "    \n",
    "    if not positive_premium_rows.empty:\n",
    "        # Step 3c: If positive Net Premium rows exist, select the one with the highest value\n",
    "        return positive_premium_rows.loc[positive_premium_rows['Net Premium'].idxmax()]\n",
    "    else:\n",
    "        # Step 3d: If all Net Premium values are negative or no positive premiums, select the first row\n",
    "        return latest_rows.iloc[0]\n",
    "\n",
    "# Step 4: Handle duplicates\n",
    "df_cleaned = (\n",
    "    df.groupby(['Policy No', 'Policy Start Date', 'Policy End Date'], group_keys=False)\n",
    "    .apply(prioritize_duplicates)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Step 5: Save the cleaned dataset to CSV\n",
    "df_cleaned.to_csv(\"cleaned_2024_PR_tie_up.csv\", index=False)\n",
    "\n",
    "print(\"Cleaned dataset saved to 'cleaned_2024_PR_tie_up.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
